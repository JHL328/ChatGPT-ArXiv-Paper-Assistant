{
    "2509.16825": {
        "SCORE": 19,
        "ARXIVID": "2509.16825",
        "COMMENT": "Matches Model Architecture: introduces a dual-domain neural operator (KANO) with theoretical expressivity gains over FNO and symbolic interpretability.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Jin Lee",
            "Ziming Liu",
            "Xinling Yu",
            "Yixuan Wang",
            "Haewon Jeong",
            "Murphy Yuezhen Niu",
            "Zheng Zhang"
        ],
        "title": "KANO: Kolmogorov-Arnold Neural Operator",
        "abstract": "We introduce Kolmogorov--Arnold Neural Operator (KANO), a dual-domain neural operator jointly parameterized by both spectral and spatial bases with intrinsic symbolic interpretability. We theoretically demonstrate that KANO overcomes the pure-spectral bottleneck of Fourier Neural Operator (FNO): KANO remains expressive over generic position-dependent dynamics for any physical input, whereas FNO stays practical only for spectrally sparse operators and strictly imposes a fast-decaying input Fourier tail. We verify our claims empirically on position-dependent differential operators, for which KANO robustly generalizes but FNO fails to. In the quantum Hamiltonian learning benchmark, KANO reconstructs ground-truth Hamiltonians in closed-form symbolic representations accurate to the fourth decimal place in coefficients and attains $\\approx 6\\times10^{-6}$ state infidelity from projective measurement data, substantially outperforming that of the FNO trained with ideal full wave function data, $\\approx 1.5\\times10^{-2}$, by orders of magnitude.",
        "arxiv_id": "2509.16825"
    },
    "2509.16989": {
        "SCORE": 19,
        "ARXIVID": "2509.16989",
        "COMMENT": "Matches Model Compression and Efficiency: proposes ternary PTQ via structured trit-plane decomposition enabling multiplication-free inference for LLMs with model-agnostic deployment.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "He Xiao",
            "Runming Yang",
            "Qingyao Yang",
            "Wendong Xu",
            "Zheng Li",
            "Yupeng Su",
            "Zhengwu Liu",
            "Hongxia Yang",
            "Ngai Wong"
        ],
        "title": "PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models",
        "abstract": "Post-training quantization (PTQ) of large language models (LLMs) to extremely low bit-widths remains challenging due to the fundamental trade-off between computational efficiency and model expressiveness. While existing ultra-low-bit PTQ methods rely on binary approximations or complex compensation mechanisms, they suffer from either limited representational capacity or computational overhead that undermines their efficiency gains. We introduce PTQ to Trit-Planes (PTQTP), the first ternary-weight PTQ framework that decomposes weight matrices into structured ternary {-1, 0, 1} trit-planes using 2x1.58-bit representation. PTQTP achieves multiplication-free inference, identical to 1-bit quantization, while maintaining superior expressiveness through its novel structured decomposition. Our approach provides: (1) a theoretically grounded progressive approximation algorithm ensuring global weight consistency; (2) model-agnostic deployment across diverse modern LLMs without architectural modifications; and (3) uniform ternary operations that eliminate the need for mixed-precision or compensation schemes. Comprehensive experiments across LLaMA3.x and Qwen3 model families (0.6B-70B parameters) demonstrate that PTQTP significantly outperforms existing low-bit PTQ methods, achieving 82.4% mathematical reasoning retention versus 0% for competing approaches. PTQTP approaches and sometimes surpasses 1.58-bit quantization-aware training performance while requiring only single-hour quantization compared to 10-14 GPU days for training-based methods. These results establish PTQTP as a practical solution for efficient LLM deployment in resource-constrained environments.",
        "arxiv_id": "2509.16989"
    },
    "2509.16857": {
        "SCORE": 19,
        "ARXIVID": "2509.16857",
        "COMMENT": "Matches ML-Systems: inference-serving with distributed prefix caching; SmartNIC-offloaded data plane, chunked pipeline, and minimal-copy memory management with strong TPOT/TTFT gains.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Xingyu Xiang",
            "Raj Joshi",
            "Yuhan Liu",
            "Jiayi Yao",
            "Chenxingyu Zhao",
            "Junchen Jiang",
            "Yang Zhou",
            "Eddie Kohler",
            "Minlan Yu"
        ],
        "title": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix Caching",
        "abstract": "Distributed prefix caching accelerates long-context LLM serving by reusing KV cache entries for common context prefixes. However, KV cache fetches can become a bottleneck when network bandwidth is limited. Compression mitigates the bandwidth issue, but can degrade overall performance when decompression interferes with model computation.   We present ShadowServe, the first SmartNIC-accelerated, interference-free prefix caching system for LLM serving. ShadowServe separates a control plane on the host and a data plane fully offloaded to the SmartNIC, which eliminates interference to both host GPU and CPU. To overcome the SmartNIC's limited compute and memory resources, we design a chunked pipeline that parallelizes data plane operations across the SmartNIC's compute resources, and a minimal-copy memory management scheme that reduces memory pressure on the SmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to 2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token (TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to up to 1.35x higher throughput.",
        "arxiv_id": "2509.16857"
    },
    "2509.16898": {
        "SCORE": 19,
        "ARXIVID": "2509.16898",
        "COMMENT": "Representation Learning \u2014 theoretical complexity results (PLS/CLS-hardness) for local optima in contrastive objectives, characterizing optimization dynamics.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Jingming Yan",
            "Yiyuan Luo",
            "Vaggos Chatziafratis",
            "Ioannis Panageas",
            "Parnian Shahkar",
            "Stelios Stavroulakis"
        ],
        "title": "The Complexity of Finding Local Optima in Contrastive Learning",
        "abstract": "Contrastive learning is a powerful technique for discovering meaningful data representations by optimizing objectives based on $\\textit{contrastive information}$, often given as a set of weighted triplets $\\{(x_i, y_i^+, z_{i}^-)\\}_{i = 1}^m$ indicating that an \"anchor\" $x_i$ is more similar to a \"positive\" example $y_i$ than to a \"negative\" example $z_i$. The goal is to find representations (e.g., embeddings in $\\mathbb{R}^d$ or a tree metric) where anchors are placed closer to positive than to negative examples. While finding $\\textit{global}$ optima of contrastive objectives is $\\mathsf{NP}$-hard, the complexity of finding $\\textit{local}$ optima -- representations that do not improve by local search algorithms such as gradient-based methods -- remains open. Our work settles the complexity of finding local optima in various contrastive learning problems by proving $\\mathsf{PLS}$-hardness in discrete settings (e.g., maximize satisfied triplets) and $\\mathsf{CLS}$-hardness in continuous settings (e.g., minimize Triplet Loss), where $\\mathsf{PLS}$ (Polynomial Local Search) and $\\mathsf{CLS}$ (Continuous Local Search) are well-studied complexity classes capturing local search dynamics in discrete and continuous optimization, respectively. Our results imply that no polynomial time algorithm (local search or otherwise) can find a local optimum for various contrastive learning problems, unless $\\mathsf{PLS}\\subseteq\\mathsf{P}$ (or $\\mathsf{CLS}\\subseteq \\mathsf{P}$ for continuous problems). Even in the unlikely scenario that $\\mathsf{PLS}\\subseteq\\mathsf{P}$ (or $\\mathsf{CLS}\\subseteq \\mathsf{P}$), our reductions imply that there exist instances where local search algorithms need exponential time to reach a local optimum, even for $d=1$ (embeddings on a line).",
        "arxiv_id": "2509.16898"
    },
    "2509.16882": {
        "SCORE": 18,
        "ARXIVID": "2509.16882",
        "COMMENT": "Model Architecture (MoE): dynamic expert specialization with adaptive router, expert-domain isolation, and progressive freezing to avoid catastrophic forgetting in multi-domain MoE adaptation.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Junzhuo Li",
            "Bo Wang",
            "Xiuze Zhou",
            "Xuming Hu"
        ],
        "title": "Dynamic Expert Specialization: Towards Catastrophic Forgetting-Free Multi-Domain MoE Adaptation",
        "abstract": "Mixture-of-Experts (MoE) models offer immense capacity via sparsely gated expert subnetworks, yet adapting them to multiple domains without catastrophic forgetting remains an open challenge. Existing approaches either incur prohibitive computation, suffer cross-domain interference, or require separate runs per domain. We propose DES-MoE, a dynamic expert specialization framework for multi-domain adaptation of Mixture-of-Experts models. DES-MoE addresses catastrophic forgetting through three innovations: (1) an adaptive router balancing pre-trained knowledge retention and task-specific updates via distillation, (2) real-time expert-domain correlation mapping to isolate domain-specific gradients, and (3) a three-phase adaptive fine-tuning schedule that progressively freezes non-specialized parameters. Evaluated on six domains (math, code, law, etc.), DES-MoE matches single-domain ESFT performance while training one unified model, reduces forgetting by 89% compared to full fine-tuning as domains scale from 2 to 6, and achieves 68% faster convergence than conventional methods. Our work establishes dynamic expert isolation as a scalable paradigm for multi-task MoE adaptation.",
        "arxiv_id": "2509.16882"
    },
    "2509.17874": {
        "SCORE": 18,
        "ARXIVID": "2509.17874",
        "COMMENT": "Model Architecture + Compression/Efficiency \u2014 introduces Nested Subspace Networks: a nested low\u2011rank reparameterization of linear layers enabling continuous, dynamic compute at inference with joint multi\u2011rank training.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Paulius Rauba",
            "Mihaela van der Schaar"
        ],
        "title": "Deep Hierarchical Learning with Nested Subspace Networks",
        "abstract": "Large neural networks are typically trained for a fixed computational budget, creating a rigid trade-off between performance and efficiency that is ill-suited for deployment in resource-constrained or dynamic environments. Existing approaches to this problem present a difficult choice: training a discrete collection of specialist models is computationally prohibitive, while dynamic methods like slimmable networks often lack the flexibility to be applied to large, pre-trained foundation models. In this work, we propose Nested Subspace Networks (NSNs), a novel architectural paradigm that enables a single model to be dynamically and granularly adjusted across a continuous spectrum of compute budgets at inference time. The core of our approach is to re-parameterize linear layers to satisfy a nested subspace property, such that the function computed at a given rank is a strict subspace of the function at any higher rank. We show that this entire hierarchy of models can be optimized jointly via an uncertainty-aware objective that learns to balance the contributions of different ranks based on their intrinsic difficulty. We demonstrate empirically that NSNs can be surgically applied to pre-trained LLMs and unlock a smooth and predictable compute-performance frontier. For example, a single NSN-adapted model can achieve a 50% reduction in inference FLOPs with only a 5 percentage point loss in accuracy. Our findings establish NSNs as a powerful framework for creating the next generation of adaptive foundation models.",
        "arxiv_id": "2509.17874"
    },
    "2509.17238": {
        "SCORE": 18,
        "ARXIVID": "2509.17238",
        "COMMENT": "MoE + ML Systems/Efficiency \u2014 training\u2011free hyper\u2011parallel inference for MoE via stochastic routing to sample multiple experts per token with specialized batching and KV\u2011cache optimization.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Soheil Zibakhsh",
            "Mohammad Samragh",
            "Kumari Nishu",
            "Lauren Hannah",
            "Arnav Kundu",
            "Minsik Cho"
        ],
        "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE",
        "abstract": "The generation quality of large language models (LLMs) is often improved by utilizing inference-time sequence-level scaling methods (e.g., Chain-of-Thought). We introduce hyper-parallel scaling, a complementary framework that improves prediction quality at the token level. Hyper-parallel scaling computes and aggregates multiple output proposals for a single token from the model. We implement this concept in Mixture-of-Experts (MoE) models, which we refer to as Roster of Experts (RoE). RoE is a training-free inference algorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects controlled stochasticity into the expert routing mechanism, enabling it to sample multiple diverse experts for each token and aggregate their outputs for a more accurate final prediction.To overcome the computational cost, we introduce an efficient batching strategy and a specialized KV-caching mechanism that minimizes compute and memory overhead. For example, RoE enables a 7B MoE model to match the performance of a 10.5B MoE model while using 30% less compute for inference. These gains are achieved without any fine-tuning of model parameters.",
        "arxiv_id": "2509.17238"
    },
    "2509.16875": {
        "SCORE": 18,
        "ARXIVID": "2509.16875",
        "COMMENT": "Model Architecture and Efficiency \u2014 Contract-and-Broadcast Self-Attention derived from a unified optimization objective achieving linear scaling and interpretability.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Qishuai Wen",
            "Zhiyuan Huang",
            "Chun-Guang Li"
        ],
        "title": "Towards Interpretable and Efficient Attention: Compressing All by Contracting a Few",
        "abstract": "Attention mechanisms in Transformers have gained significant empirical success. Nonetheless, the optimization objectives underlying their forward pass are still unclear. Additionally, the quadratic complexity of self-attention is increasingly prohibitive. Unlike the prior work on addressing the interpretability or efficiency issue separately, we propose a unified optimization objective to alleviate both issues simultaneously. By unrolling the optimization over the objective, we derive an inherently interpretable and efficient attention mechanism, which compresses all tokens into low-dimensional structures by contracting a few representative tokens and then broadcasting the contractions back. This Contract-and-Broadcast Self-Attention (CBSA) mechanism can not only scale linearly but also generalize existing attention mechanisms as its special cases. Experiments further demonstrate comparable performance and even superior advantages of CBSA on several visual tasks. Code is available at this https URL.",
        "arxiv_id": "2509.16875"
    },
    "2509.17791": {
        "SCORE": 17,
        "ARXIVID": "2509.17791",
        "COMMENT": "Compression/Efficiency: comprehensive FP4 training design space via microscaling quantization with forward/backward cost analysis and large-scale empirical study.",
        "RELEVANCE": 10,
        "NOVELTY": 7,
        "authors": [
            "Robert Hu",
            "Carlo Luschi",
            "Paul Balanca"
        ],
        "title": "Elucidating the Design Space of FP4 training",
        "abstract": "The increasing computational demands of foundation models have spurred research into low-precision training, with 4-bit floating-point (\\texttt{FP4}) formats emerging as a frontier for maximizing hardware throughput. While numerous techniques have been proposed to stabilize \\texttt{FP4} training, they often present isolated solutions with varying, and not always clear, computational overheads. This paper aims to provide a unified view of the design space of \\texttt{FP4} training. We introduce a comprehensive, quantisation gradient-based framework for microscaling quantization that allows for a theoretical analysis of the computational costs associated with different stabilization methods on both the forward and backward passes. Using a simulator built on this framework, we conduct an extensive empirical study across a wide range of machine learning tasks, including regression, image classification, diffusion models, and language models. By systematically evaluating thousands of combinations of techniques, such as novel gradient approximations, rounding strategies, and scaling methods, we identify which configurations offer the most favourable performance-to-overhead trade-off. We find that the techniques enabling the best trade-off involve carefully combining Hadamard transformations, tensor scaling and stochastic rounding. We further find that using \\texttt{UE5M3} as a scaling factor potentially offers a good compromise between range and precision with manageable computational overhead.",
        "arxiv_id": "2509.17791"
    },
    "2509.17866": {
        "SCORE": 17,
        "ARXIVID": "2509.17866",
        "COMMENT": "Representation Learning / Training Dynamics: SVD-based analysis uncovers consistent singular value scaling and coordinated singular-vector rotations induced by post-training.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Xinyu He",
            "Xianghui Cao"
        ],
        "title": "Understanding Post-Training Structural Changes in Large Language Models",
        "abstract": "Post-training fundamentally alters the behavior of large language models (LLMs), yet its impact on the internal parameter space remains poorly understood. In this work, we conduct a systematic singular value decomposition (SVD) analysis of principal linear layers in pretrained LLMs, focusing on two widely adopted post-training methods: instruction tuning and long-chain-of-thought (Long-CoT) distillation. Our analysis reveals two consistent and unexpected structural changes:(1) a near-uniform geometric scaling of singular values across layers, which theoretically modulates attention scores; and (2) highly consistent orthogonal transformations are applied to the left and right singular vectors of each matrix. Disrupting this orthogonal consistency leads to catastrophic performance degradation. Based on these findings, we propose a simple yet effective framework that interprets post-training as a reparameterization of fixed subspaces in the pretrained parameter space. Further experiments reveal that singular value scaling behaves as a secondary effect, analogous to a temperature adjustment, whereas the core functional transformation lies in the coordinated rotation of singular vectors. These results challenge the prevailing view of the parameter space in large models as a black box, uncovering the first clear regularities in how parameters evolve during training, and providing a new perspective for deeper investigation into model parameter changes.",
        "arxiv_id": "2509.17866"
    },
    "2509.17765": {
        "SCORE": 17,
        "ARXIVID": "2509.17765",
        "COMMENT": "Model Architecture (MoE) and ML Systems: Thinker\u2013Talker MoE plus streaming-friendly codec-to-causal-ConvNet design for low-latency multimodal inference.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jin Xu",
            "Zhifang Guo",
            "Hangrui Hu",
            "Yunfei Chu",
            "Xiong Wang",
            "Jinzheng He",
            "Yuxuan Wang",
            "Xian Shi",
            "Ting He",
            "Xinfa Zhu",
            "Yuanjun Lv",
            "Yongqi Wang",
            "Dake Guo",
            "He Wang",
            "Linhan Ma",
            "Pei Zhang",
            "Xinyu Zhang",
            "Hongkun Hao",
            "Zishan Guo",
            "Baosong Yang",
            "Bin Zhang",
            "Ziyang Ma",
            "Xipin Wei",
            "Shuai Bai",
            "Keqin Chen",
            "Xuejing Liu",
            "Peng Wang",
            "Mingkun Yang",
            "Dayiheng Liu",
            "Xingzhang Ren",
            "Bo Zheng",
            "Rui Men",
            "Fan Zhou",
            "Bowen Yu",
            "Jianxin Yang",
            "Le Yu",
            "Jingren Zhou",
            "Junyang Lin"
        ],
        "title": "Qwen3-Omni Technical Report",
        "abstract": "We present Qwen3-Omni, a single multimodal model that, for the first time, maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages, and speech generation in 10 languages. To reduce first-packet latency in streaming synthesis, Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms. To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.",
        "arxiv_id": "2509.17765"
    },
    "2509.17738": {
        "SCORE": 17,
        "ARXIVID": "2509.17738",
        "COMMENT": "Representation Learning / Training Dynamics: empirical and theoretical analysis linking flatness (vs. neural collapse) to generalization using grokking; isolates geometric properties predictive of generalization.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Ting Han",
            "Linara Adilova",
            "Henning Petzka",
            "Jens Kleesiek",
            "Michael Kamp"
        ],
        "title": "Flatness is Necessary, Neural Collapse is Not: Rethinking Generalization via Grokking",
        "abstract": "Neural collapse, i.e., the emergence of highly symmetric, class-wise clustered representations, is frequently observed in deep networks and is often assumed to reflect or enable generalization. In parallel, flatness of the loss landscape has been theoretically and empirically linked to generalization. Yet, the causal role of either phenomenon remains unclear: Are they prerequisites for generalization, or merely by-products of training dynamics? We disentangle these questions using grokking, a training regime in which memorization precedes generalization, allowing us to temporally separate generalization from training dynamics and we find that while both neural collapse and relative flatness emerge near the onset of generalization, only flatness consistently predicts it. Models encouraged to collapse or prevented from collapsing generalize equally well, whereas models regularized away from flat solutions exhibit delayed generalization. Furthermore, we show theoretically that neural collapse implies relative flatness under classical assumptions, explaining their empirical co-occurrence. Our results support the view that relative flatness is a potentially necessary and more fundamental property for generalization, and demonstrate how grokking can serve as a powerful probe for isolating its geometric underpinnings.",
        "arxiv_id": "2509.17738"
    },
    "2509.17786": {
        "SCORE": 17,
        "ARXIVID": "2509.17786",
        "COMMENT": "Model Compression and Efficiency: low-rank LoRA model merging via a shared core space with a no-information-loss proof and complexity analysis; algorithm\u2013system efficiency improvement for multi-task adapters.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Aniello Panariello",
            "Daniel Marczak",
            "Simone Magistri",
            "Angelo Porrello",
            "Bart{\\l}omiej Twardowski",
            "Andrew D. Bagdanov",
            "Simone Calderara",
            "Joost van de Weijer"
        ],
        "title": "Accurate and Efficient Low-Rank Model Merging in Core Space",
        "abstract": "In this paper, we address the challenges associated with merging low-rank adaptations of large neural networks. With the rise of parameter-efficient adaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning has become more accessible. While fine-tuning models with LoRA is highly efficient, existing merging methods often sacrifice this efficiency by merging fully-sized weight matrices. We propose the Core Space merging framework, which enables the merging of LoRA-adapted models within a common alignment basis, thereby preserving the efficiency of low-rank adaptation while substantially improving accuracy across tasks. We further provide a formal proof that projection into Core Space ensures no loss of information and provide a complexity analysis showing the efficiency gains. Extensive empirical results demonstrate that Core Space significantly improves existing merging techniques and achieves state-of-the-art results on both vision and language tasks while utilizing a fraction of the computational resources. Codebase is available at https://github.com/apanariello4/core-space-merging.",
        "arxiv_id": "2509.17786"
    },
    "2509.16820": {
        "SCORE": 17,
        "ARXIVID": "2509.16820",
        "COMMENT": "Model Architecture / Representation Learning: inference-time steering by injecting vectors into query/value spaces with analytical characterization; improves control efficacy over residual-stream steering.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Max Torop",
            "Aria Masoomi",
            "Masih Eskandar",
            "Jennifer Dy"
        ],
        "title": "DISCO: Disentangled Communication Steering for Large Language Models",
        "abstract": "A variety of recent methods guide large language model outputs via the inference-time addition of steering vectors to residual-stream or attention-head representations. In contrast, we propose to inject steering vectors directly into the query and value representation spaces within attention heads. We provide evidence that a greater portion of these spaces exhibit high linear discriminability of concepts --a key property motivating the use of steering vectors-- than attention head outputs. We analytically characterize the effect of our method, which we term DISentangled COmmunication (DISCO) Steering, on attention head outputs. Our analysis reveals that DISCO disentangles a strong but underutilized baseline, steering attention inputs, which implicitly modifies queries and values in a rigid manner. In contrast, DISCO's direct modulation of these components enables more granular control. We find that DISCO achieves superior performance over a number of steering vector baselines across multiple datasets on LLaMA 3.1 8B and Gemma 2 9B, with steering efficacy scoring up to 19.1% higher than the runner-up. Our results support the conclusion that the query and value spaces are powerful building blocks for steering vector methods.",
        "arxiv_id": "2509.16820"
    },
    "2509.16443": {
        "SCORE": 17,
        "ARXIVID": "2509.16443",
        "COMMENT": "Matches ML-Systems: compiler and IR for heterogeneous photonic\u2013electronic inference; algorithm\u2013system co-design enabling latency/energy-optimized operator assignment.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Ryan Tomich",
            "Zhizhen Zhong",
            "Dirk Englund"
        ],
        "title": "LightCode: Compiling LLM Inference for Photonic-Electronic Systems",
        "abstract": "The growing demand for low-latency, energy-efficient inference in large language models (LLMs) has catalyzed interest in heterogeneous architectures. While GPUs remain dominant, they are poorly suited for integration with emerging domain-specific accelerators like the Photonic Tensor Units (PTUs), which offer low-power, high-throughput linear computation. This motivates hybrid compilation strategies that combine photonic and electronic resources. We present LightCode, a compiler framework and simulator for mapping LLM inference workloads across hybrid photonic-electronic systems. LightCode introduces the Stacked Graph, an intermediate representation that encodes multiple hardware-specific realizations of each tensor operation. Hardware assignment is formulated as a constrained subgraph selection problem optimized for latency or energy under parametric cost models. We evaluate LightCode on the prefill stage of GPT-2 and Llama-7B showing that under our workload and hardware assumptions, (i) Photonic hardware reduced energy by up to 50% in our simulated workloads at maximum sequence length; (ii) multiplexing and assignment strategy yielded latency improvements exceeding 10x; and (iii) Optimizing for latency or energy resulted in distinct hardware mappings in our simulations. LightCode offers a module, foundational framework and simulator for compiling LLMs to emerging photonic accelerators.",
        "arxiv_id": "2509.16443"
    },
    "2509.16248": {
        "SCORE": 17,
        "ARXIVID": "2509.16248",
        "COMMENT": "ML Systems \u2014 compiler/code-generation: high-level source-to-source transformations that eliminate PyTorch FX graph breaks, enabling larger fused graphs and measurable latency/throughput gains.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Savini Kashmira",
            "Jayanaka Dantanarayana",
            "Thamirawaran Sathiyalogeswaran",
            "Yichao Yuan",
            "Nishil Talati",
            "Krisztian Flautner",
            "Lingjia Tang",
            "Jason Mars"
        ],
        "title": "GraphMend: Code Transformations for Fixing Graph Breaks in PyTorch 2",
        "abstract": "This paper presents GraphMend, a high-level compiler that eliminates FX graph breaks in PyTorch 2 programs. Although PyTorch 2 introduced TorchDynamo and TorchInductor to enable just-in-time graph compilation, unresolved dynamic control flow and unsupported Python constructs often fragment models into multiple FX graphs. These fragments force frequent fallbacks to eager mode, incur costly CPU-to-GPU synchronizations, and reduce optimization opportunities. GraphMend addresses this limitation by analyzing and transforming source code before execution. Built on the Jac compilation framework, GraphMend introduces two code transformations that remove graph breaks due to dynamic control flow and Python I/O functions. This design allows PyTorch's compilation pipeline to capture larger, uninterrupted FX graphs without requiring manual refactoring by developers. Evaluation across eight Hugging Face models shows that GraphMend removes all fixable graph breaks due to dynamic control flow and Python I/O functions, driving the break count to 0 in 6 models and reducing it from 5 to 2 in another model. On NVIDIA RTX 3090 and A40 GPUs, GraphMend achieves up to 75% latency reductions and up to 8% higher end-to-end throughput. These results demonstrate that high-level code transformation is an effective complement to PyTorch's dynamic JIT compilation pipeline, substantially improving both usability and performance.",
        "arxiv_id": "2509.16248"
    },
    "2509.16379": {
        "SCORE": 17,
        "ARXIVID": "2509.16379",
        "COMMENT": "Representation Learning: moment-preserving distributional descriptors with theoretical guarantees (Carleman, Cram\u00e9r\u2013Wold) as an alternative to pooling.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Xinran Liu",
            "Shansita D. Sharma",
            "Soheil Kolouri"
        ],
        "title": "EMPEROR: Efficient Moment-Preserving Representation of Distributions",
        "abstract": "We introduce EMPEROR (Efficient Moment-Preserving Representation of Distributions), a mathematically rigorous and computationally efficient framework for representing high-dimensional probability measures arising in neural network representations. Unlike heuristic global pooling operations, EMPEROR encodes a feature distribution through its statistical moments. Our approach leverages the theory of sliced moments: features are projected onto multiple directions, lightweight univariate Gaussian mixture models (GMMs) are fit to each projection, and the resulting slice parameters are aggregated into a compact descriptor. We establish determinacy guarantees via Carleman's condition and the Cram\\'er-Wold theorem, ensuring that the GMM is uniquely determined by its sliced moments, and we derive finite-sample error bounds that scale optimally with the number of slices and samples. Empirically, EMPEROR captures richer distributional information than common pooling schemes across various data modalities, while remaining computationally efficient and broadly applicable.",
        "arxiv_id": "2509.16379"
    },
    "2509.17196": {
        "SCORE": 17,
        "ARXIVID": "2509.17196",
        "COMMENT": "Representation Learning \u2014 tracks feature emergence across pretraining via sparse dictionary learning (crosscoders), shedding light on training dynamics.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Xuyang Ge",
            "Wentao Shu",
            "Jiaxing Wu",
            "Yunhua Zhou",
            "Zhengfu He",
            "Xipeng Qiu"
        ],
        "title": "Evolution of Concepts in Language Model Pre-Training",
        "abstract": "Language models obtain extensive capabilities through pre-training. However, the pre-training process remains a black box. In this work, we track linear interpretable feature evolution across pre-training snapshots using a sparse dictionary learning method called crosscoders. We find that most features begin to form around a specific point, while more complex patterns emerge in later training stages. Feature attribution analyses reveal causal connections between feature evolution and downstream performance. Our feature-level observations are highly consistent with previous findings on Transformer's two-stage learning process, which we term a statistical learning phase and a feature learning phase. Our work opens up the possibility to track fine-grained representation progress during language model learning dynamics.",
        "arxiv_id": "2509.17196"
    },
    "2509.17382": {
        "SCORE": 17,
        "ARXIVID": "2509.17382",
        "COMMENT": "Low-rank estimation theory: clean bias\u2013variance tradeoff and rank-adaptive bounds for tensor SVD estimators\u2014foundational for compression/low-rank approaches.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Shivam Kumar",
            "Haotian Xu",
            "Carlos Misael Madrid Padilla",
            "Yuehaw Khoo",
            "Oscar Hernan Madrid Padilla",
            "Daren Wang"
        ],
        "title": "Bias-variance Tradeoff in Tensor Estimation",
        "abstract": "We study denoising of a third-order tensor when the ground-truth tensor is not necessarily Tucker low-rank. Specifically, we observe $$ Y=X^\\ast+Z\\in \\mathbb{R}^{p_{1} \\times p_{2} \\times p_{3}}, $$ where $X^\\ast$ is the ground-truth tensor, and $Z$ is the noise tensor. We propose a simple variant of the higher-order tensor SVD estimator $\\widetilde{X}$. We show that uniformly over all user-specified Tucker ranks $(r_{1},r_{2},r_{3})$, $$ \\| \\widetilde{X} - X^* \\|_{ \\mathrm{F}}^2 = O \\Big( \\kappa^2 \\Big\\{ r_{1}r_{2}r_{3}+\\sum_{k=1}^{3} p_{k} r_{k} \\Big\\} \\; + \\; \\xi_{(r_{1},r_{2},r_{3})}^2\\Big) \\quad \\text{ with high probability.} $$ Here, the bias term $\\xi_{(r_1,r_2,r_3)}$ corresponds to the best achievable approximation error of $X^\\ast$ over the class of tensors with Tucker ranks $(r_1,r_2,r_3)$; $\\kappa^2$ quantifies the noise level; and the variance term $\\kappa^2 \\{r_{1}r_{2}r_{3}+\\sum_{k=1}^{3} p_{k} r_{k}\\}$ scales with the effective number of free parameters in the estimator $\\widetilde{X}$. Our analysis achieves a clean rank-adaptive bias--variance tradeoff: as we increase the ranks of estimator $\\widetilde{X}$, the bias $\\xi(r_{1},r_{2},r_{3})$ decreases and the variance increases. As a byproduct we also obtain a convenient bias-variance decomposition for the vanilla low-rank SVD matrix estimators.",
        "arxiv_id": "2509.17382"
    },
    "2509.18085": {
        "SCORE": 17,
        "ARXIVID": "2509.18085",
        "COMMENT": "ML Systems/Inference Efficiency \u2014 lossless speculative decoding for diffusion LLMs with a parallel-verifiable draft graph and offline calibration; complements KV-caching.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Sudhanshu Agrawal",
            "Risheek Garrepalli",
            "Raghavv Goel",
            "Mingu Lee",
            "Christopher Lott",
            "Fatih Porikli"
        ],
        "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding",
        "abstract": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs (AR-LLMs) with the potential to operate at significantly higher token generation rates. However, currently available open-source dLLMs often generate at much lower rates, typically decoding only a single token at every denoising timestep in order to maximize output quality. We present Spiffy, a speculative decoding algorithm that accelerates dLLM inference by $\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output distribution. This work addresses the unique challenges involved in applying ideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes draft states by leveraging the dLLM's distribution itself in an auto-speculative manner. This approach is efficient and effective, and eliminates the overheads of training and running an independent draft model. To structure the candidate draft states, we propose a novel directed draft graph which is uniquely designed to take advantage of the bidirectional, block-wise nature of dLLM generation and can be verified in parallel by the dLLM. To further optimize the structure of these draft graphs, we introduce an efficient, offline calibration algorithm that procedurally determines high-quality graph configurations. These optimized draft graphs, enabling increased acceptance rates, lead to a significant boost in the overall speedup achieved by the system. Crucially, Spiffy is also complementary to other recent innovations in improving dLLM generation speeds such as KV-caching and multi-token unmasking. We demonstrate that when combined with such parallel decoding algorithms, Spiffy is able to effectively multiply the benefits of these methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
        "arxiv_id": "2509.18085"
    },
    "2509.16213": {
        "SCORE": 17,
        "ARXIVID": "2509.16213",
        "COMMENT": "ML Systems / hardware\u2013software co-design: wafer-scale neuromorphic system with on-wafer GALS NoC, AER-based asynchronous fabric, and hierarchical synchronization enabling low-latency, high-density integration.",
        "RELEVANCE": 8,
        "NOVELTY": 9,
        "authors": [
            "Xiaolei Zhu",
            "Xiaofei Jin",
            "Ziyang Kang",
            "Chonghui Sun",
            "Junjie Feng",
            "Dingwen Hu",
            "Zengyi Wang",
            "Hanyue Zhuang",
            "Qian Zheng",
            "Huajin Tang",
            "Shi Gu",
            "Xin Du",
            "De Ma",
            "Gang Pan"
        ],
        "title": "DarwinWafer: A Wafer-Scale Neuromorphic Chip",
        "abstract": "Neuromorphic computing promises brain-like efficiency, yet today's multi-chip systems scale over PCBs and incur orders-of-magnitude penalties in bandwidth, latency, and energy, undermining biological algorithms and system efficiency. We present DarwinWafer, a hyperscale system-on-wafer that replaces off-chip interconnects with wafer-scale, high-density integration of 64 Darwin3 chiplets on a 300 mm silicon interposer. A GALS NoC within each chiplet and an AER-based asynchronous wafer fabric with hierarchical time-step synchronization provide low-latency, coherent operation across the wafer. Each chiplet implements 2.35 M neurons and 0.1 B synapses, yielding 0.15 B neurons and 6.4 B synapses per wafer.At 333 MHz and 0.8 V, DarwinWafer consumes ~100 W and achieves 4.9 pJ/SOP, with 64 TSOPS peak throughput (0.64 TSOPS/W). Realization is enabled by a holistic chiplet-interposer co-design flow (including an in-house interposer-bump planner with early SI/PI and electro-thermal closure) and a warpage-tolerant assembly that fans out I/O via PCBlets and compliant pogo-pin connections, enabling robust, demountable wafer-to-board integration. Measurements confirm 10 mV supply droop and a uniform thermal profile (34-36 {\\deg}C) under ~100 W. Application studies demonstrate whole-brain simulations: two zebrafish brains per chiplet with high connectivity fidelity (Spearman r = 0.896) and a mouse brain mapped across 32 chiplets (r = 0.645). To our knowledge, DarwinWafer represents a pioneering demonstration of wafer-scale neuromorphic computing, establishing a viable and scalable path toward large-scale, brain-like computation on silicon by replacing PCB-level interconnects with high-density, on-wafer integration.",
        "arxiv_id": "2509.16213"
    },
    "2509.17885": {
        "SCORE": 16,
        "ARXIVID": "2509.17885",
        "COMMENT": "Matches Model Compression and Efficiency: confidence-gated gradient routing for early-exit networks aligning training with inference to reduce compute.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Saad Mokssit",
            "Ouassim Karrakchou",
            "Alejandro Mousist",
            "Mounir Ghogho"
        ],
        "title": "Confidence-gated training for efficient early-exit neural networks",
        "abstract": "Early-exit neural networks reduce inference cost by enabling confident predictions at intermediate layers. However, joint training often leads to gradient interference, with deeper classifiers dominating optimization. We propose Confidence-Gated Training (CGT), a paradigm that conditionally propagates gradients from deeper exits only when preceding exits fail. This encourages shallow classifiers to act as primary decision points while reserving deeper layers for harder inputs. By aligning training with the inference-time policy, CGT mitigates overthinking, improves early-exit accuracy, and preserves efficiency. Experiments on the Indian Pines and Fashion-MNIST benchmarks show that CGT lowers average inference cost while improving overall accuracy, offering a practical solution for deploying deep models in resource-constrained environments.",
        "arxiv_id": "2509.17885"
    },
    "2509.17514": {
        "SCORE": 16,
        "ARXIVID": "2509.17514",
        "COMMENT": "Architecture analysis of Mamba (SSM) revealing limitations due to nonlinear convolution\u2014strong match to Model Architecture and training dynamics criteria.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Tianyi Chen",
            "Pengxiao Lin",
            "Zhiwei Wang",
            "Zhi-Qin John Xu"
        ],
        "title": "Achilles' Heel of Mamba: Essential difficulties of the Mamba architecture demonstrated by synthetic data",
        "abstract": "State Space Models (SSMs) have emerged as promising alternatives to attention mechanisms, with the Mamba architecture demonstrating impressive performance and linear complexity for processing long sequences. However, the fundamental differences between Mamba and Transformer architectures remain incompletely understood. In this work, we use carefully designed synthetic tasks to reveal Mamba's inherent limitations. Through experiments, we identify that Mamba's nonlinear convolution introduces an asymmetry bias that significantly impairs its ability to recognize symmetrical patterns and relationships. Using composite function and inverse sequence matching tasks, we demonstrate that Mamba strongly favors compositional solutions over symmetrical ones and struggles with tasks requiring the matching of reversed sequences. We show these limitations stem not from the SSM module itself but from the nonlinear convolution preceding it, which fuses token information asymmetrically. These insights provide a new understanding of Mamba's constraints and suggest concrete architectural improvements for future sequence models.",
        "arxiv_id": "2509.17514"
    },
    "2509.16278": {
        "SCORE": 16,
        "ARXIVID": "2509.16278",
        "COMMENT": "Model Architecture + Representation Learning \u2014 adds learned meta\u2011tokens with a dedicated meta\u2011attention mechanism acting as content\u2011based landmarks; provides internal analyses and information\u2011theoretic view of long\u2011context compression/caching.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Alok N. Shah",
            "Khush Gupta",
            "Keshav Ramji",
            "Pratik Chaudhari"
        ],
        "title": "Language Modeling with Learned Meta-Tokens",
        "abstract": "While modern Transformer-based language models (LMs) have achieved major success in multi-task generalization, they often struggle to capture long-range dependencies within their context window. This work introduces a novel approach using meta-tokens, special tokens injected during pre-training, along with a dedicated meta-attention mechanism to guide LMs to use these tokens. We pre-train a language model with a modified GPT-2 architecture equipped with meta-attention in addition to causal multi-head attention, and study the impact of these tokens on a suite of synthetic tasks. We find that data-efficient language model pre-training on fewer than 100B tokens utilizing meta-tokens and our meta-attention mechanism achieves strong performance on these tasks after fine-tuning. We suggest that these gains arise due to the meta-tokens sharpening the positional encoding. This enables them to operate as trainable, content-based landmarks, implicitly compressing preceding context and \"caching\" it in the meta-token. At inference-time, the meta-token points to relevant context, facilitating length generalization up to 2$\\times$ its context window, even after extension with YaRN. We provide further evidence of these behaviors by visualizing model internals to study the residual stream, and assessing the compression quality by information-theoretic analysis on the rate-distortion tradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a simple, data-efficient method to enhance long-context language modeling performance, while introducing new insights into the nature of their behavior towards length generalization.",
        "arxiv_id": "2509.16278"
    },
    "2509.16629": {
        "SCORE": 16,
        "ARXIVID": "2509.16629",
        "COMMENT": "Model Architecture \u2014 proposes causality\u2011induced positional encoding: learn DAG over features, hyperbolic embedding, rotary integration with theoretical properties for attention.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Kaichen Xu",
            "Yihang Du",
            "Mianpeng Liu",
            "Zimu Yu",
            "Xiaobo Sun"
        ],
        "title": "Causality-Induced Positional Encoding for Transformer-Based Representation Learning of Non-Sequential Features",
        "abstract": "Positional encoding is essential for supplementing transformer with positional information of tokens. Existing positional encoding methods demand predefined token/feature order, rendering them unsuitable for real-world data with non-sequential yet causally-related features. To address this limitation, we propose CAPE, a novel method that identifies underlying causal structure over non-sequential features as a weighted directed acyclic graph (DAG) using generalized structural equation modeling. The DAG is then embedded in hyperbolic space where its geometric structure is well-preserved using a hyperboloid model-based approach that effectively captures two important causal graph properties (causal strength & causal specificity). This step yields causality-aware positional encodings for the features, which are converted into their rotary form for integrating with transformer's self-attention mechanism. Theoretical analysis reveals that CAPE-generated rotary positional encodings possess three valuable properties for enhanced self-attention, including causal distance-induced attenuation, causal generality-induced attenuation, and robustness to positional disturbances. We evaluate CAPE over both synthetic and real-word datasets, empirically demonstrating its theoretical properties and effectiveness in enhancing transformer for data with non-sequential features. Our code is available at https://github.com/Catchxu/CAPE.",
        "arxiv_id": "2509.16629"
    },
    "2509.16293": {
        "SCORE": 16,
        "ARXIVID": "2509.16293",
        "COMMENT": "ML Systems/HPC \u2014 system for robust large-scale LLM training with fault detection, demarcation, and tolerance; production deployment evidence.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Borui Wan",
            "Gaohong Liu",
            "Zuquan Song",
            "Jun Wang",
            "Yun Zhang",
            "Guangming Sheng",
            "Shuguang Wang",
            "Houmin Wei",
            "Chenyuan Wang",
            "Weiqiang Lou",
            "Xi Yang",
            "Mofan Zhang",
            "Kaihua Jiang",
            "Cheng Ren",
            "Xiaoyun Zhi",
            "Menghan Yu",
            "Zhe Nan",
            "Zhuolin Zheng",
            "Baoquan Zhong",
            "Qinlong Wang",
            "Huan Yu",
            "Jinxin Chi",
            "Wang Zhang",
            "Yuhan Li",
            "Zixian Du",
            "Sida Zhao",
            "Yongqiang Zhang",
            "Jingzhe Tang",
            "Zherui Liu",
            "Chuan Wu",
            "Yanghua Peng",
            "Haibin Lin",
            "Wencong Xiao",
            "Xin Liu",
            "Liang Xiang"
        ],
        "title": "Robust LLM Training Infrastructure at ByteDance",
        "abstract": "The training scale of large language models (LLMs) has reached tens of thousands of GPUs and is still continuously expanding, enabling faster learning of larger models. Accompanying the expansion of the resource scale is the prevalence of failures (CUDA error, NaN values, job hang, etc.), which poses significant challenges to training stability. Any large-scale LLM training infrastructure should strive for minimal training interruption, efficient fault diagnosis, and effective failure tolerance to enable highly efficient continuous training. This paper presents ByteRobust, a large-scale GPU infrastructure management system tailored for robust and stable training of LLMs. It exploits the uniqueness of LLM training process and gives top priorities to detecting and recovering failures in a routine manner. Leveraging parallelisms and characteristics of LLM training, ByteRobust enables high-capacity fault tolerance, prompt fault demarcation, and localization with an effective data-driven approach, comprehensively ensuring continuous and efficient training of LLM tasks. ByteRobust is deployed on a production GPU platform with over 200,000 GPUs and achieves 97% ETTR for a three-month training job on 9,600 GPUs.",
        "arxiv_id": "2509.16293"
    },
    "2509.17304": {
        "SCORE": 16,
        "ARXIVID": "2509.17304",
        "COMMENT": "Optimization/Training Algorithms: proposes a variance-reduced stochastic method (SPRINT) for performative prediction with O(1/T) convergence and variance-independent error neighborhood.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Tian Xie",
            "Ding Zhu",
            "Jia Liu",
            "Mahdi Khalili",
            "Xueru Zhang"
        ],
        "title": "SPRINT: Stochastic Performative Prediction With Variance Reduction",
        "abstract": "Performative prediction (PP) is an algorithmic framework for optimizing machine learning (ML) models where the model's deployment affects the distribution of the data it is trained on. Compared to traditional ML with fixed data, designing algorithms in PP converging to a stable point -- known as a stationary performative stable (SPS) solution -- is more challenging than the counterpart in conventional ML tasks due to the model-induced distribution shifts. While considerable efforts have been made to find SPS solutions using methods such as repeated gradient descent (RGD) and greedy stochastic gradient descent (SGD-GD), most prior studies assumed a strongly convex loss until a recent work established $\\mathcal{O}(1/\\sqrt{T})$ convergence of SGD-GD to SPS solutions under smooth, non-convex losses. However, this latest progress is still based on the restricted bounded variance assumption in stochastic gradient estimates and yields convergence bounds with a non-vanishing error neighborhood that scales with the variance. This limitation motivates us to improve convergence rates and reduce error in stochastic optimization for PP, particularly in non-convex settings. Thus, we propose a new algorithm called stochastic performative prediction with variance reduction (SPRINT) and establish its convergence to an SPS solution at a rate of $\\mathcal{O}(1/T)$. Notably, the resulting error neighborhood is **independent** of the variance of the stochastic gradients. Experiments on multiple real datasets with non-convex models demonstrate that SPRINT outperforms SGD-GD in both convergence rate and stability.",
        "arxiv_id": "2509.17304"
    },
    "2509.17552": {
        "SCORE": 16,
        "ARXIVID": "2509.17552",
        "COMMENT": "Representation Learning: training-free integration of non-text foundation model embeddings into LLMs via in-context representation learning (no fine-tuning).",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Tianle Zhang",
            "Wanlong Fang",
            "Jonathan Woo",
            "Paridhi Latawa",
            "Deepak A. Subramanian",
            "Alvin Chan"
        ],
        "title": "Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning",
        "abstract": "The remarkable performance of Large Language Models (LLMs) can be enhanced with test-time computation, which relies on external tools and even other deep learning models. However, existing approaches for integrating non-text modality representations into LLMs typically require additional costly supervised training, restricting on-the-fly adaptation to new domains and modalities. In this work, we explore the feasibility of integrating representations from non-text foundational models (FMs) into text-based LLMs in a training-free manner. We propose In-Context Representation Learning (ICRL) as a proof-of-concept to allow LLMs to adaptively utilize non-text modality representations with few-shot learning. Unlike traditional in-context learning, which incorporates text-label pairs, ICRL replaces text inputs with FM representations, enabling the LLM to perform multi-modal inference without fine-tuning. We evaluate ICRL on a suite of tasks in the molecular domain, investigating three core research questions: (i) how to map FM representations into LLMs in a training-free manner, (ii) what factors influence ICRL performance, and (iii) what mechanisms underlie the effectiveness of ICRL. To the best of our knowledge, ICRL is the first training-free framework for integrating non-text modality representations into text-based LLMs, presenting a promising direction for adaptable, multi-modal generalization.",
        "arxiv_id": "2509.17552"
    },
    "2509.16842": {
        "SCORE": 16,
        "ARXIVID": "2509.16842",
        "COMMENT": "Representation Learning / training objective: doubly robust generative modeling (DoubleGen) with finite-sample guarantees; applies to diffusion, flow matching, and autoregressive LMs.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Alex Luedtke",
            "Kenji Fukumizu"
        ],
        "title": "DoubleGen: Debiased Generative Modeling of Counterfactuals",
        "abstract": "Generative models for counterfactual outcomes face two key sources of bias. Confounding bias arises when approaches fail to account for systematic differences between those who receive the intervention and those who do not. Misspecification bias arises when methods attempt to address confounding through estimation of an auxiliary model, but specify it incorrectly. We introduce DoubleGen, a doubly robust framework that modifies generative modeling training objectives to mitigate these biases. The new objectives rely on two auxiliaries -- a propensity and outcome model -- and successfully address confounding bias even if only one of them is correct. We provide finite-sample guarantees for this robustness property. We further establish conditions under which DoubleGen achieves oracle optimality -- matching the convergence rates standard approaches would enjoy if interventional data were available -- and minimax rate optimality. We illustrate DoubleGen with three examples: diffusion models, flow matching, and autoregressive language models.",
        "arxiv_id": "2509.16842"
    },
    "2509.17276": {
        "SCORE": 15,
        "ARXIVID": "2509.17276",
        "COMMENT": "Model Architecture/ML Systems: introduces optimal-transport-based probabilistic token alignment to fuse heterogeneous LLM vocabularies\u2014general, distribution-aware fusion mechanism enabling cross-architecture model merging.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Runjia Zeng",
            "James Chenhao Liang",
            "Cheng Han",
            "Zhiwen Cao",
            "Jiahao Liu",
            "Xiaojun Quan",
            "Yingjie Victor Chen",
            "Lifu Huang",
            "Tong Geng",
            "Qifan Wang",
            "Dongfang Liu"
        ],
        "title": "Probabilistic Token Alignment for Large Language Model Fusion",
        "abstract": "Training large language models (LLMs) from scratch can yield models with unique functionalities and strengths, but it is costly and often leads to redundant capabilities. A more cost-effective alternative is to fuse existing pre-trained LLMs with different architectures into a more powerful model. However, a key challenge in existing model fusion is their dependence on manually predefined vocabulary alignment, which may not generalize well across diverse contexts, leading to performance degradation in several evaluation. To solve this, we draw inspiration from distribution learning and propose the probabilistic token alignment method as a general and soft mapping for alignment, named as PTA-LLM. Our approach innovatively reformulates token alignment into a classic mathematical problem: optimal transport, seamlessly leveraging distribution-aware learning to facilitate more coherent model fusion. Apart from its inherent generality, PTA-LLM exhibits interpretability from a distributional perspective, offering insights into the essence of the token alignment. Empirical results demonstrate that probabilistic token alignment enhances the target model's performance across multiple capabilities. Our code is avaliable at https://runjia.tech/neurips_pta-llm/.",
        "arxiv_id": "2509.17276"
    },
    "2509.16664": {
        "SCORE": 15,
        "ARXIVID": "2509.16664",
        "COMMENT": "Representation Learning: proposes \u03bb-orthogonality regularization to learn compatibility-preserving affine mappings across independently trained representations\u2014principled alignment beyond strict orthogonality.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Simone Ricci",
            "Niccol\\`o Biondi",
            "Federico Pernici",
            "Ioannis Patras",
            "Alberto Del Bimbo"
        ],
        "title": "$\\boldsymbol{\\lambda}$-Orthogonality Regularization for Compatible Representation Learning",
        "abstract": "Retrieval systems rely on representations learned by increasingly powerful models. However, due to the high training cost and inconsistencies in learned representations, there is significant interest in facilitating communication between representations and ensuring compatibility across independently trained neural networks. In the literature, two primary approaches are commonly used to adapt different learned representations: affine transformations, which adapt well to specific distributions but can significantly alter the original representation, and orthogonal transformations, which preserve the original structure with strict geometric constraints but limit adaptability. A key challenge is adapting the latent spaces of updated models to align with those of previous models on downstream distributions while preserving the newly learned representation spaces. In this paper, we impose a relaxed orthogonality constraint, namely $\\lambda$-orthogonality regularization, while learning an affine transformation, to obtain distribution-specific adaptation while retaining the original learned representations. Extensive experiments across various architectures and datasets validate our approach, demonstrating that it preserves the model's zero-shot performance and ensures compatibility across model updates. Code available at: https://github.com/miccunifi/lambda_orthogonality",
        "arxiv_id": "2509.16664"
    },
    "2509.18001": {
        "SCORE": 15,
        "ARXIVID": "2509.18001",
        "COMMENT": "Representation Learning/Training Dynamics: analyzes SAM via SDE and stochastic gradient noise structure; introduces Reweighted SAM mimicking m-SAM benefits with parallelizable sampling.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Haocheng Luo",
            "Mehrtash Harandi",
            "Dinh Phung",
            "Trung Le"
        ],
        "title": "Unveiling m-Sharpness Through the Structure of Stochastic Gradient Noise",
        "abstract": "Sharpness-aware minimization (SAM) has emerged as a highly effective technique for improving model generalization, but its underlying principles are not fully understood. We investigated the phenomenon known as m-sharpness, where the performance of SAM improves monotonically as the micro-batch size for computing perturbations decreases. Leveraging an extended Stochastic Differential Equation (SDE) framework, combined with an analysis of the structure of stochastic gradient noise (SGN), we precisely characterize the dynamics of various SAM variants. Our findings reveal that the stochastic noise introduced during SAM perturbations inherently induces a variance-based sharpness regularization effect. Motivated by our theoretical insights, we introduce Reweighted SAM, which employs sharpness-weighted sampling to mimic the generalization benefits of m-SAM while remaining parallelizable. Comprehensive experiments validate the effectiveness of our theoretical analysis and proposed method.",
        "arxiv_id": "2509.18001"
    },
    "2509.16596": {
        "SCORE": 15,
        "ARXIVID": "2509.16596",
        "COMMENT": "Representation Learning/Training Dynamics: token- and parameter-level analysis of SFT\u2019s impact on knowledge; identifies non-contributing parameter updates and provides guidance for fine-tuning strategies.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Junjie Ye",
            "Yuming Yang",
            "Yang Nan",
            "Shuo Li",
            "Qi Zhang",
            "Tao Gui",
            "Xuanjing Huang",
            "Peng Wang",
            "Zhongchao Shi",
            "Jianping Fan"
        ],
        "title": "Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels",
        "abstract": "Large language models (LLMs) acquire substantial world knowledge during pre-training, which is further shaped by post-training techniques such as supervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge remains underexplored, limiting our ability to control knowledge change behavior in fine-tuned models. To address this gap, we evaluate closed-book question answering (CBQA) performance across five LLMs from the LLaMA-2 and LLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up to 14% worse than those fine-tuned on only 240 samples. Furthermore, varying the level of knowledge mastery in the fine-tuning data leads to performance fluctuations of over 12%. To investigate these effects, we analyze model behavior at both the token and parameter levels. Our analysis reveals that up to 90% of parameter updates during SFT do not contribute to knowledge enhancement. Restoring these updates can improve performance on the CBQA task, depending on the characteristics of the fine-tuning data. These insights offer practical guidance for developing fine-tuning strategies that more effectively strengthen model knowledge.",
        "arxiv_id": "2509.16596"
    },
    "2509.17153": {
        "SCORE": 15,
        "ARXIVID": "2509.17153",
        "COMMENT": "Matches Model Compression and Efficiency: uncertainty-aware compression via compact inducing weight matrix with normalizing-flow priors and spectral regularization; provides theoretical bounds and storage reduction.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Moule Lin",
            "Andrea Patane",
            "Weipeng Jing",
            "Shuhao Guan",
            "Goetz Botterweck"
        ],
        "title": "Flow-Induced Diagonal Gaussian Processes",
        "abstract": "We present Flow-Induced Diagonal Gaussian Processes (FiD-GP), a compression framework that incorporates a compact inducing weight matrix to project a neural network's weight uncertainty into a lower-dimensional subspace. Critically, FiD-GP relies on normalising-flow priors and spectral regularisations to augment its expressiveness and align the inducing subspace with feature-gradient geometry through a numerically stable projection mechanism objective. Furthermore, we demonstrate how the prediction framework in FiD-GP can help to design a single-pass projection for Out-of-Distribution (OoD) detection. Our analysis shows that FiD-GP improves uncertainty estimation ability on various tasks compared with SVGP-based baselines, satisfies tight spectral residual bounds with theoretically guaranteed OoD detection, and significantly compresses the neural network's storage requirements at the cost of increased inference computation dependent on the number of inducing weights employed. Specifically, in a comprehensive empirical study spanning regression, image classification, semantic segmentation, and out-of-distribution detection benchmarks, it cuts Bayesian training cost by several orders of magnitude, compresses parameters by roughly 51%, reduces model size by about 75%, and matches state-of-the-art accuracy and uncertainty estimation.",
        "arxiv_id": "2509.17153"
    },
    "2509.17943": {
        "SCORE": 15,
        "ARXIVID": "2509.17943",
        "COMMENT": "Matches Representation Learning: theoretical and empirical analysis of contrastive alignment\u2019s information loss and preservation of modality-specific information.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Romain Thoreau",
            "Jessie Levillain",
            "Dawa Derksen"
        ],
        "title": "Can multimodal representation learning by alignment preserve modality-specific information?",
        "abstract": "Combining multimodal data is a key issue in a wide range of machine learning tasks, including many remote sensing problems. In Earth observation, early multimodal data fusion methods were based on specific neural network architectures and supervised learning. Ever since, the scarcity of labeled data has motivated self-supervised learning techniques. State-of-the-art multimodal representation learning techniques leverage the spatial alignment between satellite data from different modalities acquired over the same geographic area in order to foster a semantic alignment in the latent space. In this paper, we investigate how this methods can preserve task-relevant information that is not shared across modalities. First, we show, under simplifying assumptions, when alignment strategies fundamentally lead to an information loss. Then, we support our theoretical insight through numerical experiments in more realistic settings. With those theoretical and empirical evidences, we hope to support new developments in contrastive learning for the combination of multimodal satellite data. Our code and data is publicly available at https://github.com/Romain3Ch216/alg_maclean_25.",
        "arxiv_id": "2509.17943"
    },
    "2509.16395": {
        "SCORE": 15,
        "ARXIVID": "2509.16395",
        "COMMENT": "Compression/Efficiency: low-rank parameter-evolution subspace for EDNNs, reducing trainable parameters and compute for PDE solvers.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jiahao Zhang",
            "Shiheng Zhang",
            "Guang Lin"
        ],
        "title": "Low-Rank Adaptation of Evolutionary Deep Neural Networks for Efficient Learning of Time-Dependent PDEs",
        "abstract": "We study the Evolutionary Deep Neural Network (EDNN) framework for accelerating numerical solvers of time-dependent partial differential equations (PDEs). We introduce a Low-Rank Evolutionary Deep Neural Network (LR-EDNN), which constrains parameter evolution to a low-rank subspace, thereby reducing the effective dimensionality of training while preserving solution accuracy. The low-rank tangent subspace is defined layer-wise by the singular value decomposition (SVD) of the current network weights, and the resulting update is obtained by solving a well-posed, tractable linear system within this subspace. This design augments the underlying numerical solver with a parameter efficient EDNN component without requiring full fine-tuning of all network weights. We evaluate LR-EDNN on representative PDE problems and compare it against corresponding baselines. Across cases, LR-EDNN achieves comparable accuracy with substantially fewer trainable parameters and reduced computational cost. These results indicate that low-rank constraints on parameter velocities, rather than full-space updates, provide a practical path toward scalable, efficient, and reproducible scientific machine learning for PDEs.",
        "arxiv_id": "2509.16395"
    },
    "2509.17401": {
        "SCORE": 15,
        "ARXIVID": "2509.17401",
        "COMMENT": "Representation Learning \u2014 mechanistic interpretability of ViTs via sparse autoencoders and a residual replacement model yielding faithful, simplified circuits.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jinyeong Kim",
            "Junhyeok Kim",
            "Yumin Shim",
            "Joohyeok Kim",
            "Sunyoung Jung",
            "Seong Jae Hwang"
        ],
        "title": "Interpreting vision transformers via residual replacement model",
        "abstract": "How do vision transformers (ViTs) represent and process the world? This paper addresses this long-standing question through the first systematic analysis of 6.6K features across all layers, extracted via sparse autoencoders, and by introducing the residual replacement model, which replaces ViT computations with interpretable features in the residual stream. Our analysis reveals not only a feature evolution from low-level patterns to high-level semantics, but also how ViTs encode curves and spatial positions through specialized feature types. The residual replacement model scalably produces a faithful yet parsimonious circuit for human-scale interpretability by significantly simplifying the original computations. As a result, this framework enables intuitive understanding of ViT mechanisms. Finally, we demonstrate the utility of our framework in debiasing spurious correlations.",
        "arxiv_id": "2509.17401"
    },
    "2509.16598": {
        "SCORE": 15,
        "ARXIVID": "2509.16598",
        "COMMENT": "Model Compression and Efficiency \u2014 uses layer pruning to construct an 'amateur' model for contrastive decoding, improving factuality with minimal inference overhead.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Byeongho Yu",
            "Changhun Lee",
            "Jungyu Jin",
            "Eunhyeok Park"
        ],
        "title": "PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality",
        "abstract": "To mitigate the hallucination problem in large language models, DoLa exploits early exit logits from the same model as a contrastive prior. However, we found that these early exit logits tend to be flat, low in magnitude, and fail to reflect meaningful contrasts. To address this, we propose PruneCD, a novel contrastive decoding method that constructs the amateur model via layer pruning rather than early exit. This design leads to more informative and well-aligned logits, enabling more effective contrastive decoding. Through qualitative and quantitative analyses, we demonstrate that PruneCD consistently improves factuality with minimal inference overhead, offering a robust and practical approach to mitigating hallucinations in LLMs.",
        "arxiv_id": "2509.16598"
    },
    "2509.17543": {
        "SCORE": 15,
        "ARXIVID": "2509.17543",
        "COMMENT": "Model Compression and Efficiency \u2014 dataset/distribution compression across samples and dimensions via MMD-based latent projection with linear time/memory complexity.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Dominic Broadbent",
            "Nick Whiteley",
            "Robert Allison",
            "Tom Lovett"
        ],
        "title": "Bilateral Distribution Compression: Reducing Both Data Size and Dimensionality",
        "abstract": "Existing distribution compression methods reduce dataset size by minimising the Maximum Mean Discrepancy (MMD) between original and compressed sets, but modern datasets are often large in both sample size and dimensionality. We propose Bilateral Distribution Compression (BDC), a two-stage framework that compresses along both axes while preserving the underlying distribution, with overall linear time and memory complexity in dataset size and dimension. Central to BDC is the Decoded MMD (DMMD), which quantifies the discrepancy between the original data and a compressed set decoded from a low-dimensional latent space. BDC proceeds by (i) learning a low-dimensional projection using the Reconstruction MMD (RMMD), and (ii) optimising a latent compressed set with the Encoded MMD (EMMD). We show that this procedure minimises the DMMD, guaranteeing that the compressed set faithfully represents the original distribution. Experiments show that across a variety of scenarios BDC can achieve comparable or superior performance to ambient-space compression at substantially lower cost.",
        "arxiv_id": "2509.17543"
    },
    "2509.16268": {
        "SCORE": 15,
        "ARXIVID": "2509.16268",
        "COMMENT": "Representation Learning / training dynamics: causal layer- and token-level interventions to analyze how function calling alters internal computation in LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Zhenlan Ji",
            "Daoyuan Wu",
            "Wenxuan Wang",
            "Pingchuan Ma",
            "Shuai Wang",
            "Lei Ma"
        ],
        "title": "Digging Into the Internal: Causality-Based Analysis of LLM Function Calling",
        "abstract": "Function calling (FC) has emerged as a powerful technique for facilitating large language models (LLMs) to interact with external systems and perform structured tasks. However, the mechanisms through which it influences model behavior remain largely under-explored. Besides, we discover that in addition to the regular usage of FC, this technique can substantially enhance the compliance of LLMs with user instructions. These observations motivate us to leverage causality, a canonical analysis method, to investigate how FC works within LLMs. In particular, we conduct layer-level and token-level causal interventions to dissect FC's impact on the model's internal computational logic when responding to user queries. Our analysis confirms the substantial influence of FC and reveals several in-depth insights into its mechanisms. To further validate our findings, we conduct extensive experiments comparing the effectiveness of FC-based instructions against conventional prompting methods. We focus on enhancing LLM safety robustness, a critical LLM application scenario, and evaluate four mainstream LLMs across two benchmark datasets. The results are striking: FC shows an average performance improvement of around 135% over conventional prompting methods in detecting malicious inputs, demonstrating its promising potential to enhance LLM reliability and capability in practical applications.",
        "arxiv_id": "2509.16268"
    },
    "2509.16554": {
        "SCORE": 15,
        "ARXIVID": "2509.16554",
        "COMMENT": "Model Architecture \u2014 ViT autoencoder with class-token global latent prior and convergence-aware attention head pruning for efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Vahid Jebraeeli",
            "Hamid Krim",
            "Derya Cansever"
        ],
        "title": "ViTCAE: ViT-based Class-conditioned Autoencoder",
        "abstract": "Vision Transformer (ViT) based autoencoders often underutilize the global Class token and employ static attention mechanisms, limiting both generative control and optimization efficiency. This paper introduces ViTCAE, a framework that addresses these issues by re-purposing the Class token into a generative linchpin. In our architecture, the encoder maps the Class token to a global latent variable that dictates the prior distribution for local, patch-level latent variables, establishing a robust dependency where global semantics directly inform the synthesis of local details. Drawing inspiration from opinion dynamics, we treat each attention head as a dynamical system of interacting tokens seeking consensus. This perspective motivates a convergence-aware temperature scheduler that adaptively anneals each head's influence function based on its distributional stability. This process enables a principled head-freezing mechanism, guided by theoretically-grounded diagnostics like an attention evolution distance and a consensus/cluster functional. This technique prunes converged heads during training to significantly improve computational efficiency without sacrificing fidelity. By unifying a generative Class token with an adaptive attention mechanism rooted in multi-agent consensus theory, ViTCAE offers a more efficient and controllable approach to transformer-based generation.",
        "arxiv_id": "2509.16554"
    },
    "2509.17251": {
        "SCORE": 15,
        "ARXIVID": "2509.17251",
        "COMMENT": "Training dynamics/implicit regularization: instance-wise finite-sample risk comparisons showing when GD dominates ridge and SGD.",
        "RELEVANCE": 7,
        "NOVELTY": 8,
        "authors": [
            "Jingfeng Wu",
            "Peter L. Bartlett",
            "Jason D. Lee",
            "Sham M. Kakade",
            "Bin Yu"
        ],
        "title": "Risk Comparisons in Linear Regression: Implicit Regularization Dominates Explicit Regularization",
        "abstract": "Existing theory suggests that for linear regression problems categorized by capacity and source conditions, gradient descent (GD) is always minimax optimal, while both ridge regression and online stochastic gradient descent (SGD) are polynomially suboptimal for certain categories of such problems. Moving beyond minimax theory, this work provides instance-wise comparisons of the finite-sample risks for these algorithms on any well-specified linear regression problem.   Our analysis yields three key findings. First, GD dominates ridge regression: with comparable regularization, the excess risk of GD is always within a constant factor of ridge, but ridge can be polynomially worse even when tuned optimally. Second, GD is incomparable with SGD. While it is known that for certain problems GD can be polynomially better than SGD, the reverse is also true: we construct problems, inspired by benign overfitting theory, where optimally stopped GD is polynomially worse. Finally, GD dominates SGD for a significant subclass of problems -- those with fast and continuously decaying covariance spectra -- which includes all problems satisfying the standard capacity condition.",
        "arxiv_id": "2509.17251"
    },
    "2509.17774": {
        "SCORE": 15,
        "ARXIVID": "2509.17774",
        "COMMENT": "Model Architecture analysis and algorithmic efficiency: polynomial-time methods for predictive equivalence and related DT tasks, avoiding exponential DNF minimization.",
        "RELEVANCE": 7,
        "NOVELTY": 8,
        "authors": [
            "Joao Marques-Silva",
            "Alexey Ignatiev"
        ],
        "title": "Efficient & Correct Predictive Equivalence for Decision Trees",
        "abstract": "The Rashomon set of decision trees (DTs) finds importance uses. Recent work showed that DTs computing the same classification function, i.e. predictive equivalent DTs, can represent a significant fraction of the Rashomon set. Such redundancy is undesirable. For example, feature importance based on the Rashomon set becomes inaccurate due the existence of predictive equivalent DTs, i.e. DTs with the same prediction for every possible input. In recent work, McTavish et al. proposed solutions for several computational problems related with DTs, including that of deciding predictive equivalent DTs. This approach, which this paper refers to as MBDSR, consists of applying the well-known method of Quine-McCluskey (QM) for obtaining minimum-size DNF (disjunctive normal form) representations of DTs, which are then used for comparing DTs for predictive equivalence. Furthermore, the minimum-size DNF representation was also applied to computing explanations for the predictions made by DTs, and to finding predictions in the presence of missing data. However, the problem of formula minimization is hard for the second level of the polynomial hierarchy, and the QM method may exhibit worst-case exponential running time and space. This paper first demonstrates that there exist decision trees that trigger the worst-case exponential running time and space of the QM method. Second, the paper shows that the MBDSR approach can produce incorrect results for the problem of deciding predictive equivalence. Third, the paper shows that any of the problems to which the minimum-size DNF representation has been applied to can in fact be solved in polynomial time, in the size of the DT. The experiments confirm that, for DTs for which the the worst-case of the QM method is triggered, the algorithms proposed in this paper are orders of magnitude faster than the ones proposed by McTavish et al.",
        "arxiv_id": "2509.17774"
    },
    "2509.16769": {
        "SCORE": 14,
        "ARXIVID": "2509.16769",
        "COMMENT": "Model Architecture: proposes a discriminative per-class mixture of hyperplanes with soft-OR (log-sum-exp) aggregation\u2014an interpretable mixture-style classifier with linear-time inference.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Prasanth K K",
            "Shubham Sharma"
        ],
        "title": "Geometric Mixture Classifier (GMC): A Discriminative Per-Class Mixture of Hyperplanes",
        "abstract": "Many real world categories are multimodal, with single classes occupying disjoint regions in feature space. Classical linear models (logistic regression, linear SVM) use a single global hyperplane and perform poorly on such data, while high-capacity methods (kernel SVMs, deep nets) fit multimodal structure but at the expense of interpretability, heavier tuning, and higher computational cost. We propose the Geometric Mixture Classifier (GMC), a discriminative model that represents each class as a mixture of hyperplanes. Within each class, GMC combines plane scores via a temperature-controlled soft-OR (log-sum-exp), smoothly approximating the max; across classes, standard softmax yields probabilistic posteriors. GMC optionally uses Random Fourier Features (RFF) for nonlinear mappings while keeping inference linear in the number of planes and features. Our practical training recipe: geometry-aware k-means initialization, silhouette-based plane budgeting, alpha annealing, usage-aware L2 regularization, label smoothing, and early stopping, makes GMC plug-and-play. Across synthetic multimodal datasets (moons, circles, blobs, spirals) and tabular/image benchmarks (iris, wine, WDBC, digits), GMC consistently outperforms linear baselines and k-NN, is competitive with RBF-SVM, Random Forests, and small MLPs, and provides geometric introspection via per-plane and class responsibility visualizations. Inference scales linearly in planes and features, making GMC CPU-friendly, with single-digit microsecond latency per example, often faster than RBF-SVM and compact MLPs. Post-hoc temperature scaling reduces ECE from about 0.06 to 0.02. GMC thus strikes a favorable balance of accuracy, interpretability, and efficiency: it is more expressive than linear models and lighter, more transparent, and faster than kernel or deep models.",
        "arxiv_id": "2509.16769"
    },
    "2509.16959": {
        "SCORE": 14,
        "ARXIVID": "2509.16959",
        "COMMENT": "Training Dynamics: introduces a general gradient interference-aware scheduling algorithm via graph coloring to improve multi-task learning optimization.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Santosh Patapati",
            "Trisanth Srinivasan"
        ],
        "title": "Gradient Interference-Aware Graph Coloring for Multitask Learning",
        "abstract": "When different objectives conflict with each other in multi-task learning, gradients begin to interfere and slow convergence, thereby reducing the final model's performance. To address this, we introduce a scheduler that computes gradient interference, constructs an interference graph, and then applies greedy graph-coloring to partition tasks into groups that align well with each other. At each training step, only one group (color class) of tasks are activated. The grouping partition is constantly recomputed as task relationships evolve throughout training. By ensuring that each mini-batch contains only tasks that pull the model in the same direction, our method improves the effectiveness of any underlying multi-task learning optimizer without additional tuning. Since tasks within these groups will update in compatible directions, model performance will be improved rather than impeded. Empirical results on six different datasets show that this interference-aware graph-coloring approach consistently outperforms baselines and state-of-the-art multi-task optimizers.",
        "arxiv_id": "2509.16959"
    },
    "2509.17207": {
        "SCORE": 14,
        "ARXIVID": "2509.17207",
        "COMMENT": "Model Architecture / Representation Learning: proposes Replaced Token Denoising with a discriminator\u2013generator scheme for pretraining point-cloud transformers, improving robustness and efficiency.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Gunner Stone",
            "Youngsook Choi",
            "Alireza Tavakkoli",
            "Ankita Shukla"
        ],
        "title": "Point-RTD: Replaced Token Denoising for Pretraining Transformer Models on Point Clouds",
        "abstract": "Pre-training strategies play a critical role in advancing the performance of transformer-based models for 3D point cloud tasks. In this paper, we introduce Point-RTD (Replaced Token Denoising), a novel pretraining strategy designed to improve token robustness through a corruption-reconstruction framework. Unlike traditional mask-based reconstruction tasks that hide data segments for later prediction, Point-RTD corrupts point cloud tokens and leverages a discriminator-generator architecture for denoising. This shift enables more effective learning of structural priors and significantly enhances model performance and efficiency. On the ShapeNet dataset, Point-RTD reduces reconstruction error by over 93% compared to PointMAE, and achieves more than 14x lower Chamfer Distance on the test set. Our method also converges faster and yields higher classification accuracy on ShapeNet, ModelNet10, and ModelNet40 benchmarks, clearly outperforming the baseline Point-MAE framework in every case.",
        "arxiv_id": "2509.17207"
    },
    "2509.17636": {
        "SCORE": 14,
        "ARXIVID": "2509.17636",
        "COMMENT": "Matches Representation Learning/Theory: corrected whitening for spherical GMMs in high-dim regime using random matrix theory to restore orthogonality.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Mohammed Racim Moussa Boudjemaa",
            "Alper Kalle",
            "Xiaoyi Mai",
            "Jos\\'e Henrique de Morais Goulart",
            "C\\'edric F\\'evotte"
        ],
        "title": "Whitening Spherical Gaussian Mixtures in the Large-Dimensional Regime",
        "abstract": "Whitening is a classical technique in unsupervised learning that can facilitate estimation tasks by standardizing data. An important application is the estimation of latent variable models via the decomposition of tensors built from high-order moments. In particular, whitening orthogonalizes the means of a spherical Gaussian mixture model (GMM), thereby making the corresponding moment tensor orthogonally decomposable, hence easier to decompose. However, in the large-dimensional regime (LDR) where data are high-dimensional and scarce, the standard whitening matrix built from the sample covariance becomes ineffective because the latter is spectrally distorted. Consequently, whitened means of a spherical GMM are no longer orthogonal. Using random matrix theory, we derive exact limits for their dot products, which are generally nonzero in the LDR. As our main contribution, we then construct a corrected whitening matrix that restores asymptotic orthogonality, allowing for performance gains in spherical GMM estimation.",
        "arxiv_id": "2509.17636"
    },
    "2509.16411": {
        "SCORE": 14,
        "ARXIVID": "2509.16411",
        "COMMENT": "Representation Learning: provides theoretical geometry/expressivity results for dual-encoder embeddings in hierarchical retrieval and a pretrain\u2013finetune training recipe addressing long-distance retrieval behavior.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Chong You",
            "Rajesh Jayaram",
            "Ananda Theertha Suresh",
            "Robin Nittka",
            "Felix Yu",
            "Sanjiv Kumar"
        ],
        "title": "Hierarchical Retrieval: The Geometry and a Pretrain-Finetune Recipe",
        "abstract": "Dual encoder (DE) models, where a pair of matching query and document are embedded into similar vector representations, are widely used in information retrieval due to their simplicity and scalability. However, the Euclidean geometry of the embedding space limits the expressive power of DEs, which may compromise their quality. This paper investigates such limitations in the context of hierarchical retrieval (HR), where the document set has a hierarchical structure and the matching documents for a query are all of its ancestors. We first prove that DEs are feasible for HR as long as the embedding dimension is linear in the depth of the hierarchy and logarithmic in the number of documents. Then we study the problem of learning such embeddings in a standard retrieval setup where DEs are trained on samples of matching query and document pairs. Our experiments reveal a lost-in-the-long-distance phenomenon, where retrieval accuracy degrades for documents further away in the hierarchy. To address this, we introduce a pretrain-finetune recipe that significantly improves long-distance retrieval without sacrificing performance on closer documents. We experiment on a realistic hierarchy from WordNet for retrieving documents at various levels of abstraction, and show that pretrain-finetune boosts the recall on long-distance pairs from 19% to 76%. Finally, we demonstrate that our method improves retrieval of relevant products on a shopping queries dataset.",
        "arxiv_id": "2509.16411"
    },
    "2509.17186": {
        "SCORE": 14,
        "ARXIVID": "2509.17186",
        "COMMENT": "Matches Model Architecture/Efficiency: novel dendritic resonate-and-fire neuron with adaptive thresholds yielding sparse spiking for long-sequence modeling.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Dehao Zhang",
            "Malu Zhang",
            "Shuai Wang",
            "Jingya Wang",
            "Wenjie Wei",
            "Zeyu Ma",
            "Guoqing Wang",
            "Yang Yang",
            "HaiZhou Li"
        ],
        "title": "Dendritic Resonate-and-Fire Neuron for Effective and Efficient Long Sequence Modeling",
        "abstract": "The explosive growth in sequence length has intensified the demand for effective and efficient long sequence modeling. Benefiting from intrinsic oscillatory membrane dynamics, Resonate-and-Fire (RF) neurons can efficiently extract frequency components from input signals and encode them into spatiotemporal spike trains, making them well-suited for long sequence modeling. However, RF neurons exhibit limited effective memory capacity and a trade-off between energy efficiency and training speed on complex temporal tasks. Inspired by the dendritic structure of biological neurons, we propose a Dendritic Resonate-and-Fire (D-RF) model, which explicitly incorporates a multi-dendritic and soma architecture. Each dendritic branch encodes specific frequency bands by utilizing the intrinsic oscillatory dynamics of RF neurons, thereby collectively achieving comprehensive frequency representation. Furthermore, we introduce an adaptive threshold mechanism into the soma structure that adjusts the threshold based on historical spiking activity, reducing redundant spikes while maintaining training efficiency in long sequence tasks. Extensive experiments demonstrate that our method maintains competitive accuracy while substantially ensuring sparse spikes without compromising computational efficiency during training. These results underscore its potential as an effective and efficient solution for long sequence modeling on edge platforms.",
        "arxiv_id": "2509.17186"
    },
    "2509.16277": {
        "SCORE": 14,
        "ARXIVID": "2509.16277",
        "COMMENT": "Matches Representation Learning/Training Dynamics: information-theoretic regularizer enforcing smooth mutual information and entropy decay across layers for stable compression.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Haobo Yang",
            "Shiyan Zhang",
            "Zhuoyi Yang",
            "Jilong Guo",
            "Jun Yang",
            "Xinyu Zhang"
        ],
        "title": "Stabilizing Information Flow Entropy: Regularization for Safe and Interpretable Autonomous Driving Perception",
        "abstract": "Deep perception networks in autonomous driving traditionally rely on data-intensive training regimes and post-hoc anomaly detection, often disregarding fundamental information-theoretic constraints governing stable information processing. We reconceptualize deep neural encoders as hierarchical communication chains that incrementally compress raw sensory inputs into task-relevant latent features. Within this framework, we establish two theoretically justified design principles for robust perception: (D1) smooth variation of mutual information between consecutive layers, and (D2) monotonic decay of latent entropy with network depth. Our analysis shows that, under realistic architectural assumptions, particularly blocks comprising repeated layers of similar capacity, enforcing smooth information flow (D1) naturally encourages entropy decay (D2), thus ensuring stable compression. Guided by these insights, we propose Eloss, a novel entropy-based regularizer designed as a lightweight, plug-and-play training objective. Rather than marginal accuracy improvements, this approach represents a conceptual shift: it unifies information-theoretic stability with standard perception tasks, enabling explicit, principled detection of anomalous sensor inputs through entropy deviations. Experimental validation on large-scale 3D object detection benchmarks (KITTI and nuScenes) demonstrates that incorporating Eloss consistently achieves competitive or improved accuracy while dramatically enhancing sensitivity to anomalies, amplifying distribution-shift signals by up to two orders of magnitude. This stable information-compression perspective not only improves interpretability but also establishes a solid theoretical foundation for safer, more robust autonomous driving perception systems.",
        "arxiv_id": "2509.16277"
    },
    "2509.17530": {
        "SCORE": 14,
        "ARXIVID": "2509.17530",
        "COMMENT": "Model Architecture \u2014 uses a hypernetwork to generate task-specific weights and a data-free unlearning mechanism (parameter-noise alignment) for continual learning.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Sayanta Adhikari",
            "Vishnuprasadh Kumaravelu",
            "P. K. Srijith"
        ],
        "title": "An Unlearning Framework for Continual Learning",
        "abstract": "Growing concerns surrounding AI safety and data privacy have driven the development of Machine Unlearning as a potential solution. However, current machine unlearning algorithms are designed to complement the offline training paradigm. The emergence of the Continual Learning (CL) paradigm promises incremental model updates, enabling models to learn new tasks sequentially. Naturally, some of those tasks may need to be unlearned to address safety or privacy concerns that might arise. We find that applying conventional unlearning algorithms in continual learning environments creates two critical problems: performance degradation on retained tasks and task relapse, where previously unlearned tasks resurface during subsequent learning. Furthermore, most unlearning algorithms require data to operate, which conflicts with CL's philosophy of discarding past data. A clear need arises for unlearning algorithms that are data-free and mindful of future learning. To that end, we propose UnCLe, an Unlearning framework for Continual Learning. UnCLe employs a hypernetwork that learns to generate task-specific network parameters, using task embeddings. Tasks are unlearned by aligning the corresponding generated network parameters with noise, without requiring any data. Empirical evaluations on several vision data sets demonstrate UnCLe's ability to sequentially perform multiple learning and unlearning operations with minimal disruption to previously acquired knowledge.",
        "arxiv_id": "2509.17530"
    },
    "2509.17755": {
        "SCORE": 14,
        "ARXIVID": "2509.17755",
        "COMMENT": "Model Architecture and Representation Learning \u2014 introduces neural modules to learn repeated antiderivatives, enabling continuous cumulative operators in neural fields.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Fizza Rubab",
            "Ntumba Elie Nsampi",
            "Martin Balint",
            "Felix Mujkanovic",
            "Hans-Peter Seidel",
            "Tobias Ritschel",
            "Thomas Leimk\\\"uhler"
        ],
        "title": "Learning Neural Antiderivatives",
        "abstract": "Neural fields offer continuous, learnable representations that extend beyond traditional discrete formats in visual computing. We study the problem of learning neural representations of repeated antiderivatives directly from a function, a continuous analogue of summed-area tables. Although widely used in discrete domains, such cumulative schemes rely on grids, which prevents their applicability in continuous neural contexts. We introduce and analyze a range of neural methods for repeated integration, including both adaptations of prior work and novel designs. Our evaluation spans multiple input dimensionalities and integration orders, assessing both reconstruction quality and performance in downstream tasks such as filtering and rendering. These results enable integrating classical cumulative operators into modern neural systems and offer insights into learning tasks involving differential and integral operators.",
        "arxiv_id": "2509.17755"
    },
    "2509.17291": {
        "SCORE": 14,
        "ARXIVID": "2509.17291",
        "COMMENT": "Model Architecture: interpretable graph generation by separating pattern learning via random-walk trajectories from graph construction through joint optimization.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Rahul Nandakumar",
            "Deepayan Chakrabarti"
        ],
        "title": "GraphWeave: Interpretable and Robust Graph Generation via Random Walk Trajectories",
        "abstract": "Given a set of graphs from some unknown family, we want to generate new graphs from that family. Recent methods use diffusion on either graph embeddings or the discrete space of nodes and edges. However, simple changes to embeddings (say, adding noise) can mean uninterpretable changes in the graph. In discrete-space diffusion, each step may add or remove many nodes/edges. It is hard to predict what graph patterns we will observe after many diffusion steps. Our proposed method, called GraphWeave, takes a different approach. We separate pattern generation and graph construction. To find patterns in the training graphs, we see how they transform vectors during random walks. We then generate new graphs in two steps. First, we generate realistic random walk \"trajectories\" which match the learned patterns. Then, we find the optimal graph that fits these trajectories. The optimization infers all edges jointly, which improves robustness to errors. On four simulated and five real-world benchmark datasets, GraphWeave outperforms existing methods. The most significant differences are on large-scale graph structures such as PageRank, cuts, communities, degree distributions, and flows. GraphWeave is also 10x faster than its closest competitor. Finally, GraphWeave is simple, needing only a transformer and standard optimizers.",
        "arxiv_id": "2509.17291"
    },
    "2509.17990": {
        "SCORE": 14,
        "ARXIVID": "2509.17990",
        "COMMENT": "Representation Learning \u2014 learns dynamics that preserve a target distribution from unordered snapshots; analyzes inductive biases; includes a training-free high-dimensional variant.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Yanbo Zhang",
            "Michael Levin"
        ],
        "title": "Equilibrium flow: From Snapshots to Dynamics",
        "abstract": "Scientific data, from cellular snapshots in biology to celestial distributions in cosmology, often consists of static patterns from underlying dynamical systems. These snapshots, while lacking temporal ordering, implicitly encode the processes that preserve them. This work investigates how strongly such a distribution constrains its underlying dynamics and how to recover them. We introduce the Equilibrium flow method, a framework that learns continuous dynamics that preserve a given pattern distribution. Our method successfully identifies plausible dynamics for 2-D systems and recovers the signature chaotic behavior of the Lorenz attractor. For high-dimensional Turing patterns from the Gray-Scott model, we develop an efficient, training-free variant that achieves high fidelity to the ground truth, validated both quantitatively and qualitatively. Our analysis reveals the solution space is constrained not only by the data but also by the learning model's inductive biases. This capability extends beyond recovering known systems, enabling a new paradigm of inverse design for Artificial Life. By specifying a target pattern distribution, we can discover the local interaction rules that preserve it, leading to the spontaneous emergence of complex behaviors, such as life-like flocking, attraction, and repulsion patterns, from simple, user-defined snapshots.",
        "arxiv_id": "2509.17990"
    },
    "2509.16547": {
        "SCORE": 14,
        "ARXIVID": "2509.16547",
        "COMMENT": "Representation Learning/Theory \u2014 formal verification and complexity of checking consistency/applicability/exhaustiveness of rules extracted from ReLU/Boolean networks.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Adrian Wurm"
        ],
        "title": "Checking extracted rules in Neural Networks",
        "abstract": "In this paper we investigate formal verification of extracted rules for Neural Networks under a complexity theoretic point of view. A rule is a global property or a pattern concerning a large portion of the input space of a network. These rules are algorithmically extracted from networks in an effort to better understand their inner way of working. Here, three problems will be in the focus: Does a given set of rules apply to a given network? Is a given set of rules consistent or do the rules contradict themselves? Is a given set of rules exhaustive in the sense that for every input the output is determined? Finding algorithms that extract such rules out of networks has been investigated over the last 30 years, however, to the author's current knowledge, no attempt in verification was made until now. A lot of attempts of extracting rules use heuristics involving randomness and over-approximation, so it might be beneficial to know whether knowledge obtained in that way can actually be trusted.   We investigate the above questions for neural networks with ReLU-activation as well as for Boolean networks, each for several types of rules. We demonstrate how these problems can be reduced to each other and show that most of them are co-NP-complete.",
        "arxiv_id": "2509.16547"
    },
    "2509.16931": {
        "SCORE": 14,
        "ARXIVID": "2509.16931",
        "COMMENT": "Model Compression and Efficiency: residual quantization to approximate attention for low-latency pre-ranking with improved accuracy\u2013efficiency trade-off.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Yutong Li",
            "Yu Zhu",
            "Yichen Qiao",
            "Ziyu Guan",
            "Lv Shao",
            "Tong Liu",
            "Bo Zheng"
        ],
        "title": "Equip Pre-ranking with Target Attention by Residual Quantization",
        "abstract": "The pre-ranking stage in industrial recommendation systems faces a fundamental conflict between efficiency and effectiveness. While powerful models like Target Attention (TA) excel at capturing complex feature interactions in the ranking stage, their high computational cost makes them infeasible for pre-ranking, which often relies on simplistic vector-product models. This disparity creates a significant performance bottleneck for the entire system. To bridge this gap, we propose TARQ, a novel pre-ranking framework. Inspired by generative models, TARQ's key innovation is to equip pre-ranking with an architecture approximate to TA by Residual Quantization. This allows us to bring the modeling power of TA into the latency-critical pre-ranking stage for the first time, establishing a new state-of-the-art trade-off between accuracy and efficiency. Extensive offline experiments and large-scale online A/B tests at Taobao demonstrate TARQ's significant improvements in ranking performance. Consequently, our model has been fully deployed in production, serving tens of millions of daily active users and yielding substantial business improvements.",
        "arxiv_id": "2509.16931"
    }
}