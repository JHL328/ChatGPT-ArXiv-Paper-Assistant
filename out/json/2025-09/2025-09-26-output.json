{
    "2509.21271": {
        "SCORE": 19,
        "ARXIVID": "2509.21271",
        "COMMENT": "Matches ML Systems/HPC: heterogeneous CPU\u2013GPU co-design and offloading (adaptive weight offload, repartitioning, casting, speculative exec, optimized Adam) with strong throughput gains on Superchips; extends to ZeRO and sequence parallelism.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Xinyu Lian",
            "Masahiro Tanaka",
            "Olatunji Ruwase",
            "Minjia Zhang"
        ],
        "title": "SuperOffload: Unleashing the Power of Large-Scale LLM Training on Superchips",
        "abstract": "The emergence of Superchips represents a significant advancement in next-generation AI hardware. These Superchips employ a tightly coupled heterogeneous architecture that integrates GPU and CPU on the same package, which offers unprecedented computational power. However, there has been scant research investigating how LLM training benefits from this new architecture. In this work, for the first time, we study LLM training solutions based on offloading for Superchips. We observe important differences between Superchips and traditional loosely-coupled GPU-CPU architecture, which necessitate revisiting prevailing assumptions about offloading. Based on that, we present SuperOffload, a Superchip-centric offloading system that simultaneously uses Hopper GPU, Grace CPU, and NVLink-C2C interconnect more efficiently. SuperOffload accomplishes this via a combination of techniques, such as adaptive weight offloading, bucketization repartitioning, Superchip-aware casting, speculative execution, and a highly optimized Adam optimizer for Grace CPUs. Our evaluation of SuperOffload on NVIDIA GH200 demonstrates up to 2.5x throughput improvement compared to state-of-the-art offloading-based systems, enabling training of up to 25B model on a single Superchip while achieving high training throughput. We also extend SuperOffload with ZeRO-style data parallelism and DeepSpeed-Ulysses sequence parallelism, enabling training of 13B model with sequence lengths up to 1 million tokens on 8 GH200 while achieving 55% MFU.",
        "arxiv_id": "2509.21271"
    },
    "2509.20784": {
        "SCORE": 19,
        "ARXIVID": "2509.20784",
        "COMMENT": "Matches Representation Learning and Autoencoders: defines atomic units of LLM representations with RIP/uniqueness guarantees and shows SAEs can recover them, advancing mechanistic interpretability.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Chenhui Hu",
            "Pengfei Cao",
            "Yubo Chen",
            "Kang Liu",
            "Jun Zhao"
        ],
        "title": "Towards Atoms of Large Language Models",
        "abstract": "The fundamental units of internal representations in large language models (LLMs) remain undefined, limiting further understanding of their mechanisms. Neurons or features are often regarded as such units, yet neurons suffer from polysemy, while features face concerns of unreliable reconstruction and instability. To address this issue, we propose the Atoms Theory, which defines such units as atoms. We introduce the atomic inner product (AIP) to correct representation shifting, formally define atoms, and prove the conditions that atoms satisfy the Restricted Isometry Property (RIP), ensuring stable sparse representations over atom set and linking to compressed sensing. Under stronger conditions, we further establish the uniqueness and exact $\\ell_1$ recoverability of the sparse representations, and provide guarantees that single-layer sparse autoencoders (SAEs) with threshold activations can reliably identify the atoms. To validate the Atoms Theory, we train threshold-activated SAEs on Gemma2-2B, Gemma2-9B, and Llama3.1-8B, achieving 99.9% sparse reconstruction across layers on average, and more than 99.8% of atoms satisfy the uniqueness condition, compared to 0.5% for neurons and 68.2% for features, showing that atoms more faithfully capture intrinsic representations of LLMs. Scaling experiments further reveal the link between SAEs size and recovery capacity. Overall, this work systematically introduces and validates Atoms Theory of LLMs, providing a theoretical framework for understanding internal representations and a foundation for mechanistic interpretability. Code available at https://github.com/ChenhuiHu/towards_atoms.",
        "arxiv_id": "2509.20784"
    },
    "2509.21181": {
        "SCORE": 19,
        "ARXIVID": "2509.21181",
        "COMMENT": "Theory/Representation Learning: closed-form scaling laws for ||w_p||_r under \u2113_p bias with elbow/threshold; extends to DLNs linking explicit and implicit bias.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Shuofeng Zhang",
            "Ard Louis"
        ],
        "title": "Closed-form $\\ell_r$ norm scaling with data for overparameterized linear regression and diagonal linear networks under $\\ell_p$ bias",
        "abstract": "For overparameterized linear regression with isotropic Gaussian design and minimum-$\\ell_p$ interpolator $p\\in(1,2]$, we give a unified, high-probability characterization for the scaling of the family of parameter norms $ \\\\{ \\lVert \\widehat{w_p} \\rVert_r \\\\}_{r \\in [1,p]} $ with sample size.   We solve this basic, but unresolved question through a simple dual-ray analysis, which reveals a competition between a signal *spike* and a *bulk* of null coordinates in $X^\\top Y$, yielding closed-form predictions for (i) a data-dependent transition $n_\\star$ (the \"elbow\"), and (ii) a universal threshold $r_\\star=2(p-1)$ that separates $\\lVert \\widehat{w_p} \\rVert_r$'s which plateau from those that continue to grow with an explicit exponent.   This unified solution resolves the scaling of *all* $\\ell_r$ norms within the family $r\\in [1,p]$ under $\\ell_p$-biased interpolation, and explains in one picture which norms saturate and which increase as $n$ grows.   We then study diagonal linear networks (DLNs) trained by gradient descent. By calibrating the initialization scale $\\alpha$ to an effective $p_{\\mathrm{eff}}(\\alpha)$ via the DLN separable potential, we show empirically that DLNs inherit the same elbow/threshold laws, providing a predictive bridge between explicit and implicit bias.   Given that many generalization proxies depend on $\\lVert \\widehat {w_p} \\rVert_r$, our results suggest that their predictive power will depend sensitively on which $l_r$ norm is used.",
        "arxiv_id": "2509.21181"
    },
    "2509.21081": {
        "SCORE": 18,
        "ARXIVID": "2509.21081",
        "COMMENT": "ML Systems: kernel-level optimization for MLA attention combining naive/absorb to exploit shared prefixes with large throughput gains on GPUs/NPUs.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Ahmet Caner Y\\\"uz\\\"ug\\\"uler",
            "Ahmet \\c{C}elik",
            "Jiawei Zhuang",
            "Lukas Cavigelli"
        ],
        "title": "TyphoonMLA: A Mixed Naive-Absorb MLA Kernel For Shared Prefix",
        "abstract": "Multi-Head Latent Attention (MLA) is a recent attention mechanism adopted in state-of-the-art LLMs such as DeepSeek-v3 and Kimi K2. Thanks to its novel formulation, MLA allows two functionally equivalent but computationally distinct kernel implementations: naive and absorb. While the naive kernels (e.g., FlashAttention) are typically preferred in training and prefill for their computational efficiency, existing decoding kernels (e.g., FlashMLA) rely on the absorb method to minimize HBM bandwidth usage. However, the compute-bound nature of the absorb implementations prohibits performance benefits from data reuse opportunities in attention calculations, such as shared prefixes. In this work, we introduce TyphoonMLA, a hybrid approach that combines naive and absorb formulations to harness the strengths of both. TyphoonMLA effectively leverages the shared prefix by applying the naive formulation to the compute-bound parts of attention calculations, while reducing the bandwidth requirements for non-shared parts by using the absorb formulation. As a result, TyphoonMLA improves the throughput of attention calculations in MLA architectures by up to 3x and 3.24x on NPU and GPUs, with only a 3% overhead in HBM size.",
        "arxiv_id": "2509.21081"
    },
    "2509.21164": {
        "SCORE": 18,
        "ARXIVID": "2509.21164",
        "COMMENT": "Strong match to Model Architecture: MoE-like latent collaboration via cross-attention among heterogeneous LLM experts with a learned router and joint objective enabling single-pass multi-expert inference.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Jacob Fein-Ashley",
            "Dhruv Parikh",
            "Rajgopal Kannan",
            "Viktor Prasanna"
        ],
        "title": "Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just What They Say",
        "abstract": "Open-source Large Language Models (LLMs) increasingly specialize by domain (e.g., math, code, general reasoning), motivating systems that leverage complementary strengths across models. Prior multi-LLM approaches either (i) route a query to one or a few experts and generate independently, (ii) aggregate outputs from each model via costly multi-turn exchanges, or (iii) fuse weights into a single model-typically requiring architectural homogeneity. We introduce Mixture of Thoughts (MoT), a simple method for latent-level collaboration among heterogeneous experts under a global routing scheme. For each query, a lightweight router selects top-$K$ experts and designates a primary expert; uniformly placed interaction layers project hidden states into a shared latent space where the primary expert performs cross-attention over its active (selected) peers. Pre-trained experts remain frozen; only the router and the lightweight interaction layers are trained with a novel joint training objective that improves both the expert selection and inter-expert collaboration. Across five in-distribution (ID) and three out-of-distribution (OOD) benchmarks, MoT surpasses the current routing and aggregation-based state-of-the-art, Avengers, by $+0.38\\%$ and $+2.92\\%$, respectively. Further, MoT significantly outperforms the best-performing single model. It achieves this with single-pass inference, runtime comparable to routing baselines, and none of the overheads of iterative aggregation. MoT offers a simple latent-space mechanism for combining heterogeneous LLMs, a practical step toward broader multi-LLM collaboration. Our code is publicly available at https://github.com/jacobfa/mot.",
        "arxiv_id": "2509.21164"
    },
    "2509.21275": {
        "SCORE": 18,
        "ARXIVID": "2509.21275",
        "COMMENT": "ML Systems: introduces Elastic Pipeline Parallelism combining token- and batch-level PP with resource-aware splitting and stage-aware adaptive checkpointing for distributed long-context LLM training.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Shiju Wang",
            "Yujie Wang",
            "Ao Sun",
            "Fangcheng Fu",
            "Zijian Zhu",
            "Bin Cui",
            "Xu Han",
            "Kaisheng Ma"
        ],
        "title": "Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM Training",
        "abstract": "Long context training is crucial for LLM's context extension. Existing schemes, such as sequence parallelism, incur substantial communication overhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness hinges on partitioning granularity. Batch-level PP dividing input samples exhibits high memory consumption in long-context scenario, whereas token-level PP splitting sequences into slices alleviates memory overhead but may incur hardware under-utilization. This trade-off motivates adaptively selecting PP granularity to match resource and workload characteristics. Moreover, sequence length distribution of the real-world dataset exhibits skewness, posing a challenge on PP's workload balance and efficient scheduling. Current static PP scheduling methods overlook the variance of sequence length, leading to suboptimal performance. In this paper, we propose Elastic Pipeline Parallelism (EPP) that orchestrates token-level PP and batch-level PP to adapt to resource and workload heterogeneity. We build InfiniPipe, a distributed training system that unleashes the potential of EPP via (1) a resource-aware and workload-balanced sequence processor that splits long sequences and packs short ones; and (2) a co-optimization methodology that jointly optimizes pipeline schedule and gradient checkpointing via a mechanism named stage-aware chunk-level adaptive checkpointing. Comprehensive experiments demonstrate that InfiniPipe achieves a 1.69x speedup over state-of-the-art systems.",
        "arxiv_id": "2509.21275"
    },
    "2509.20503": {
        "SCORE": 18,
        "ARXIVID": "2509.20503",
        "COMMENT": "Model Architecture & Efficiency: new attention-like layer achieving better scaling via efficient inversion of tree-structured matrices (combines sparsity and recurrent structure).",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Evgenii Egorov",
            "Hanno Ackermann",
            "Markus Nagel",
            "Hong Cai"
        ],
        "title": "Myosotis: structured computation for attention like layer",
        "abstract": "Attention layers apply a sequence-to-sequence mapping whose parameters depend on the pairwise interactions of the input elements. However, without any structural assumptions, memory and compute scale quadratically with the sequence length. The two main ways to mitigate this are to introduce sparsity by ignoring a sufficient amount of pairwise interactions or to introduce recurrent dependence along them, as SSM does. Although both approaches are reasonable, they both have disadvantages. We propose a novel algorithm that combines the advantages of both concepts. Our idea is based on the efficient inversion of tree-structured matrices.",
        "arxiv_id": "2509.20503"
    },
    "2509.20581": {
        "SCORE": 18,
        "ARXIVID": "2509.20581",
        "COMMENT": "Model Architecture and Efficiency: introduces multi-resolution attention with hierarchical sequence reduction achieving O(n log n) complexity.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Ayan Sar",
            "Sampurna Roy",
            "Kanav Gupta",
            "Anurag Kaushish",
            "Tanupriya Choudhury",
            "Abhijit Kumar"
        ],
        "title": "Hierarchical Resolution Transformers: A Wavelet-Inspired Architecture for Multi-Scale Language Understanding",
        "abstract": "Transformer architectures have achieved state-of-the-art performance across natural language tasks, yet they fundamentally misrepresent the hierarchical nature of human language by processing text as flat token sequences. This results in quadratic computational cost, weak computational cost, weak compositional generalization, and inadequate discourse-level modeling. We propose Hierarchical Resolution Transformer (HRT), a novel wavelet-inspired neural architecture that processes language simultaneously across multiple resolutions, from characters to discourse-level units. HRT constructs a multi-resolution attention, enabling bottom-up composition and top-down contextualization. By employing exponential sequence reduction across scales, HRT achieves O(nlogn) complexity, offering significant efficiency improvements over standard transformers. We evaluated HRT on a diverse suite of benchmarks, including GLUE, SuperGLUE, Long Range Arena, and WikiText-103, and results demonstrated that HRT outperforms standard transformer baselines by an average of +3.8% on GLUE, +4.5% on SuperGLUE, and +6.1% on Long Range Arena, while reducing memory usage by 42% and inference latency by 37% compared to BERT and GPT style models of similar parameter count. Ablation studies confirm the effectiveness of cross-resolution attention and scale-specialized modules, showing that each contributes independently to both efficiency and accuracy. Our findings establish HRT as the first architecture to align computational structure with the hierarchical organization of human language, demonstrating that multi-scale, wavelet-inspired processing yields both theoretical efficiency gains and practical improvements in language understanding.",
        "arxiv_id": "2509.20581"
    },
    "2509.20829": {
        "SCORE": 18,
        "ARXIVID": "2509.20829",
        "COMMENT": "Representation Learning/Training Dynamics: unified explanation of grokking and information bottleneck via neural collapse geometry and time-scale analysis.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Keitaro Sakamoto",
            "Issei Sato"
        ],
        "title": "Explaining Grokking and Information Bottleneck through Neural Collapse Emergence",
        "abstract": "The training dynamics of deep neural networks often defy expectations, even as these models form the foundation of modern machine learning. Two prominent examples are grokking, where test performance improves abruptly long after the training loss has plateaued, and the information bottleneck principle, where models progressively discard input information irrelevant to the prediction task as training proceeds. However, the mechanisms underlying these phenomena and their relations remain poorly understood. In this work, we present a unified explanation of such late-phase phenomena through the lens of neural collapse, which characterizes the geometry of learned representations. We show that the contraction of population within-class variance is a key factor underlying both grokking and information bottleneck, and relate this measure to the neural collapse measure defined on the training set. By analyzing the dynamics of neural collapse, we show that distinct time scales between fitting the training set and the progression of neural collapse account for the behavior of the late-phase phenomena. Finally, we validate our theoretical findings on multiple datasets and architectures.",
        "arxiv_id": "2509.20829"
    },
    "2509.20721": {
        "SCORE": 18,
        "ARXIVID": "2509.20721",
        "COMMENT": "Representation Learning/Training Dynamics: theoretical link between scaling exponents and data covariance spectra (redundancy).",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Yuda Bi",
            "Vince D Calhoun"
        ],
        "title": "Scaling Laws are Redundancy Laws",
        "abstract": "Scaling laws, a defining feature of deep learning, reveal a striking power-law improvement in model performance with increasing dataset and model size. Yet, their mathematical origins, especially the scaling exponent, have remained elusive. In this work, we show that scaling laws can be formally explained as redundancy laws. Using kernel regression, we show that a polynomial tail in the data covariance spectrum yields an excess risk power law with exponent alpha = 2s / (2s + 1/beta), where beta controls the spectral tail and 1/beta measures redundancy. This reveals that the learning curve's slope is not universal but depends on data redundancy, with steeper spectra accelerating returns to scale. We establish the law's universality across boundedly invertible transformations, multi-modal mixtures, finite-width approximations, and Transformer architectures in both linearized (NTK) and feature-learning regimes. This work delivers the first rigorous mathematical explanation of scaling laws as finite-sample redundancy laws, unifying empirical observations with theoretical foundations.",
        "arxiv_id": "2509.20721"
    },
    "2509.20577": {
        "SCORE": 17,
        "ARXIVID": "2509.20577",
        "COMMENT": "Model Architecture: depth-specialized Mixture-of-Experts with learned routing for conditional computation across layers, improving efficiency.",
        "RELEVANCE": 10,
        "NOVELTY": 7,
        "authors": [
            "Sampurna Roy",
            "Ayan Sar",
            "Anurag Kaushish",
            "Kanav Gupta",
            "Tanupriya Choudhury",
            "Abhijit Kumar"
        ],
        "title": "Dynamic Reasoning Chains through Depth-Specialized Mixture-of-Experts in Transformer Architectures",
        "abstract": "Contemporary transformer architectures apply identical processing depth to all inputs, creating inefficiencies and limiting reasoning quality. Simple factual queries are subjected to the same multilayered computation as complex logical problems, wasting resources while constraining deep inference. To overcome this, we came up with a concept of Dynamic Reasoning Chains through Depth Specialised Mixture of Experts (DS-MoE), a modular framework that extends the Mixture of Experts paradigm from width-based to depth specialised computation. DS-MoE introduces expert modules optimised for distinct reasoning depths, shallow pattern recognition, compositional reasoning, logical inference, memory integration, and meta-cognitive supervision. A learned routing network dynamically assembles custom reasoning chains, activating only the necessary experts to match input complexity. The dataset on which we trained and evaluated DS-MoE is on The Pile, an 800GB corpus covering diverse domains such as scientific papers, legal texts, programming code, and web content, enabling systematic assessment across reasoning depths. Experimental results demonstrate that DS-MoE achieves up to 16 per cent computational savings and 35 per cent faster inference compared to uniform-depth transformers, while delivering 2.8 per cent higher accuracy on complex multi-step reasoning benchmarks. Furthermore, routing decisions yield interpretable reasoning chains, enhancing transparency and scalability. These findings establish DS-MoE as a significant advancement in adaptive neural architectures, demonstrating that depth-specialised modular processing can simultaneously improve efficiency, reasoning quality, and interpretability in large-scale language models.",
        "arxiv_id": "2509.20577"
    },
    "2509.21221": {
        "SCORE": 17,
        "ARXIVID": "2509.21221",
        "COMMENT": "ML Systems: decentralized, churn-tolerant distributed training with a new flow-based routing algorithm and large-scale evaluation.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Nikolay Blagoev",
            "Bart Cox",
            "J\\'er\\'emie Decouchant",
            "Lydia Y. Chen"
        ],
        "title": "Go With The Flow: Churn-Tolerant Decentralized Training of Large Language Models",
        "abstract": "Motivated by the emergence of large language models (LLMs) and the importance of democratizing their training, we propose GWTF, the first crash tolerant practical decentralized training framework for LLMs. Differently from existing distributed and federated training frameworks, GWTF enables the efficient collaborative training of a LLM on heterogeneous clients that volunteer their resources. In addition, GWTF addresses node churn, i.e., clients joining or leaving the system at any time, and network instabilities, i.e., network links becoming unstable or unreliable. The core of GWTF is a novel decentralized flow algorithm that finds the most effective routing that maximizes the number of microbatches trained with the lowest possible delay. We extensively evaluate GWTF on GPT-like and LLaMa-like models and compare it against the prior art. Our results indicate that GWTF reduces the training time by up to 45% in realistic and challenging scenarios that involve heterogeneous client nodes distributed over 10 different geographic locations with a high node churn rate.",
        "arxiv_id": "2509.21221"
    },
    "2509.20599": {
        "SCORE": 17,
        "ARXIVID": "2509.20599",
        "COMMENT": "HPC/Training memory optimization: near-reversible Runge\u2013Kutta schemes enabling memory-efficient, accurate backprop through neural SDEs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Daniil Shmelev",
            "Cristopher Salvi"
        ],
        "title": "Explicit and Effectively Symmetric Schemes for Neural SDEs",
        "abstract": "Backpropagation through (neural) SDE solvers is traditionally approached in two ways: discretise-then-optimise, which offers accurate gradients but incurs prohibitive memory costs due to storing the full computational graph (even when mitigated by checkpointing); and optimise-then-discretise, which achieves constant memory cost by solving an auxiliary backward SDE, but suffers from slower evaluation and gradient approximation errors. Algebraically reversible solvers promise both memory efficiency and gradient accuracy, yet existing methods such as the Reversible Heun scheme are often unstable under complex models and large step sizes. We address these limitations by introducing a novel class of stable, near-reversible Runge--Kutta schemes for neural SDEs. These Explicit and Effectively Symmetric (EES) schemes retain the benefits of reversible solvers while overcoming their instability, enabling memory-efficient training without severe restrictions on step size or model complexity. Through numerical experiments, we demonstrate the superior stability and reliability of our schemes, establishing them as a practical foundation for scalable and accurate training of neural SDEs.",
        "arxiv_id": "2509.20599"
    },
    "2509.20605": {
        "SCORE": 17,
        "ARXIVID": "2509.20605",
        "COMMENT": "Representation learning + efficiency: learns compact neural basis functions with kernel-theoretic analysis, progressive growth/pruning algorithms, and generalization bounds.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Su Ann Low",
            "Quentin Rommel",
            "Kevin S. Miller",
            "Adam J. Thorpe",
            "Ufuk Topcu"
        ],
        "title": "Function Spaces Without Kernels: Learning Compact Hilbert Space Representations",
        "abstract": "Function encoders are a recent technique that learn neural network basis functions to form compact, adaptive representations of Hilbert spaces of functions. We show that function encoders provide a principled connection to feature learning and kernel methods by defining a kernel through an inner product of the learned feature map. This kernel-theoretic perspective explains their ability to scale independently of dataset size while adapting to the intrinsic structure of data, and it enables kernel-style analysis of neural models. Building on this foundation, we develop two training algorithms that learn compact bases: a progressive training approach that constructively grows bases, and a train-then-prune approach that offers a computationally efficient alternative after training. Both approaches use principles from PCA to reveal the intrinsic dimension of the learned space. In parallel, we derive finite-sample generalization bounds using Rademacher complexity and PAC-Bayes techniques, providing inference time guarantees. We validate our approach on a polynomial benchmark with a known intrinsic dimension, and on nonlinear dynamical systems including a Van der Pol oscillator and a two-body orbital model, demonstrating that the same accuracy can be achieved with substantially fewer basis functions. This work suggests a path toward neural predictors with kernel-level guarantees, enabling adaptable models that are both efficient and principled at scale.",
        "arxiv_id": "2509.20605"
    },
    "2509.20997": {
        "SCORE": 17,
        "ARXIVID": "2509.20997",
        "COMMENT": "Model Architecture and Representation Learning: introduces a binary autoencoder with minibatch entropy minimization (via STE) to yield sparse, independent features for mechanistic interpretability.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Hakaze Cho",
            "Haolin Yang",
            "Brian M. Kurkoski",
            "Naoya Inoue"
        ],
        "title": "Binary Autoencoder for Mechanistic Interpretability of Large Language Models",
        "abstract": "Existing works are dedicated to untangling atomized numerical components (features) from the hidden states of Large Language Models (LLMs) for interpreting their mechanism. However, they typically rely on autoencoders constrained by some implicit training-time regularization on single training instances (i.e., $L_1$ normalization, top-k function, etc.), without an explicit guarantee of global sparsity among instances, causing a large amount of dense (simultaneously inactive) features, harming the feature sparsity and atomization. In this paper, we propose a novel autoencoder variant that enforces minimal entropy on minibatches of hidden activations, thereby promoting feature independence and sparsity across instances. For efficient entropy calculation, we discretize the hidden activations to 1-bit via a step function and apply gradient estimation to enable backpropagation, so that we term it as Binary Autoencoder (BAE) and empirically demonstrate two major applications: (1) Feature set entropy calculation. Entropy can be reliably estimated on binary hidden activations, which we empirically evaluate and leverage to characterize the inference dynamics of LLMs and In-context Learning. (2) Feature untangling. Similar to typical methods, BAE can extract atomized features from LLM's hidden states. To robustly evaluate such feature extraction capability, we refine traditional feature-interpretation methods to avoid unreliable handling of numerical tokens, and show that BAE avoids dense features while producing the largest number of interpretable ones among baselines, which confirms the effectiveness of BAE serving as a feature extractor.",
        "arxiv_id": "2509.20997"
    },
    "2509.20789": {
        "SCORE": 17,
        "ARXIVID": "2509.20789",
        "COMMENT": "Representation Learning/Training Dynamics: formalizes SSM inductive bias via an induced kernel and introduces task-dependent initialization by spectrum matching to improve data efficiency.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Qiyu Chen",
            "Guozhang Chen"
        ],
        "title": "Aligning Inductive Bias for Data-Efficient Generalization in State Space Models",
        "abstract": "The remarkable success of large-scale models is fundamentally tied to scaling laws, yet the finite nature of high-quality data presents a looming challenge. One of the next frontiers in modeling is data efficiency: the ability to learn more from less. A model's inductive bias is a critical lever for this, but foundational sequence models like State Space Models (SSMs) rely on a fixed bias. This fixed prior is sample-inefficient when a task's underlying structure does not match. In this work, we introduce a principled framework to solve this problem. We first formalize the inductive bias of linear time-invariant SSMs through an SSM-induced kernel, mathematically and empirically proving its spectrum is directly governed by the model's frequency response. Further, we propose a method of Task-Dependent Initialization (TDI): power spectrum matching, a fast and efficient method that aligns the model's inductive bias with the task's spectral characteristics before large-scale training. Our experiments on a diverse set of real-world benchmarks show that TDI significantly improves generalization and sample efficiency, particularly in low-data regimes. This work provides a theoretical and practical tool to create more data-efficient models, a crucial step towards sustainable scaling.",
        "arxiv_id": "2509.20789"
    },
    "2509.21153": {
        "SCORE": 17,
        "ARXIVID": "2509.21153",
        "COMMENT": "Model Compression/Efficiency and ML Systems: wavelet-based tokenization enabling adaptive-resolution inference with KV caching and early exits for compute\u2013accuracy trade-offs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Moshe Kimhi",
            "Erez Koifman",
            "Ehud Rivlin",
            "Eli Schwartz",
            "Chaim Baskin"
        ],
        "title": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP",
        "abstract": "We introduce WAVECLIP, a single unified model for adaptive resolution inference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces standard patch embeddings with a multi-level wavelet decomposition, enabling the model to process images coarse to fine while naturally supporting multiple resolutions within the same model. At inference time, the model begins with low resolution tokens and refines only when needed, using key-value caching and causal cross-level attention to reuse computation, effectively introducing to the model only new information when needed. We evaluate WAVECLIP in zero-shot classification, demonstrating that a simple confidence-based gating mechanism enables adaptive early exits. This allows users to dynamically choose a compute-accuracy trade-off using a single deployed model. Our approach requires only lightweight distillation from a frozen CLIP teacher and achieves competitive accuracy with significant computational savings.",
        "arxiv_id": "2509.21153"
    },
    "2509.20979": {
        "SCORE": 17,
        "ARXIVID": "2509.20979",
        "COMMENT": "ML Systems: learning-augmented GPU caching (embeddings and KV-cache) with adaptive LARU algorithm and robustness to prediction errors; improves inference throughput/TTFT.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Peng Chen",
            "Jiaji Zhang",
            "Hailiang Zhao",
            "Yirong Zhang",
            "Jiahong Yu",
            "Xueyan Tang",
            "Yixuan Wang",
            "Hao Li",
            "Jianping Zou",
            "Gang Xiong",
            "Kingsum Chow",
            "Shuibing He",
            "Shuiguang Deng"
        ],
        "title": "Toward Robust and Efficient ML-Based GPU Caching for Modern Inference",
        "abstract": "In modern GPU inference, cache efficiency remains a major bottleneck. In recommendation models, embedding hit rates largely determine throughput, while in large language models, KV-cache misses substantially increase time-to-first-token (TTFT). Heuristic policies such as \\textsc{LRU} often struggle under structured access patterns. Learning-based approaches are promising, but in practice face two major limitations: they degrade sharply when predictions are inaccurate, or they gain little even with accurate predictions due to conservative designs. Some also incur high overhead, further limiting practicality.   We present \\textsc{LCR}, a practical framework for learning-based GPU caching that delivers performance gains while ensuring robustness and efficiency. Its core algorithm, \\textsc{LARU}, enhances \\textsc{LRU} with machine-learned predictions and dynamically adapts to prediction accuracy through online error estimation. When predictions are accurate, \\textsc{LARU} achieves near-optimal performance. With inaccurate predictions, it degrades gracefully to near-\\textsc{LRU} performance. With \\textsc{LCR}, we bridge the gap between empirical progress and theoretical advances in learning-based caching.   Experiments show that \\textsc{LCR} delivers consistent gains under realistic conditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\\% and reduces P99 TTFT by up to 28.3\\%, outperforming widely used inference systems. Even under poor predictions, its performance remains stable, demonstrating practical robustness.",
        "arxiv_id": "2509.20979"
    },
    "2509.21042": {
        "SCORE": 17,
        "ARXIVID": "2509.21042",
        "COMMENT": "Matches Representation Learning and Model Architecture analysis: theoretical/empirical study of how causal masks encode position and interact with RoPE in Transformer decoders.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Junu Kim",
            "Xiao Liu",
            "Zhenghao Lin",
            "Lei Ji",
            "Yeyun Gong",
            "Edward Choi"
        ],
        "title": "Behind RoPE: How Does Causal Mask Encode Positional Information?",
        "abstract": "While explicit positional encodings such as RoPE are a primary source of positional information in Transformer decoders, the causal mask also provides positional information. In this work, we prove that the causal mask can induce position-dependent patterns in attention scores, even without parameters or causal dependency in the input. Our theoretical analysis indicates that the induced attention pattern tends to favor nearby query-key pairs, mirroring the behavior of common positional encodings. Empirical analysis confirms that trained models exhibit the same behavior, with learned parameters further amplifying these patterns. Notably, we found that the interaction of causal mask and RoPE distorts RoPE's relative attention score patterns into non-relative ones. We consistently observed this effect in modern large language models, suggesting the importance of considering the causal mask as a source of positional information alongside explicit positional encodings.",
        "arxiv_id": "2509.21042"
    },
    "2509.20950": {
        "SCORE": 17,
        "ARXIVID": "2509.20950",
        "COMMENT": "Model Architecture: introduces Decoupled-Value Attention that decouples similarity (inputs) from value (labels), an attention rule mirroring GP updates; shows localized attention as key to PFN scaling.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Kaustubh Sharma",
            "Simardeep Singh",
            "Parikshit Pareek"
        ],
        "title": "Decoupled-Value Attention for Prior-Data Fitted Networks: GP Inference for Physical Equations",
        "abstract": "Prior-data fitted networks (PFNs) are a promising alternative to time-consuming Gaussian Process (GP) inference for creating fast surrogates of physical systems. PFN reduces the computational burden of GP-training by replacing Bayesian inference in GP with a single forward pass of a learned prediction model. However, with standard Transformer attention, PFNs show limited effectiveness on high-dimensional regression tasks. We introduce Decoupled-Value Attention (DVA)-- motivated by the GP property that the function space is fully characterized by the kernel over inputs and the predictive mean is a weighted sum of training targets. DVA computes similarities from inputs only and propagates labels solely through values. Thus, the proposed DVA mirrors the Gaussian-process update while remaining kernel-free. We demonstrate that the crucial factor for scaling PFNs is the attention rule rather than the architecture itself. Specifically, our results demonstrate that (a) localized attention consistently reduces out-of-sample validation loss in PFNs across different dimensional settings, with validation loss reduced by more than 50% in five- and ten-dimensional cases, and (b) the role of attention is more decisive than the choice of backbone architecture, showing that CNN-based PFNs can perform at par with their Transformer-based counterparts. The proposed PFNs provide 64-dimensional power flow equation approximations with a mean absolute error of the order of 1E-3, while being over 80x faster than exact GP inference.",
        "arxiv_id": "2509.20950"
    },
    "2509.20368": {
        "SCORE": 17,
        "ARXIVID": "2509.20368",
        "COMMENT": "ML Systems/Efficiency: verifier-driven, per-step adaptive test-time compute allocation (resample/backtrack/stop) based on local difficulty for better accuracy\u2013compute tradeoffs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Theo Uscidda",
            "Matthew Trager",
            "Michael Kleinman",
            "Aditya Chattopadhyay",
            "Wei Xia",
            "Stefano Soatto"
        ],
        "title": "LATTS: Locally Adaptive Test-Time Scaling",
        "abstract": "One common strategy for improving the performance of Large Language Models (LLMs) on downstream tasks involves using a \\emph{verifier model} to either select the best answer from a pool of candidates or to steer the auto-regressive generation process towards better outputs. This class of methods typically results in improved accuracy at the cost of increased computation at test-time, a paradigm known as \\emph{test-time scaling}. However, most existing approaches increase computation uniformly across all samples and generation steps, without considering the complexity of individual instances, leading to inefficient resource use. We address this limitation by proposing an approach, called \\emph{Locally Adaptive Test-Time Scaling (LATTS)}, that allocates variable compute across generation steps. Specifically, at each generation step, LATTS employs a verifier-based acceptance criterion to decide whether to resample, backtrack, restart, or stop the generation process. This criterion effectively adjusts the per-step computational effort based on a precise notion of \\emph{local difficulty} derived from the verifier model. Empirical results show that LATTS achieves significantly superior accuracy--compute tradeoffs compared to standard verifier-based methods.",
        "arxiv_id": "2509.20368"
    },
    "2509.21128": {
        "SCORE": 16,
        "ARXIVID": "2509.21128",
        "COMMENT": "Strong match to Representation Learning/Training Dynamics: graph-based analysis quantifying how SFT vs RL shape reasoning trajectories and step-level structures across scales.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Kohsei Matsutani",
            "Shota Takashiro",
            "Gouki Minegishi",
            "Takeshi Kojima",
            "Yusuke Iwasawa",
            "Yutaka Matsuo"
        ],
        "title": "RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs",
        "abstract": "Large language models (LLMs) are typically trained by reinforcement learning (RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on reasoning traces to improve their reasoning abilities. However, how these methods shape reasoning capabilities remains largely elusive. Going beyond an accuracy-based investigation of how these two components sculpt the reasoning process, this paper introduces a novel analysis framework that quantifies reasoning paths and captures their qualitative changes under each training process (with models of 1.5B, 7B, and 14B parameters on mathematical domains). Specifically, we investigate the reasoning process at two levels of granularity: the trajectory-level, which examines complete reasoning outputs, and the step-level, which analyzes reasoning graphs whose nodes correspond to individual reasoning steps. Notably, clustering of unique reasoning trajectories shows complementary effects: RL compresses incorrect trajectories, whereas SFT expands correct ones. Step-level analysis reveals that RL steepens (about 2.5 times), while SFT flattens (reduced to about one-third), the decay rates of node visitation frequency, degree, and betweenness centrality distributions in the reasoning graph. This indicates that RL concentrates reasoning functionality into a small subset of steps, while SFT homogenizes it across many steps. Furthermore, by evaluating the reasoning graph topologies from multiple perspectives, we delineate the shared and distinct characteristics of RL and SFT. Our work presents a novel reasoning path perspective that explains why the current best practice of two-stage training, with SFT followed by RL, is successful, and offers practical implications for data construction and more efficient learning approaches.",
        "arxiv_id": "2509.21128"
    },
    "2509.20416": {
        "SCORE": 16,
        "ARXIVID": "2509.20416",
        "COMMENT": "Inference efficiency/ML Systems: non-autoregressive cascaded speculative drafter enabling single-pass drafting with lossless verification for faster LLM decoding.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Haiduo Huang",
            "Jiangcheng Song",
            "Wenzhe Zhao",
            "Pengju Ren"
        ],
        "title": "FastEagle: Cascaded Drafting for Accelerating Speculative Decoding",
        "abstract": "Speculative decoding accelerates generation by drafting candidates and verifying them in parallel, yet state-of-the-art drafters (e.g., EAGLE) still require N sequential passes to propose N tokens. We present FastEagle, a non-autoregressive cascaded drafter that emits an entire draft in a single forward pass. FastEagle replaces temporal steps with a lightweight layer cascade and trains with layer-wise supervision to mitigate error accumulation. Coupled with a constrained draft tree that preserves lossless verification cost, FastEagle delivers substantial wall-clock speedups over strong autoregressive drafters while maintaining competitive acceptance behavior. Across multiple LLMs (Vicuna-13B, LLaMA-Instruct 3.x, and DeepSeek-R1-Distill-LLaMA) and tasks (MT-Bench, HumanEval, GSM8K, CNN/DM, Alpaca), FastEagle consistently outperforms EAGLE-3 in speedup under both greedy and stochastic decoding, with comparable average acceptance lengths. These results indicate that removing sequential dependencies in drafting is a practical path toward lossless LLM inference acceleration.",
        "arxiv_id": "2509.20416"
    },
    "2509.21012": {
        "SCORE": 16,
        "ARXIVID": "2509.21012",
        "COMMENT": "Representation learning/training dynamics: identifies task-oriented information removal via low-rank filters and \u2018denoising heads\u2019 as a mechanism for ICL.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Hakaze Cho",
            "Haolin Yang",
            "Gouki Minegishi",
            "Naoya Inoue"
        ],
        "title": "Mechanism of Task-oriented Information Removal in In-context Learning",
        "abstract": "In-context Learning (ICL) is an emerging few-shot learning paradigm based on modern Language Models (LMs), yet its inner mechanism remains unclear. In this paper, we investigate the mechanism through a novel perspective of information removal. Specifically, we demonstrate that in the zero-shot scenario, LMs encode queries into non-selective representations in hidden states containing information for all possible tasks, leading to arbitrary outputs without focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we find that selectively removing specific information from hidden states by a low-rank filter effectively steers LMs toward the intended task. Building on these findings, by measuring the hidden states on carefully designed metrics, we observe that few-shot ICL effectively simulates such task-oriented information removal processes, selectively removing the redundant information from entangled non-selective representations, and improving the output based on the demonstrations, which constitutes a key mechanism underlying ICL. Moreover, we identify essential attention heads inducing the removal operation, termed Denoising Heads, which enables the ablation experiments blocking the information removal operation from the inference, where the ICL accuracy significantly degrades, especially when the correct label is absent from the few-shot demonstrations, confirming both the critical role of the information removal mechanism and denoising heads.",
        "arxiv_id": "2509.21012"
    },
    "2509.20986": {
        "SCORE": 16,
        "ARXIVID": "2509.20986",
        "COMMENT": "Model Compression and Efficiency: improved knowledge distillation via nullspace-guided feature refinement (LoRA-based) to suppress artifacts while preserving signal.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Geunhyeok Yu",
            "Sunjae Jeong",
            "Yoonyoung Choi",
            "Jaeseung Kim",
            "Hyoseok Hwang"
        ],
        "title": "SiNGER: A Clearer Voice Distills Vision Transformers Further",
        "abstract": "Vision Transformers are widely adopted as the backbone of vision foundation models, but they are known to produce high-norm artifacts that degrade representation quality. When knowledge distillation transfers these features to students, high-norm artifacts dominate the objective, so students overfit to artifacts and underweight informative signals, diminishing the gains from larger models. Prior work attempted to remove artifacts but encountered an inherent trade-off between artifact suppression and preserving informative signals from teachers. To address this, we introduce Singular Nullspace-Guided Energy Reallocation (SiNGER), a novel distillation framework that suppresses artifacts while preserving informative signals. The key idea is principled teacher feature refinement: during refinement, we leverage the nullspace-guided perturbation to preserve information while suppressing artifacts. Then, the refined teacher's features are distilled to a student. We implement this perturbation efficiently with a LoRA-based adapter that requires minimal structural modification. Extensive experiments show that \\oursname consistently improves student models, achieving state-of-the-art performance in multiple downstream tasks and producing clearer and more interpretable representations.",
        "arxiv_id": "2509.20986"
    },
    "2509.20840": {
        "SCORE": 16,
        "ARXIVID": "2509.20840",
        "COMMENT": "Strong match to Representation Learning/Training Dynamics: two-stage scheduling with MI-based FastPID and an async controller to balance modalities, with theory on error bounds.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Jiaqi Tang",
            "Yinsong Xu",
            "Yang Liu",
            "Qingchao Chen"
        ],
        "title": "Shaping Initial State Prevents Modality Competition in Multi-modal Fusion: A Two-stage Scheduling Framework via Fast Partial Information Decomposition",
        "abstract": "Multi-modal fusion often suffers from modality competition during joint training, where one modality dominates the learning process, leaving others under-optimized. Overlooking the critical impact of the model's initial state, most existing methods address this issue during the joint learning stage. In this study, we introduce a two-stage training framework to shape the initial states through unimodal training before the joint training. First, we propose the concept of Effective Competitive Strength (ECS) to quantify a modality's competitive strength. Our theoretical analysis further reveals that properly shaping the initial ECS by unimodal training achieves a provably tighter error bound. However, ECS is computationally intractable in deep neural networks. To bridge this gap, we develop a framework comprising two core components: a fine-grained computable diagnostic metric and an asynchronous training controller. For the metric, we first prove that mutual information(MI) is a principled proxy for ECS. Considering MI is induced by per-modality marginals and thus treats each modality in isolation, we further propose FastPID, a computationally efficient and differentiable solver for partial information decomposition, which decomposes the joint distribution's information into fine-grained measurements: modality-specific uniqueness, redundancy, and synergy. Guided by these measurements, our asynchronous controller dynamically balances modalities by monitoring uniqueness and locates the ideal initial state to start joint training by tracking peak synergy. Experiments on diverse benchmarks demonstrate that our method achieves state-of-the-art performance. Our work establishes that shaping the pre-fusion models' initial state is a powerful strategy that eases competition before it starts, reliably unlocking synergistic multi-modal fusion.",
        "arxiv_id": "2509.20840"
    },
    "2509.20615": {
        "SCORE": 16,
        "ARXIVID": "2509.20615",
        "COMMENT": "Representation Learning and Model Architecture: proposes a latent operator surrogate framework with theoretical approximation guarantees for ODE/PDE solution operators\u2014foundational and general.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Matthias Chung",
            "Deepanshu Verma",
            "Max Collins",
            "Amit N. Subrahmanya",
            "Varuni Katti Sastry",
            "Vishwas Rao"
        ],
        "title": "Latent Twins",
        "abstract": "Over the past decade, scientific machine learning has transformed the development of mathematical and computational frameworks for analyzing, modeling, and predicting complex systems. From inverse problems to numerical PDEs, dynamical systems, and model reduction, these advances have pushed the boundaries of what can be simulated. Yet they have often progressed in parallel, with representation learning and algorithmic solution methods evolving largely as separate pipelines. With \\emph{Latent Twins}, we propose a unifying mathematical framework that creates a hidden surrogate in latent space for the underlying equations. Whereas digital twins mirror physical systems in the digital world, Latent Twins mirror mathematical systems in a learned latent space governed by operators. Through this lens, classical modeling, inversion, model reduction, and operator approximation all emerge as special cases of a single principle. We establish the fundamental approximation properties of Latent Twins for both ODEs and PDEs and demonstrate the framework across three representative settings: (i) canonical ODEs, capturing diverse dynamical regimes; (ii) a PDE benchmark using the shallow-water equations, contrasting Latent Twin simulations with DeepONet and forecasts with a 4D-Var baseline; and (iii) a challenging real-data geopotential reanalysis dataset, reconstructing and forecasting from sparse, noisy observations. Latent Twins provide a compact, interpretable surrogate for solution operators that evaluate across arbitrary time gaps in a single-shot, while remaining compatible with scientific pipelines such as assimilation, control, and uncertainty quantification. Looking forward, this framework offers scalable, theory-grounded surrogates that bridge data-driven representation learning and classical scientific modeling across disciplines.",
        "arxiv_id": "2509.20615"
    },
    "2509.20977": {
        "SCORE": 16,
        "ARXIVID": "2509.20977",
        "COMMENT": "Representation Learning/Mechanistic Interpretability: circuit-based neuron localization (CNF satisfiability) enables targeted unlearning with precise forget/retain interventions.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Hang Chen",
            "Jiaying Zhu",
            "Xinyu Yang",
            "Wenya Wang"
        ],
        "title": "CLUE: Conflict-guided Localization for LLM Unlearning Framework",
        "abstract": "The LLM unlearning aims to eliminate the influence of undesirable data without affecting causally unrelated information. This process typically involves using a forget set to remove target information, alongside a retain set to maintain non-target capabilities. While recent localization-based methods demonstrate promise in identifying important neurons to be unlearned, they fail to disentangle neurons responsible for forgetting undesirable knowledge or retaining essential skills, often treating them as a single entangled group. As a result, these methods apply uniform interventions, risking catastrophic over-forgetting or incomplete erasure of the target knowledge. To address this, we turn to circuit discovery, a mechanistic interpretability technique, and propose the Conflict-guided Localization for LLM Unlearning framEwork (CLUE). This framework identifies the forget and retain circuit composed of important neurons, and then the circuits are transformed into conjunctive normal forms (CNF). The assignment of each neuron in the CNF satisfiability solution reveals whether it should be forgotten or retained. We then provide targeted fine-tuning strategies for different categories of neurons. Extensive experiments demonstrate that, compared to existing localization methods, CLUE achieves superior forget efficacy and retain utility through precise neural localization.",
        "arxiv_id": "2509.20977"
    },
    "2509.20404": {
        "SCORE": 16,
        "ARXIVID": "2509.20404",
        "COMMENT": "Representation Learning theory: new high-dimensional model leveraging structured correlation with learnability characterized by VCN_{k,k}-dimension.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Leonardo N. Coregliano",
            "Maryanthe Malliaris"
        ],
        "title": "Sample completion, structured correlation, and Netflix problems",
        "abstract": "We develop a new high-dimensional statistical learning model which can take advantage of structured correlation in data even in the presence of randomness. We completely characterize learnability in this model in terms of VCN${}_{k,k}$-dimension (essentially $k$-dependence from Shelah's classification theory). This model suggests a theoretical explanation for the success of certain algorithms in the 2006~Netflix Prize competition.",
        "arxiv_id": "2509.20404"
    },
    "2509.21199": {
        "SCORE": 16,
        "ARXIVID": "2509.21199",
        "COMMENT": "Matches Representation Learning/training dynamics: establishes a Fano-style capacity upper bound for single-pass LLM reasoning and proposes capacity-aware multi-step structuring.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Kaiyang Wan",
            "Lang Gao",
            "Honglin Mu",
            "Preslav Nakov",
            "Yuxia Wang",
            "Xiuying Chen"
        ],
        "title": "A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in Multi-Hop QA",
        "abstract": "Multi-Hop Question Answering (MHQA) requires integrating dispersed, interdependent evidence through sequential reasoning under noise. This task is challenging for LLMs as they have a finite per-pass output capacity, beyond which the integration of task-relevant evidence proves unreliable. Consequently, the single-pass reasoning paradigm is inherently vulnerable to this capacity overflow. To formalize this bottleneck, our analysis establishes a Fano-style accuracy upper bound, defining a theoretical performance ceiling for single-pass LLMs. This bound reveals that accuracy inevitably collapses once task complexity exceeds model capacity, providing general principles for capacity-aware representation and structuring of MHQA in LLMs. Building on these principles, we introduce a proof-of-concept multi-call framework for MHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware task decomposition with active pruning of prior reasoning traces, keeping the information load within the single-pass limit. It further achieves robustness by a dependency-explicit workflow that enables precise control over the reasoning path. We construct a stringent and noise-rich benchmark to validate our theory and framework. Experimental results show that model behavior aligns with our predicted capacity curves while InfoQA achieves consistent performance improvements. We hope our work inspires more LLM multi-step reasoning methods: \\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.",
        "arxiv_id": "2509.21199"
    },
    "2509.20618": {
        "SCORE": 16,
        "ARXIVID": "2509.20618",
        "COMMENT": "Theoretical ML: new gapped scale-sensitive dimensions controlling covering numbers and lower bounds on offset Rademacher complexity.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Zeyu Jia",
            "Yury Polyanskiy",
            "Alexander Rakhlin"
        ],
        "title": "A Gapped Scale-Sensitive Dimension and Lower Bounds for Offset Rademacher Complexity",
        "abstract": "We study gapped scale-sensitive dimensions of a function class in both sequential and non-sequential settings. We demonstrate that covering numbers for any uniformly bounded class are controlled above by these gapped dimensions, generalizing the results of \\cite{anthony2000function,alon1997scale}. Moreover, we show that the gapped dimensions lead to lower bounds on offset Rademacher averages, thereby strengthening existing approaches for proving lower bounds on rates of convergence in statistical and online learning.",
        "arxiv_id": "2509.20618"
    },
    "2509.20408": {
        "SCORE": 16,
        "ARXIVID": "2509.20408",
        "COMMENT": "Model Architecture/Training Objective: theoretical framework and algorithms for multi-agent GFlowNets with guarantees.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Leo Maxime Brunswic",
            "Haozhi Wang",
            "Shuang Luo",
            "Jianye Hao",
            "Amir Rasouli",
            "Yinchuan Li"
        ],
        "title": "A Theory of Multi-Agent Generative Flow Networks",
        "abstract": "Generative flow networks utilize a flow-matching loss to learn a stochastic policy for generating objects from a sequence of actions, such that the probability of generating a pattern can be proportional to the corresponding given reward. However, a theoretical framework for multi-agent generative flow networks (MA-GFlowNets) has not yet been proposed. In this paper, we propose the theory framework of MA-GFlowNets, which can be applied to multiple agents to generate objects collaboratively through a series of joint actions. We further propose four algorithms: a centralized flow network for centralized training of MA-GFlowNets, an independent flow network for decentralized execution, a joint flow network for achieving centralized training with decentralized execution, and its updated conditional version. Joint Flow training is based on a local-global principle allowing to train a collection of (local) GFN as a unique (global) GFN. This principle provides a loss of reasonable complexity and allows to leverage usual results on GFN to provide theoretical guarantees that the independent policies generate samples with probability proportional to the reward function. Experimental results demonstrate the superiority of the proposed framework compared to reinforcement learning and MCMC-based methods.",
        "arxiv_id": "2509.20408"
    },
    "2509.21000": {
        "SCORE": 15,
        "ARXIVID": "2509.21000",
        "COMMENT": "Model Architecture: enhances GNN expressiveness via Local-UIDs with theoretical guarantees, a generalizable architectural insight.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Qingyu Han",
            "Qian Li",
            "Linxin Yang",
            "Qian Chen",
            "Qingjiang Shi",
            "Ruoyu Sun"
        ],
        "title": "Feature Augmentation of GNNs for ILPs: Local Uniqueness Suffices",
        "abstract": "Integer Linear Programs (ILPs) are central to real-world optimizations but notoriously difficult to solve. Learning to Optimize (L2O) has emerged as a promising paradigm, with Graph Neural Networks (GNNs) serving as the standard backbone. However, standard anonymous GNNs are limited in expressiveness for ILPs, and the common enhancement of augmenting nodes with globally unique identifiers (UIDs) typically introduces spurious correlations that severely harm generalization. To address this tradeoff, we propose a parsimonious Local-UID scheme based on d-hop uniqueness coloring, which ensures identifiers are unique only within each node's d-hop neighborhood. Building on this scheme, we introduce ColorGNN, which incorporates color information via color-conditioned embeddings, and ColorUID, a lightweight feature-level variant. We prove that for d-layer networks, Local-UIDs achieve the expressive power of Global-UIDs while offering stronger generalization. Extensive experiments show that our approach (i) yields substantial gains on three ILP benchmarks, (ii) exhibits strong OOD generalization on linear programming datasets, and (iii) further improves a general graph-level task when paired with a state-of-the-art method.",
        "arxiv_id": "2509.21000"
    },
    "2509.21196": {
        "SCORE": 15,
        "ARXIVID": "2509.21196",
        "COMMENT": "Strong match to Model Architecture: operator-decomposed neural operator with a constrained differential branch and Transformer-based integral branch for stable long-range dynamics.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Hao Wu",
            "Yuan Gao",
            "Fan Xu",
            "Fan Zhang",
            "Qingsong Wen",
            "Kun Wang",
            "Xiaomeng Huang",
            "Xian Wu"
        ],
        "title": "Differential-Integral Neural Operator for Long-Term Turbulence Forecasting",
        "abstract": "Accurately forecasting the long-term evolution of turbulence represents a grand challenge in scientific computing and is crucial for applications ranging from climate modeling to aerospace engineering. Existing deep learning methods, particularly neural operators, often fail in long-term autoregressive predictions, suffering from catastrophic error accumulation and a loss of physical fidelity. This failure stems from their inability to simultaneously capture the distinct mathematical structures that govern turbulent dynamics: local, dissipative effects and global, non-local interactions. In this paper, we propose the {\\textbf{\\underline{D}}}ifferential-{\\textbf{\\underline{I}}}ntegral {\\textbf{\\underline{N}}}eural {\\textbf{\\underline{O}}}perator (\\method{}), a novel framework designed from a first-principles approach of operator decomposition. \\method{} explicitly models the turbulent evolution through parallel branches that learn distinct physical operators: a local differential operator, realized by a constrained convolutional network that provably converges to a derivative, and a global integral operator, captured by a Transformer architecture that learns a data-driven global kernel. This physics-based decomposition endows \\method{} with exceptional stability and robustness. Through extensive experiments on the challenging 2D Kolmogorov flow benchmark, we demonstrate that \\method{} significantly outperforms state-of-the-art models in long-term forecasting. It successfully suppresses error accumulation over hundreds of timesteps, maintains high fidelity in both the vorticity fields and energy spectra, and establishes a new benchmark for physically consistent, long-range turbulence forecast.",
        "arxiv_id": "2509.21196"
    },
    "2509.20882": {
        "SCORE": 15,
        "ARXIVID": "2509.20882",
        "COMMENT": "Representation learning/theory: theoretical analysis of concept-based in-context learning with insights on similarity measures, embedding dimension, and demo size effects.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Huaze Tang",
            "Tianren Peng",
            "Shao-lun Huang"
        ],
        "title": "On Theoretical Interpretations of Concept-Based In-Context Learning",
        "abstract": "In-Context Learning (ICL) has emerged as an important new paradigm in natural language processing and large language model (LLM) applications. However, the theoretical understanding of the ICL mechanism remains limited. This paper aims to investigate this issue by studying a particular ICL approach, called concept-based ICL (CB-ICL). In particular, we propose theoretical analyses on applying CB-ICL to ICL tasks, which explains why and when the CB-ICL performs well for predicting query labels in prompts with only a few demonstrations. In addition, the proposed theory quantifies the knowledge that can be leveraged by the LLMs to the prompt tasks, and leads to a similarity measure between the prompt demonstrations and the query input, which provides important insights and guidance for model pre-training and prompt engineering in ICL. Moreover, the impact of the prompt demonstration size and the dimension of the LLM embeddings in ICL are also explored based on the proposed theory. Finally, several real-data experiments are conducted to validate the practical usefulness of CB-ICL and the corresponding theory.",
        "arxiv_id": "2509.20882"
    },
    "2509.21091": {
        "SCORE": 15,
        "ARXIVID": "2509.21091",
        "COMMENT": "Test-time compute/ensemble efficiency: analyzes Best-of-N asymptotics, proposes adaptive N selection, and optimizes multi-LLM weighted ensembles for inference-time allocation.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Junpei Komiyama",
            "Daisuke Oba",
            "Masafumi Oyamada"
        ],
        "title": "Best-of-$\\infty$ -- Asymptotic Performance of Test-Time Compute",
        "abstract": "We study best-of-$N$ for large language models (LLMs) where the selection is based on majority voting. In particular, we analyze the limit $N \\to \\infty$, which we denote as Best-of-$\\infty$. While this approach achieves impressive performance in the limit, it requires an infinite test-time budget. To address this, we propose an adaptive generation scheme that selects $N$ based on answer agreement, thereby efficiently allocating inference-time computation. Beyond adaptivity, we extend the framework to weighted ensembles of multiple LLMs, showing that such mixtures can outperform any individual model. The optimal ensemble weighting is formulated and efficiently computed as a mixed-integer linear program. Extensive experiments demonstrate the effectiveness of our approach.",
        "arxiv_id": "2509.21091"
    },
    "2509.21049": {
        "SCORE": 15,
        "ARXIVID": "2509.21049",
        "COMMENT": "Learning theory/training dynamics: Lagrangian formulation unifying and deriving learning algorithms (e.g., Bellman optimality, Adam) from first principles.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Siyuan Guo",
            "Bernhard Sch\\\"olkopf"
        ],
        "title": "Physics of Learning: A Lagrangian perspective to different learning paradigms",
        "abstract": "We study the problem of building an efficient learning system. Efficient learning processes information in the least time, i.e., building a system that reaches a desired error threshold with the least number of observations. Building upon least action principles from physics, we derive classic learning algorithms, Bellman's optimality equation in reinforcement learning, and the Adam optimizer in generative models from first principles, i.e., the Learning $\\textit{Lagrangian}$. We postulate that learning searches for stationary paths in the Lagrangian, and learning algorithms are derivable by seeking the stationary trajectories.",
        "arxiv_id": "2509.21049"
    },
    "2509.21286": {
        "SCORE": 15,
        "ARXIVID": "2509.21286",
        "COMMENT": "Model Architecture: theoretical characterization of maxout networks (parameter spaces, extremal f-vectors, separating hypersurfaces) providing structural insights.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Andrei Balakin",
            "Shelby Cox",
            "Georg Loho",
            "Bernd Sturmfels"
        ],
        "title": "Maxout Polytopes",
        "abstract": "Maxout polytopes are defined by feedforward neural networks with maxout activation function and non-negative weights after the first layer. We characterize the parameter spaces and extremal f-vectors of maxout polytopes for shallow networks, and we study the separating hypersurfaces which arise when a layer is added to the network. We also show that maxout polytopes are cubical for generic networks without bottlenecks.",
        "arxiv_id": "2509.21286"
    },
    "2509.20939": {
        "SCORE": 15,
        "ARXIVID": "2509.20939",
        "COMMENT": "Model Architecture analysis: derives causal, theoretical links between stem kernels, input resolution, pooling, and preprocessing normalization and robustness to Gaussian noise.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Bum Jun Kim",
            "Makoto Kawano",
            "Yusuke Iwasawa",
            "Yutaka Matsuo"
        ],
        "title": "Unlocking Noise-Resistant Vision: Key Architectural Secrets for Robust Models",
        "abstract": "While the robustness of vision models is often measured, their dependence on specific architectural design choices is rarely dissected. We investigate why certain vision architectures are inherently more robust to additive Gaussian noise and convert these empirical insights into simple, actionable design rules. Specifically, we performed extensive evaluations on 1,174 pretrained vision models, empirically identifying four consistent design patterns for improved robustness against Gaussian noise: larger stem kernels, smaller input resolutions, average pooling, and supervised vision transformers (ViTs) rather than CLIP ViTs, which yield up to 506 rank improvements and 21.6\\%p accuracy gains. We then develop a theoretical analysis that explains these findings, converting observed correlations into causal mechanisms. First, we prove that low-pass stem kernels attenuate noise with a gain that decreases quadratically with kernel size and that anti-aliased downsampling reduces noise energy roughly in proportion to the square of the downsampling factor. Second, we demonstrate that average pooling is unbiased and suppresses noise in proportion to the pooling window area, whereas max pooling incurs a positive bias that grows slowly with window size and yields a relatively higher mean-squared error and greater worst-case sensitivity. Third, we reveal and explain the vulnerability of CLIP ViTs via a pixel-space Lipschitz bound: The smaller normalization standard deviations used in CLIP preprocessing amplify worst-case sensitivity by up to 1.91 times relative to the Inception-style preprocessing common in supervised ViTs. Our results collectively disentangle robustness into interpretable modules, provide a theory that explains the observed trends, and build practical, plug-and-play guidelines for designing vision models more robust against Gaussian noise.",
        "arxiv_id": "2509.20939"
    },
    "2509.21318": {
        "SCORE": 14,
        "ARXIVID": "2509.21318",
        "COMMENT": "Model Compression/Efficiency and ML Systems: few-step distillation for rectified flows with timestep sharing and split-timestep fine-tuning plus quantization/memory optimizations for on-device inference.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Hmrishav Bandyopadhyay",
            "Rahim Entezari",
            "Jim Scott",
            "Reshinth Adithyan",
            "Yi-Zhe Song",
            "Varun Jampani"
        ],
        "title": "SD3.5-Flash: Distribution-Guided Distillation of Generative Flows",
        "abstract": "We present SD3.5-Flash, an efficient few-step distillation framework that brings high-quality image generation to accessible consumer devices. Our approach distills computationally prohibitive rectified flow models through a reformulated distribution matching objective tailored specifically for few-step generation. We introduce two key innovations: \"timestep sharing\" to reduce gradient noise and \"split-timestep fine-tuning\" to improve prompt alignment. Combined with comprehensive pipeline optimizations like text encoder restructuring and specialized quantization, our system enables both rapid generation and memory-efficient deployment across different hardware configurations. This democratizes access across the full spectrum of devices, from mobile phones to desktop computers. Through extensive evaluation including large-scale user studies, we demonstrate that SD3.5-Flash consistently outperforms existing few-step methods, making advanced generative AI truly accessible for practical deployment.",
        "arxiv_id": "2509.21318"
    },
    "2509.21287": {
        "SCORE": 14,
        "ARXIVID": "2509.21287",
        "COMMENT": "Model Architecture: a tensor-network text encoder with explicit syntactic composition; Efficiency via tensor factorization for parameter reduction.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Kin Ian Lo",
            "Hala Hawashin",
            "Mina Abbaszadeh",
            "Tilen Limback-Stokin",
            "Hadi Wazni",
            "Mehrnoosh Sadrzadeh"
        ],
        "title": "DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding",
        "abstract": "Recent vision-language models excel at large-scale image-text alignment but often neglect the compositional structure of language, leading to failures on tasks that hinge on word order and predicate-argument structure. We introduce DisCoCLIP, a multimodal encoder that combines a frozen CLIP vision transformer with a novel tensor network text encoder that explicitly encodes syntactic structure. Sentences are parsed with a Combinatory Categorial Grammar parser to yield distributional word tensors whose contractions mirror the sentence's grammatical derivation. To keep the model efficient, high-order tensors are factorized with tensor decompositions, reducing parameter count from tens of millions to under one million. Trained end-to-end with a self-supervised contrastive loss, DisCoCLIP markedly improves sensitivity to verb semantics and word order: it raises CLIP's SVO-Probes verb accuracy from 77.6% to 82.4%, boosts ARO attribution and relation scores by over 9% and 4%, and achieves 93.7% on a newly introduced SVO-Swap benchmark. These results demonstrate that embedding explicit linguistic structure via tensor networks yields interpretable, parameter-efficient representations that substantially improve compositional reasoning in vision-language tasks.",
        "arxiv_id": "2509.21287"
    },
    "2509.21150": {
        "SCORE": 14,
        "ARXIVID": "2509.21150",
        "COMMENT": "Model Architecture: modality-specific tokenization for CAD via sequence VQ-VAE (autoencoder-style) producing primitive-aware discrete codes.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Ruiyu Wang",
            "Shizhao Sun",
            "Weijian Ma",
            "Jiang Bian"
        ],
        "title": "CAD-Tokenizer: Towards Text-based CAD Prototyping via Modality-Specific Tokenization",
        "abstract": "Computer-Aided Design (CAD) is a foundational component of industrial prototyping, where models are defined not by raw coordinates but by construction sequences such as sketches and extrusions. This sequential structure enables both efficient prototype initialization and subsequent editing. Text-guided CAD prototyping, which unifies Text-to-CAD generation and CAD editing, has the potential to streamline the entire design pipeline. However, prior work has not explored this setting, largely because standard large language model (LLM) tokenizers decompose CAD sequences into natural-language word pieces, failing to capture primitive-level CAD semantics and hindering attention modules from modeling geometric structure. We conjecture that a multimodal tokenization strategy, aligned with CAD's primitive and structural nature, can provide more effective representations. To this end, we propose CAD-Tokenizer, a framework that represents CAD data with modality-specific tokens using a sequence-based VQ-VAE with primitive-level pooling and constrained decoding. This design produces compact, primitive-aware representations that align with CAD's structural nature. Applied to unified text-guided CAD prototyping, CAD-Tokenizer significantly improves instruction following and generation quality, achieving better quantitative and qualitative performance over both general-purpose LLMs and task-specific baselines.",
        "arxiv_id": "2509.21150"
    },
    "2509.21296": {
        "SCORE": 14,
        "ARXIVID": "2509.21296",
        "COMMENT": "Matches Representation Learning/training dynamics: theoretical limits of reconstruction attacks tied to implicit bias and memorization in trained networks.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Yehonatan Refael",
            "Guy Smorodinsky",
            "Ofir Lindenbaum",
            "Itay Safran"
        ],
        "title": "No Prior, No Leakage: Revisiting Reconstruction Attacks in Trained Neural Networks",
        "abstract": "The memorization of training data by neural networks raises pressing concerns for privacy and security. Recent work has shown that, under certain conditions, portions of the training set can be reconstructed directly from model parameters. Some of these methods exploit implicit bias toward margin maximization, suggesting that properties often regarded as beneficial for generalization may actually compromise privacy. Yet despite striking empirical demonstrations, the reliability of these attacks remains poorly understood and lacks a solid theoretical foundation. In this work, we take a complementary perspective: rather than designing stronger attacks, we analyze the inherent weaknesses and limitations of existing reconstruction methods and identify conditions under which they fail. We rigorously prove that, without incorporating prior knowledge about the data, there exist infinitely many alternative solutions that may lie arbitrarily far from the true training set, rendering reconstruction fundamentally unreliable. Empirically, we further demonstrate that exact duplication of training examples occurs only by chance. Our results refine the theoretical understanding of when training set leakage is possible and offer new insights into mitigating reconstruction attacks. Remarkably, we demonstrate that networks trained more extensively, and therefore satisfying implicit bias conditions more strongly -- are, in fact, less susceptible to reconstruction attacks, reconciling privacy with the need for strong generalization in this setting.",
        "arxiv_id": "2509.21296"
    },
    "2509.20591": {
        "SCORE": 14,
        "ARXIVID": "2509.20591",
        "COMMENT": "Model Architecture/Efficiency: hierarchical Neural FMM separating local vs far-field interactions to learn Green\u2019s operators.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Emilio McAllister Fognini",
            "Marta M. Betcke",
            "Ben T. Cox"
        ],
        "title": "Learning Greens Operators through Hierarchical Neural Networks Inspired by the Fast Multipole Method",
        "abstract": "The Fast Multipole Method (FMM) is an efficient numerical algorithm for computation of long-ranged forces in $N$-body problems within gravitational and electrostatic fields. This method utilizes multipole expansions of the Green's function inherent to the underlying dynamical systems. Despite its widespread application in physics and engineering, the integration of FMM with modern machine learning architectures remains underexplored. In this work, we propose a novel neural network architecture, the Neural FMM, that integrates the information flow of the FMM into a hierarchical machine learning framework for learning the Green's operator of an Elliptic PDE. Our Neural FMM architecture leverages a hierarchical computation flow of the FMM method to split up the local and far-field interactions and efficiently learn their respective representations.",
        "arxiv_id": "2509.20591"
    },
    "2509.20968": {
        "SCORE": 14,
        "ARXIVID": "2509.20968",
        "COMMENT": "Representation Learning: multiview self-supervision on circuits with an Equivalence Alignment Loss to enable effective masked modeling across heterogeneous views.",
        "RELEVANCE": 7,
        "NOVELTY": 7,
        "authors": [
            "Zhengyuan Shi",
            "Jingxin Wang",
            "Wentao Jiang",
            "Chengyu Ma",
            "Ziyang Zheng",
            "Zhufei Chu",
            "Weikang Qian",
            "Qiang Xu"
        ],
        "title": "Alignment Unlocks Complementarity: A Framework for Multiview Circuit Representation Learning",
        "abstract": "Multiview learning on Boolean circuits holds immense promise, as different graph-based representations offer complementary structural and semantic information. However, the vast structural heterogeneity between views, such as an And-Inverter Graph (AIG) versus an XOR-Majority Graph (XMG), poses a critical barrier to effective fusion, especially for self-supervised techniques like masked modeling. Naively applying such methods fails, as the cross-view context is perceived as noise. Our key insight is that functional alignment is a necessary precondition to unlock the power of multiview self-supervision. We introduce MixGate, a framework built on a principled training curriculum that first teaches the model a shared, function-aware representation space via an Equivalence Alignment Loss. Only then do we introduce a multiview masked modeling objective, which can now leverage the aligned views as a rich, complementary signal. Extensive experiments, including a crucial ablation study, demonstrate that our alignment-first strategy transforms masked modeling from an ineffective technique into a powerful performance driver.",
        "arxiv_id": "2509.20968"
    }
}