{
    "2510.05064": {
        "SCORE": 19,
        "ARXIVID": "2510.05064",
        "COMMENT": "Model Compression and Efficiency \u2014 zero-shot model size interpolation via re-incorporating teacher blocks after distillation/pruning without additional training.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Sara Kangaslahti",
            "Nihal V. Nayak",
            "Jonathan Geuter",
            "Marco Fumero",
            "Francesco Locatello",
            "David Alvarez-Melis"
        ],
        "title": "Boomerang Distillation Enables Zero-Shot Model Size Interpolation",
        "abstract": "Large language models (LLMs) are typically deployed under diverse memory and compute constraints. Existing approaches build model families by training each size independently, which is prohibitively expensive and provides only coarse-grained size options. In this work, we identify a novel phenomenon that we call boomerang distillation: starting from a large base model (the teacher), one first distills down to a small student and then progressively reconstructs intermediate-sized models by re-incorporating blocks of teacher layers into the student without any additional training. This process produces zero-shot interpolated models of many intermediate sizes whose performance scales smoothly between the student and teacher, often matching or surpassing pretrained or distilled models of the same size. We further analyze when this type of interpolation succeeds, showing that alignment between teacher and student through pruning and distillation is essential. Boomerang distillation thus provides a simple and efficient way to generate fine-grained model families, dramatically reducing training cost while enabling flexible adaptation across deployment environments. The code and models are available at https://github.com/dcml-lab/boomerang-distillation.",
        "arxiv_id": "2510.05064"
    },
    "2510.04476": {
        "SCORE": 19,
        "ARXIVID": "2510.04476",
        "COMMENT": "Model Architecture + Systems: new attention (CCA/CCGQA) compresses KV-cache and FLOPs with fused kernels; faster training and prefill on long contexts, works with MoE.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Tomas Figliolia",
            "Nicholas Alonso",
            "Rishi Iyer",
            "Quentin Anthony",
            "Beren Millidge"
        ],
        "title": "Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space",
        "abstract": "Multi-headed Attention's (MHA) quadratic compute and linearly growing KV-cache make long-context transformers expensive to train and serve. Prior works such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA) shrink the cache, speeding decode, but leave compute, which determines prefill and training speed, largely unchanged. We introduce Compressed Convolutional Attention (CCA), a novel attention method which down-projects queries, keys, and values and performs the entire attention operation inside the shared latent space. This simple design dramatically cuts parameters, KV-cache, and FLOPs all at once by the desired compression factor. Because CCA is orthogonal to head-sharing, we combine the two to form Compressed Convolutional Grouped Query Attention (CCGQA), which further tightens the compute-bandwidth Pareto frontier so that users can tune compression toward either FLOP or memory limits without sacrificing quality. Experiments show that CCGQA consistently outperforms both GQA and MLA at equal KV-cache compression on dense and MoE models. Additionally, we show that CCGQA outperforms all other attention methods on MoE models with half the KV-cache of GQA and MLA, achieving an 8x KV-cache compression with no drop in performance compared to standard MHA. CCA and CCGQA also dramatically reduce the FLOP cost of attention which leads to substantially faster training and prefill than existing methods. On H100 GPUs, our fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence length of 16k relative to MHA, and accelerates backward by about 1.3x.",
        "arxiv_id": "2510.04476"
    },
    "2510.03470": {
        "SCORE": 19,
        "ARXIVID": "2510.03470",
        "COMMENT": "Model Architecture / Training Dynamics \u2014 theoretical Residual Expansion Theorem explains depth via implicit ensembles and prescribes principled residual scaling, clarifying normalization-free training.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Benoit Dherin",
            "Michael Munn"
        ],
        "title": "On residual network depth",
        "abstract": "Deep residual architectures, such as ResNet and the Transformer, have enabled models of unprecedented depth, yet a formal understanding of why depth is so effective remains an open question. A popular intuition, following Veit et al. (2016), is that these residual networks behave like ensembles of many shallower models. Our key finding is an explicit analytical formula that verifies this ensemble perspective, proving that increasing network depth is mathematically equivalent to expanding the size of this implicit ensemble. Furthermore, our expansion reveals a hierarchical ensemble structure in which the combinatorial growth of computation paths leads to an explosion in the output signal, explaining the historical necessity of normalization layers in training deep models. This insight offers a first principles explanation for the historical dependence on normalization layers and sheds new light on a family of successful normalization-free techniques like SkipInit and Fixup. However, while these previous approaches infer scaling factors through optimizer analysis or a heuristic analogy to Batch Normalization, our work offers the first explanation derived directly from the network's inherent functional structure. Specifically, our Residual Expansion Theorem reveals that scaling each residual module provides a principled solution to taming the combinatorial explosion inherent to these architectures. We further show that this scaling acts as a capacity controls that also implicitly regularizes the model's complexity.",
        "arxiv_id": "2510.03470"
    },
    "2510.03358": {
        "SCORE": 18,
        "ARXIVID": "2510.03358",
        "COMMENT": "Representation Learning + Compression/Efficiency: theoretical/empirical rank analysis of time-series Transformers and compressibility; practical compression with large speed/memory gains.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Annan Yu",
            "Danielle C. Maddix",
            "Boran Han",
            "Xiyuan Zhang",
            "Abdul Fatir Ansari",
            "Oleksandr Shchur",
            "Christos Faloutsos",
            "Andrew Gordon Wilson",
            "Michael W. Mahoney",
            "Yuyang Wang"
        ],
        "title": "Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility",
        "abstract": "Transformers are widely used across data modalities, and yet the principles distilled from text models often transfer imperfectly to models trained to other modalities. In this paper, we analyze Transformers through the lens of rank structure. Our focus is on the time series setting, where the structural properties of the data differ remarkably from those of text or vision. We show that time-series embeddings, unlike text or vision, exhibit sharply decaying singular value spectra: small patch sizes and smooth continuous mappings concentrate the data into low-rank subspaces. From this, we prove that the associated $Q/K/V$ projections admit accurate low-rank approximations, and that attention layers become compressible in proportion to the decay of the embedding spectrum. We introduce the concept of flow-of-ranks, a phenomenon by which nonlinear mixing across depth inflates the rank, explaining why early layers are most amenable to compression and why ranks grow with depth. Guided by these theoretical and empirical results, we use these insights to compress Chronos, a large time series foundation model, achieving a reduction of $65\\%$ in inference time and $81\\%$ in memory, without loss of accuracy. Our findings provide principled guidance for allocating width, depth, and heads in time series foundation models, and for exploiting their inherent compressibility.",
        "arxiv_id": "2510.03358"
    },
    "2510.03434": {
        "SCORE": 18,
        "ARXIVID": "2510.03434",
        "COMMENT": "Strong match to ML Systems and Model Architecture (MoE): decentralized training of independent expert diffusion models with a router; eliminates synchronization and heterogeneous-hardware friendly.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Zhiying Jiang",
            "Raihan Seraj",
            "Marcos Villagra",
            "Bidhan Roy"
        ],
        "title": "Paris: A Decentralized Trained Open-Weight Diffusion Model",
        "abstract": "We present Paris, the first publicly released diffusion model pre-trained entirely through decentralized computation. Paris demonstrates that high-quality text-to-image generation can be achieved without centrally coordinated infrastructure. Paris is open for research and commercial use. Paris required implementing our Distributed Diffusion Training framework from scratch. The model consists of 8 expert diffusion models (129M-605M parameters each) trained in complete isolation with no gradient, parameter, or intermediate activation synchronization. Rather than requiring synchronized gradient updates across thousands of GPUs, we partition data into semantically coherent clusters where each expert independently optimizes its subset while collectively approximating the full distribution. A lightweight transformer router dynamically selects appropriate experts at inference, achieving generation quality comparable to centrally coordinated baselines. Eliminating synchronization enables training on heterogeneous hardware without specialized interconnects. Empirical validation confirms that Paris's decentralized training maintains generation quality while removing the dedicated GPU cluster requirement for large-scale diffusion models. Paris achieves this using 14$\\times$ less training data and 16$\\times$ less compute than the prior decentralized baseline.",
        "arxiv_id": "2510.03434"
    },
    "2510.04694": {
        "SCORE": 18,
        "ARXIVID": "2510.04694",
        "COMMENT": "Direct match to Model Architecture (MoE): analyzes multilingual routing dynamics, correlates routing alignment with performance, and proposes effective inference-time router steering.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Lucas Bandarkar",
            "Chenyuan Yang",
            "Mohsen Fayyaz",
            "Junlin Hu",
            "Nanyun Peng"
        ],
        "title": "Multilingual Routing in Mixture-of-Experts",
        "abstract": "Mixture-of-Experts (MoE) architectures have become the key to scaling modern LLMs, yet little is understood about how their sparse routing dynamics respond to multilingual data. In this work, we analyze expert routing patterns using parallel multilingual datasets and present highly interpretable layer-wise phenomena. We find that MoE models route tokens in language-specific ways in the early and late decoder layers but exhibit significant cross-lingual routing alignment in middle layers, mirroring parameter-sharing trends observed in dense LLMs. In particular, we reveal a clear, strong correlation between a model's performance in a given language and how similarly its tokens are routed to English in these layers. Extending beyond correlation, we explore inference-time interventions that induce higher cross-lingual routing alignment. We introduce a method that steers the router by promoting middle-layer task experts frequently activated in English, and it successfully increases multilingual performance. These 1-2% gains are remarkably consistent across two evaluation tasks, three models, and 15+ languages, especially given that these simple interventions override routers of extensively trained, state-of-the-art LLMs. In comparison, interventions outside of the middle layers or targeting multilingual-specialized experts only yield performance degradation. Altogether, we present numerous findings that explain how MoEs process non-English text and demonstrate that generalization is limited by the model's ability to leverage language-universal experts in all languages.",
        "arxiv_id": "2510.04694"
    },
    "2510.03275": {
        "SCORE": 18,
        "ARXIVID": "2510.03275",
        "COMMENT": "Compression/Efficiency \u2014 extreme low-bit (1\u20131.58 bit) LLM quantization via Sigma-Delta with adjustable over-sampling ratio, Hadamard smoothing, and fine-grained MultiOSR allocation.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Junhao Xia",
            "Ming Zhao",
            "Limin Xiao",
            "Xiujun Zhang"
        ],
        "title": "SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size",
        "abstract": "Large language models (LLMs) face significant computational and memory challenges, making extremely low-bit quantization crucial for their efficient deployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size, a novel framework that enables extremely low-bit quantization of LLMs while preserving their linguistic reasoning capabilities. A distinctive feature of SDQ-LLM is the continuous adjustability of the Over-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM constraints by selecting fractional OSR (e.g. 2.5 times) for an optimal trade-off between model size and accuracy. SDQ-LLM uses upsampling combined with Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding high-precision parameters into 1-bit or 1.58-bit representations, replacing the multiplication operations within linear layers with addition. This approach significantly enhances inference efficiency under extremely low-bit quantization. To further reduce the loss of quantization precision, we incorporate Hadamard-based weight smoothing prior to quantization, improving the stability and robustness of the weight representations. Furthermore, to fully leverage the continuity of the OSR and reduce precision loss, recognizing the correlation between quantization sensitivity and weight variance, we propose a fine-grained, layer- and linear-wise OSR allocation strategy, MultiOSR. This strategy distributes OSR both across layers and within each layer, based on weight variance and parameter scale. Finally, extensive experiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a more efficient and high-precision performance even under highly aggressive low-OSR settings. Our code is available at https://github.com/Dreamlittlecat/LLM-Quant-Factory.",
        "arxiv_id": "2510.03275"
    },
    "2510.03246": {
        "SCORE": 18,
        "ARXIVID": "2510.03246",
        "COMMENT": "Model Compression and Efficiency: structured pruning with ADMM and closed-form mask rules; reduces global pruning memory from O(N) to O(sqrt N) with layer-wise sparsity allocation.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Xinyuan Song",
            "Guangji Bai",
            "Liang Zhao"
        ],
        "title": "StructPrune: Structured Global Pruning asymptotics with $\\mathcal{O}(\\sqrt{N})$ GPU Memory",
        "abstract": "Pruning is critical for scaling large language models (LLMs). Global pruning achieves strong performance but requires $\\mathcal{O}(N)$ memory, which is infeasible for billion-parameter models. Local pruning reduces GPU memory usage to that of a single layer by pruning layers independently, but it neglects inter-layer dependencies and often leads to suboptimal performance in high-sparsity regimes. Unlike unstructured pruning, structured pruning produces regular sparsity patterns that align well with GPU kernels and library optimizations, making it more hardware-efficient. However, structured pruning typically relies on global pruning, since structured patterns are more prone to severe performance degradation under local optimization. To jointly achieve structured pruning and the memory efficiency of local pruning, we propose a divide-and-conquer strategy that decomposes the global pruning problem into coordinated subproblems across different modules, each of which fits within limited GPU memory. Building on this idea, we design \\textbf{STRUPRUNE}, an ADMM-based framework that integrates structured sparsity into the pruning process, combining the memory efficiency of local pruning with the hardware compatibility of structured methods. We derive a closed-form analytical solution for structured pruning masks that provides an explicit rule for layer-wise sparsity allocation, and further develop an energy-based asymptotic framework yielding a softmax-form allocation scheme that simplifies optimization while adapting to heterogeneous layer importance. Experiments demonstrate that STRUPRUNE matches the perplexity of global structured pruning while reducing memory cost from $\\mathcal{O}(N)$ to $\\mathcal{O}(\\sqrt{N})$, enabling practical deployment at the billion-parameter scale.",
        "arxiv_id": "2510.03246"
    },
    "2510.04008": {
        "SCORE": 18,
        "ARXIVID": "2510.04008",
        "COMMENT": "Model Architecture/Efficiency: linear-time attention via sharpened angular similarity with randomized projections and soft LSH; enables ultra-long-context training with system-level runtime/memory gains.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Sahil Joshi",
            "Agniva Chowdhury",
            "Amar Kanakamedala",
            "Ekam Singh",
            "Evan Tu",
            "Anshumali Shrivastava"
        ],
        "title": "Replacing Softmax Similarity with a Sharpened Angular Similarity: Theory and Practice of Scaling To Billion-Context Attention",
        "abstract": "Softmax Attention has a quadratic time complexity, which becomes prohibitive to run at long contexts, even with highly optimized GPU kernels. For example, FlashAttention (an exact, GPU-optimized implementation of Softmax Attention) cannot complete a single forward-backward pass of a multi-head attention layer once the context exceeds ~4 million tokens on an NVIDIA GH200 (96 GB). We introduce RACE Attention, a kernel-inspired alternative to Softmax Attention that is linear in sequence length and embedding dimension. RACE Attention replaces the exponential kernel with a sharpened angular (cosine) similarity, and approximates attention outputs via randomized projections and soft Locality-Sensitive Hashing (LSH). Across language modeling, masked language modeling, and text classification, RACE Attention matches the accuracy of strong baselines while reducing runtime and memory. In a controlled scale test, it processes up to 12 million tokens during a single forward-backward pass on an NVIDIA GH200 GPU and 75 million tokens on an Intel Xeon Gold 5220R CPU, well beyond the practical limits of the current state-of-the-art attention implementations. RACE Attention thus offers a practical, theoretically grounded mechanism for outrageously long context windows on today's hardware. We hope that it gets adopted in practice.",
        "arxiv_id": "2510.04008"
    },
    "2510.03511": {
        "SCORE": 18,
        "ARXIVID": "2510.03511",
        "COMMENT": "Model Architecture: symmetry-equivariant Transformer via Platonic group reference frames with principled weight sharing; Efficiency: equivalent dynamic group convolution enabling a linear-time variant.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Mohammad Mohaiminul Islam",
            "Rishabh Anand",
            "David R. Wessels",
            "Friso de Kruiff",
            "Thijs P. Kuipers",
            "Rex Ying",
            "Clara I. S\\'anchez",
            "Sharvaree Vadgama",
            "Georg B\\\"okman",
            "Erik J. Bekkers"
        ],
        "title": "Platonic Transformers: A Solid Choice For Equivariance",
        "abstract": "While widespread, Transformers lack inductive biases for geometric symmetries common in science and computer vision. Existing equivariant methods often sacrifice the efficiency and flexibility that make Transformers so effective through complex, computationally intensive designs. We introduce the Platonic Transformer to resolve this trade-off. By defining attention relative to reference frames from the Platonic solid symmetry groups, our method induces a principled weight-sharing scheme. This enables combined equivariance to continuous translations and Platonic symmetries, while preserving the exact architecture and computational cost of a standard Transformer. Furthermore, we show that this attention is formally equivalent to a dynamic group convolution, which reveals that the model learns adaptive geometric filters and enables a highly scalable, linear-time convolutional variant. Across diverse benchmarks in computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular property prediction (QM9, OMol25), the Platonic Transformer achieves competitive performance by leveraging these geometric constraints at no additional cost.",
        "arxiv_id": "2510.03511"
    },
    "2510.04500": {
        "SCORE": 18,
        "ARXIVID": "2510.04500",
        "COMMENT": "Compression/Efficiency + Architecture: Fixed Parameter Expansion widens networks at constant non-zero parameter count to reduce superposition/interference, improving accuracy without more stored weights.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Linghao Kong",
            "Inimai Subramanian",
            "Yonadav Shavit",
            "Micah Adler",
            "Dan Alistarh",
            "Nir Shavit"
        ],
        "title": "Expand Neurons, Not Parameters",
        "abstract": "This work demonstrates how increasing the number of neurons in a network without increasing its number of non-zero parameters improves performance. We show that this gain corresponds with a decrease in interference between multiple features that would otherwise share the same neurons. To reduce such entanglement at a fixed non-zero parameter count, we introduce Fixed Parameter Expansion (FPE): replace a neuron with multiple children and partition the parent's weights disjointly across them, so that each child inherits a non-overlapping subset of connections. On symbolic tasks, specifically Boolean code problems, clause-aligned FPE systematically reduces polysemanticity metrics and yields higher task accuracy. Notably, random splits of neuron weights approximate these gains, indicating that reduced collisions, not precise assignment, are a primary driver. Consistent with the superposition hypothesis, the benefits of FPE grow with increasing interference: when polysemantic load is high, accuracy improvements are the largest. Transferring these insights to real models (classifiers over CLIP embeddings and deeper multilayer networks) we find that widening networks while maintaining a constant non-zero parameter count consistently increases accuracy. These results identify an interpretability-grounded mechanism to leverage width against superposition, improving performance without increasing the number of non-zero parameters. Such a direction is well matched to modern accelerators, where memory movement of non-zero parameters, rather than raw compute, is the dominant bottleneck.",
        "arxiv_id": "2510.04500"
    },
    "2508.04581": {
        "SCORE": 18,
        "ARXIVID": "2508.04581",
        "COMMENT": "Model Compression/Efficiency: inter-layer weight sharing via matrix dictionary atoms in attention; large parameter reduction without distillation.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Magauiya Zhussip",
            "Dmitriy Shopkhoev",
            "Ammar Ali",
            "Stamatios Lefkimmiatis"
        ],
        "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning",
        "abstract": "Large language models (LLMs) have revolutionized AI applications, yet their high computational and memory demands hinder their widespread deployment. Existing compression techniques focus on intra-block optimizations (e.g. low-rank approximation, attention head pruning), while the repetitive layered structure of transformers implies significant inter-block redundancy - a dimension largely unexplored beyond key-value (KV) caching. Inspired by dictionary learning in CNNs, we propose a framework for structured weight sharing across transformer layers. Our approach decomposes attention projection matrices into shared dictionary atoms, reducing the attention module's parameters by 66.7% while achieving on-par performance. Unlike complex methods requiring distillation or architectural changes, MASA (Matrix Atom Sharing in Attention) operates as a drop-in replacement - trained with standard optimizers - and represents each layer's weights as linear combinations of shared matrix atoms. Experiments across scales (100M-700M parameters) show that MASA achieves better benchmark accuracy and perplexity than grouped-query attention (GQA), low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at comparable parameter budgets. Ablation studies confirm robustness to the dictionary size and the efficacy of shared representations in capturing cross-layer statistical regularities. Extending to Vision Transformers (ViT), MASA matches performance metrics on image classification and detection tasks with 66.7% fewer attention parameters. By combining dictionary learning strategies with transformer efficiency, MASA offers a scalable blueprint for parameter-efficient models without sacrificing performance. Finally, we investigate the possibility of employing MASA on pretrained LLMs to reduce their number of parameters without experiencing any significant drop in their performance.",
        "arxiv_id": "2508.04581"
    },
    "2510.03279": {
        "SCORE": 18,
        "ARXIVID": "2510.03279",
        "COMMENT": "Model Architecture/Efficiency: analyzes memory decay in SSMs (Mamba) and introduces MemMamba with state summarization and cross-layer/token attention to preserve long-range memory at linear complexity.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Youjin Wang",
            "Yangjingyi Chen",
            "Jiahao Yan",
            "Jiaxuan Lu",
            "Xiao Sun"
        ],
        "title": "MemMamba: Rethinking Memory Patterns in State Space Model",
        "abstract": "With the explosive growth of data, long-sequence modeling has become increasingly important in tasks such as natural language processing and bioinformatics. However, existing methods face inherent trade-offs between efficiency and memory. Recurrent neural networks suffer from gradient vanishing and explosion, making them hard to scale. Transformers can model global dependencies but are constrained by quadratic complexity. Recently, selective state-space models such as Mamba have demonstrated high efficiency with O(n) time and O(1) recurrent inference, yet their long-range memory decays exponentially. In this work, we conduct mathematical derivations and information-theoretic analysis to systematically uncover the memory decay mechanism of Mamba, answering a fundamental question: what is the nature of Mamba's long-range memory and how does it retain information? To quantify key information loss, we further introduce horizontal-vertical memory fidelity metrics that capture degradation both within and across layers. Inspired by how humans distill and retain salient information when reading long documents, we propose MemMamba, a novel architectural framework that integrates state summarization mechanism together with cross-layer and cross-token attention, which alleviates long-range forgetting while preserving linear complexity. MemMamba achieves significant improvements over existing Mamba variants and Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval, while delivering a 48% speedup in inference efficiency. Both theoretical analysis and empirical results demonstrate that MemMamba achieves a breakthrough in the complexity-memory trade-off, offering a new paradigm for ultra-long sequence modeling.",
        "arxiv_id": "2510.03279"
    },
    "2510.03267": {
        "SCORE": 18,
        "ARXIVID": "2510.03267",
        "COMMENT": "Model Compression and Efficiency \u2014 post-training ternarization for LLMs via an asymmetric ternary quantizer with training-free iterative fitting and activation-aware grid alignment, plus structural similarity\u2013based reordering; demonstrates memory and speed gains.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Xianglong Yan",
            "Chengzhu Bao",
            "Zhiteng Li",
            "Tianao Zhang",
            "Kaicheng Yang",
            "Haotong Qin",
            "Ruobing Xie",
            "Xingwu Sun",
            "Yulun Zhang"
        ],
        "title": "PT$^2$-LLM: Post-Training Ternarization for Large Language Models",
        "abstract": "Large Language Models (LLMs) have shown impressive capabilities across diverse tasks, but their large memory and compute demands hinder deployment. Ternarization has gained attention as a promising compression technique, delivering substantial size reduction and high computational efficiency. However, its potential in the post-training quantization (PTQ) setting remains underexplored, due to the challenge of training-free parameter optimization and the quantization difficulty posed by outliers and dispersed weights. To address these issues, we propose PT$^2$-LLM, a post-training ternarization framework tailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with a two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which alternates between optimal ternary grid construction and flexible rounding to minimize quantization error, and (2) Activation-aware Grid Alignment (AGA), which further refines the ternary grid to better match full-precision outputs. In addition, we propose a plug-and-play Structural Similarity-based Reordering (SSR) strategy that leverages inter-column structural similarity to ease quantization and mitigate outlier effects, further enhancing overall performance. Extensive experiments demonstrate that PT$^2$-LLM delivers competitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with lower memory cost, while also accelerating both prefill and decoding to achieve end-to-end speedup. The code and models will be available at https://github.com/XIANGLONGYAN/PT2-LLM.",
        "arxiv_id": "2510.03267"
    },
    "2510.04212": {
        "SCORE": 18,
        "ARXIVID": "2510.04212",
        "COMMENT": "Model Compression and Efficiency + ML Systems \u2014 mechanistic analysis of low-precision Transformer training failures in FlashAttention and a minimal kernel modification to stabilize training.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Haiquan Qiu",
            "Quanming Yao"
        ],
        "title": "Why Low-Precision Transformer Training Fails: An Analysis on Flash Attention",
        "abstract": "The pursuit of computational efficiency has driven the adoption of low-precision formats for training transformer models. However, this progress is often hindered by notorious training instabilities. This paper provides the first mechanistic explanation for a long-standing and unresolved failure case where training with flash attention in low-precision settings leads to catastrophic loss explosions. Our in-depth analysis reveals that the failure is not a random artifact but caused by two intertwined phenomena: the emergence of similar low-rank representations within the attention mechanism and the compounding effect of biased rounding errors inherent in low-precision arithmetic. We demonstrate how these factors create a vicious cycle of error accumulation that corrupts weight updates, ultimately derailing the training dynamics. To validate our findings, we introduce a minimal modification to the flash attention that mitigates the bias in rounding errors. This simple change stabilizes the training process, confirming our analysis and offering a practical solution to this persistent problem.",
        "arxiv_id": "2510.04212"
    },
    "2510.03291": {
        "SCORE": 18,
        "ARXIVID": "2510.03291",
        "COMMENT": "Model Compression and Efficiency: unified post-training pruning (unstructured and N:M) with global coordination and one-shot masks without weight updates.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Yizhuo Ding",
            "Wanying Qu",
            "Jiawei Geng",
            "Wenqi Shao",
            "Yanwei Fu"
        ],
        "title": "UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs",
        "abstract": "Large Language Models (LLMs) achieve strong performance across diverse tasks but face prohibitive computational and memory costs. Pruning offers a promising path by inducing sparsity while preserving architectural flexibility. However, existing methods struggle to balance efficiency and robustness: local metric approaches prune layer by layer but often collapse under high sparsity, whereas global feedback methods enforce consistency at the cost of expensive weight updates or restrictive semi-structured formats. We present UniPruning, a unified post-training pruning framework that combines the speed of local saliency metrics with the stability of global coordination, enabled by a mirror descent based optimization, all without updating model weights. UniPruning leverages fast layer-wise scoring and a lightweight global controller to allocate a single sparsity budget, supporting both unstructured and semi-structured N :M pruning within one framework. After a brief calibration, it can generate pruning masks for arbitrary sparsity levels in one shot, and adapts seamlessly to hardware-aware constraints. Extensive experiments on multiple pretrained LLM families and standard benchmarks show that UniPruning consistently delivers competitive or superior perplexity and zero-shot accuracy. Ablation studies further highlight the importance of mirror descent and local saliency anchoring. Overall, UniPruning provides an efficient, principled, and scalable solution for sparsifying large-scale LLMs. Our code is available at: https://github.com/RainbowQTT/UniPruning.",
        "arxiv_id": "2510.03291"
    },
    "2510.03293": {
        "SCORE": 18,
        "ARXIVID": "2510.03293",
        "COMMENT": "Model Architecture (MoE) + ML Systems: plug-and-play inference-time routing for load balancing using gate-score distributions without retraining.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Rana Shahout",
            "Colin Cai",
            "Yilun Du",
            "Minlan Yu",
            "Michael Mitzenmacher"
        ],
        "title": "From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing",
        "abstract": "Mixture-of-Experts (MoE) models can scale parameter capacity by routing each token to a subset of experts through a learned gate function. While conditional routing reduces training costs, it shifts the burden on inference memory: expert parameters and activations consume memory, limiting the number of experts per device. As tokens are routed, some experts become overloaded while others are underutilized. Because experts are mapped to GPUs, this imbalance translates directly into degraded system performance in terms of latency, throughput, and cost. We present LASER, a plug-and-play, inference-time routing algorithm that balances load while preserving accuracy. LASER adapts to the shape of the gate's score distribution. When scores provide a clear preference, it routes to the strongest experts; when scores are more uniform, it broadens the set of viable experts and routes to the least-loaded among them. Because LASER relies only on gate scores from a trained model, it integrates directly into existing MoE inference pipelines without retraining or finetuning. We evaluate LASER on Mixtral-8x7B and DeepSeek-MoE-16b-chat across four datasets (ARC-Easy, ARC-Challenge, MMLU, and GSM8K). LASER improves load balancing, translating into lower latency and higher throughput, while keeping the accuracy changes negligible.",
        "arxiv_id": "2510.03293"
    },
    "2510.04944": {
        "SCORE": 18,
        "ARXIVID": "2510.04944",
        "COMMENT": "Model Architecture and Efficiency: formalizes SSM\u2013masked-attention duality, extends to diagonal SSMs, gives equivalence conditions and complexity bounds.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Jerry Yao-Chieh Hu",
            "Xiwen Zhang",
            "Weimin Wu",
            "Han Liu"
        ],
        "title": "On Structured State-Space Duality",
        "abstract": "Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence between a simple Structured State-Space Model (SSM) and a masked attention mechanism. In particular, a state-space model with a scalar-times-identity state matrix is equivalent to a masked self-attention with a $1$-semiseparable causal mask. Consequently, the same sequence transformation (model) has two algorithmic realizations: as a linear-time $O(T)$ recurrence or as a quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize this duality: (i) we extend SSD from the scalar-identity case to general diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs match the scalar case's training complexity lower bounds while supporting richer dynamics; (iii) we establish a necessary and sufficient condition under which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we show that such duality fails to extend to standard softmax attention due to rank explosion. Together, these results tighten bridge between recurrent SSMs and Transformers, and widen the design space for expressive yet efficient sequence models.",
        "arxiv_id": "2510.04944"
    },
    "2505.02819": {
        "SCORE": 18,
        "ARXIVID": "2505.02819",
        "COMMENT": "Model Compression and Efficiency: training-free depth pruning by linearizing Transformer blocks with calibration-only merging; practical LLM compression with open-source code.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Dmitriy Shopkhoev",
            "Ammar Ali",
            "Magauiya Zhussip",
            "Valentin Malykh",
            "Stamatios Lefkimmiatis",
            "Nikos Komodakis",
            "Sergey Zagoruyko"
        ],
        "title": "ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization",
        "abstract": "We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation, which approximates the pruned blocks. The estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at https://github.com/mts-ai/ReplaceMe.",
        "arxiv_id": "2505.02819"
    },
    "2510.03989": {
        "SCORE": 18,
        "ARXIVID": "2510.03989",
        "COMMENT": "Matches Model Architecture: offers a continuous integro-differential formulation of Transformers (attention as non-local integral operator, layer norm as projection), yielding a rigorous architectural interpretation.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Xue-Cheng Tai",
            "Hao Liu",
            "Lingfeng Li",
            "Raymond H. Chan"
        ],
        "title": "A Mathematical Explanation of Transformers for Large Language Models and GPTs",
        "abstract": "The Transformer architecture has revolutionized the field of sequence modeling and underpins the recent breakthroughs in large language models (LLMs). However, a comprehensive mathematical theory that explains its structure and operations remains elusive. In this work, we propose a novel continuous framework that rigorously interprets the Transformer as a discretization of a structured integro-differential equation. Within this formulation, the self-attention mechanism emerges naturally as a non-local integral operator, and layer normalization is characterized as a projection to a time-dependent constraint. This operator-theoretic and variational perspective offers a unified and interpretable foundation for understanding the architecture's core components, including attention, feedforward layers, and normalization. Our approach extends beyond previous theoretical analyses by embedding the entire Transformer operation in continuous domains for both token indices and feature dimensions. This leads to a principled and flexible framework that not only deepens theoretical insight but also offers new directions for architecture design, analysis, and control-based interpretations. This new interpretation provides a step toward bridging the gap between deep learning architectures and continuous mathematical modeling, and contributes a foundational perspective to the ongoing development of interpretable and theoretically grounded neural network models.",
        "arxiv_id": "2510.03989"
    },
    "2510.03272": {
        "SCORE": 17,
        "ARXIVID": "2510.03272",
        "COMMENT": "Representation Learning/Architecture Theory: recasts Transformer as a continuous PDE system to explain necessity of residuals and layer norm (stability analysis).",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yukun Zhang",
            "Xueqing Zhou"
        ],
        "title": "PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling",
        "abstract": "The Transformer architecture has revolutionized artificial intelligence, yet a principled theoretical understanding of its internal mechanisms remains elusive. This paper introduces a novel analytical framework that reconceptualizes the Transformer's discrete, layered structure as a continuous spatiotemporal dynamical system governed by a master Partial Differential Equation (PDE). Within this paradigm, we map core architectural components to distinct mathematical operators: self-attention as a non-local interaction, the feed-forward network as a local reaction, and, critically, residual connections and layer normalization as indispensable stabilization mechanisms. We do not propose a new model, but rather employ the PDE system as a theoretical probe to analyze the mathematical necessity of these components. By comparing a standard Transformer with a PDE simulator that lacks explicit stabilizers, our experiments provide compelling empirical evidence for our central thesis. We demonstrate that without residual connections, the system suffers from catastrophic representational drift, while the absence of layer normalization leads to unstable, explosive training dynamics. Our findings reveal that these seemingly heuristic \"tricks\" are, in fact, fundamental mathematical stabilizers required to tame an otherwise powerful but inherently unstable continuous system. This work offers a first-principles explanation for the Transformer's design and establishes a new paradigm for analyzing deep neural networks through the lens of continuous dynamics.",
        "arxiv_id": "2510.03272"
    },
    "2510.03283": {
        "SCORE": 17,
        "ARXIVID": "2510.03283",
        "COMMENT": "ML Systems \u2014 colocated inference and fine-tuning with iteration-level scheduling and memory management for SLO-aware continual retraining on edge GPUs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yufei Li",
            "Yu Fu",
            "Yue Dong",
            "Cong Liu"
        ],
        "title": "MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment",
        "abstract": "Large language models (LLMs) deployed on edge servers are increasingly used in latency-sensitive applications such as personalized assistants, recommendation, and content moderation. However, the non-stationary nature of user data necessitates frequent retraining, which introduces a fundamental tension between inference latency and model accuracy under constrained GPU resources. Existing retraining strategies either delay model updates, over-commit resources to retraining, or overlook iteration-level retraining granularity. In this paper, we identify that iteration-level scheduling is crucial for adapting retraining frequency to model drift without violating service-level objectives (SLOs). We propose MACE, a hybrid LLM system that colocates concurrent inference (prefill, decode) and fine-tuning, with intelligent memory management to maximize task performance while promising inference throughput. MACE leverages the insight that not all model updates equally affect output alignment and allocates GPU cycles accordingly to balance throughput, latency, and update freshness. Our trace-driven evaluation shows that MACE matches or exceeds continuous retraining while reducing inference latency by up to 63% and maintaining throughput under resource constraints. Compared to periodic retraining, MACE improves latency breakdown across prefill, decode, and finetune stages, and sustains GPU utilization above 85% in NVIDIA AGX Orin. These results demonstrate that iteration-level hybrid scheduling is a promising direction for deploying LLMs with continual learning capabilities on edge platforms.",
        "arxiv_id": "2510.03283"
    },
    "2510.04327": {
        "SCORE": 17,
        "ARXIVID": "2510.04327",
        "COMMENT": "Representation Learning \u2014 unified \u03bcP-style learning-rate scaling law (\u03b7* \u221d L^{-3/2}) and initialization for CNNs/ResNets enabling robust depth scaling.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Haosong Zhang",
            "Shenxi Wu",
            "Yichi Zhang",
            "Wei Lin"
        ],
        "title": "Arithmetic-Mean $\\mu$P for Modern Architectures: A Unified Learning-Rate Scale for CNNs and ResNets",
        "abstract": "Choosing an appropriate learning rate remains a key challenge in scaling depth of modern deep networks. The classical maximal update parameterization ($\\mu$P) enforces a fixed per-layer update magnitude, which is well suited to homogeneous multilayer perceptrons (MLPs) but becomes ill-posed in heterogeneous architectures where residual accumulation and convolutions introduce imbalance across layers. We introduce Arithmetic-Mean $\\mu$P (AM-$\\mu$P), which constrains not each individual layer but the network-wide average one-step pre-activation second moment to a constant scale. Combined with a residual-aware He fan-in initialization - scaling residual-branch weights by the number of blocks ($\\mathrm{Var}[W]=c/(K\\cdot \\mathrm{fan\\text{-}in})$) - AM-$\\mu$P yields width-robust depth laws that transfer consistently across depths. We prove that, for one- and two-dimensional convolutional networks, the maximal-update learning rate satisfies $\\eta^\\star(L)\\propto L^{-3/2}$; with zero padding, boundary effects are constant-level as $N\\gg k$. For standard residual networks with general conv+MLP blocks, we establish $\\eta^\\star(L)=\\Theta(L^{-3/2})$, with $L$ the minimal depth. Empirical results across a range of depths confirm the $-3/2$ scaling law and enable zero-shot learning-rate transfer, providing a unified and practical LR principle for convolutional and deep residual networks without additional tuning overhead.",
        "arxiv_id": "2510.04327"
    },
    "2510.03871": {
        "SCORE": 17,
        "ARXIVID": "2510.03871",
        "COMMENT": "Training dynamics/optimal scaling \u2014 identifies an operator-norm invariant governing optimal (\u03b7,B) across model/data scales; provides scaling rules and large-scale validation.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Oleg Filatov",
            "Jiangtao Wang",
            "Jan Ebert",
            "Stefan Kesselheim"
        ],
        "title": "Optimal Scaling Needs Optimal Norm",
        "abstract": "Despite recent progress in optimal hyperparameter transfer under model and dataset scaling, no unifying explanatory principle has been established. Using the Scion optimizer, we discover that joint optimal scaling across model and dataset sizes is governed by a single invariant: the operator norm of the output layer. Across models with up to 1.3B parameters trained on up to 138B tokens, the optimal learning rate/batch size pair $(\\eta^{\\ast}, B^{\\ast})$ consistently has the same operator norm value - a phenomenon we term norm transfer. This constant norm condition is necessary but not sufficient: while for each dataset size, multiple $(\\eta, B)$ reach the optimal norm, only a unique $(\\eta^{\\ast}, B^{\\ast})$ achieves the best loss. As a sufficient condition, we provide the first measurement of $(\\eta^{\\ast}, B^{\\ast})$ scaling with dataset size for Scion, and find that the scaling rules are consistent with those of the Adam optimizer. Tuning per-layer-group learning rates also improves model performance, with the output layer being the most sensitive and hidden layers benefiting from lower learning rates. We provide practical insights on norm-guided optimal scaling and release our Distributed Scion (Disco) implementation with logs from over two thousand runs to support research on LLM training dynamics at scale.",
        "arxiv_id": "2510.03871"
    },
    "2510.03784": {
        "SCORE": 17,
        "ARXIVID": "2510.03784",
        "COMMENT": "Model Architecture: theoretical guidance for allocating attention heads/dimensions across Transformer layers; identifies softmax saturation and budget-aware strategies.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Ruoxi Yu",
            "Haotian Jiang",
            "Jingpu Cheng",
            "Penghao Yu",
            "Qianxiao Li",
            "Zhong Li"
        ],
        "title": "Allocation of Parameters in Transformers",
        "abstract": "Transformers have achieved remarkable successes across a wide range of applications, yet the theoretical foundation of their model efficiency remains underexplored. In this work, we investigate how the model parameters -- mainly attention heads and head dimensions -- should be allocated across layers to balance expressivity and efficiency. We first provide mathematical analysis on the role of early layers in information extraction from an approximation perspective, with a theoretical characterization on the trade-off between the number of heads and head dimension under a fixed parameter budget. In addition, we uncover and prove the \\emph{saturation} behavior of softmax activations: Continuously increasing head dimensions can lead to diminishing returns in learning errors, particularly for long sequences. Supported by both theory and experiments, this saturation pattern suggests that later layers can operate more efficiently with reduced parameters. Combining these insights, we propose principled strategies for allocating attention heads and dimensions across Transformers' layers, shedding light on theoretically-grounded model efficiency of Transformer-based architectures.",
        "arxiv_id": "2510.03784"
    },
    "2510.04547": {
        "SCORE": 17,
        "ARXIVID": "2510.04547",
        "COMMENT": "Compression/Efficiency: training-free post-training quantization for transformer vision encoders via prefix tokens to mitigate activation outliers.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Seunghyeon Kim",
            "Jinho Kim",
            "Taesun Yeom",
            "Wonpyo Park",
            "Kyuyeun Kim",
            "Jaeho Lee"
        ],
        "title": "Post-training quantization of vision encoders needs prefixing registers",
        "abstract": "Transformer-based vision encoders -- such as CLIP -- are central to multimodal intelligence, powering applications from autonomous web agents to robotic control. Since these applications often demand real-time processing of massive visual data, reducing the inference cost of vision encoders is critical. Post-training quantization offers a practical path, but remains challenging even at 8-bit precision due to massive-scale activations (i.e., outliers). In this work, we propose $\\textit{RegCache}$, a training-free algorithm to mitigate outliers in vision encoders, enabling quantization with significantly smaller accuracy drops. The proposed RegCache introduces outlier-prone yet semantically meaningless prefix tokens to the target vision encoder, which prevents other tokens from having outliers. Notably, we observe that outliers in vision encoders behave differently from those in language models, motivating two technical innovations: middle-layer prefixing and token deletion. Experiments show that our method consistently improves the accuracy of quantized models across both text-supervised and self-supervised vision encoders.",
        "arxiv_id": "2510.04547"
    },
    "2510.03268": {
        "SCORE": 17,
        "ARXIVID": "2510.03268",
        "COMMENT": "Representation Learning: provides the first theoretical framework explaining the modality gap in multimodal contrastive learning via dimension collapse and its impact on pair alignment, with prescriptions (hyperplane rotation/shared projection).",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Lingjie Yi",
            "Raphael Douady",
            "Chao Chen"
        ],
        "title": "Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment",
        "abstract": "Multimodal contrastive learning (MCL) aims to embed data from different modalities in a shared embedding space. However, empirical evidence shows that representations from different modalities occupy completely separate regions of embedding space, a phenomenon referred to as the modality gap. Moreover, experimental findings on how the size of the modality gap influences downstream performance are inconsistent. These observations raise two key questions: (1) What causes the modality gap? (2) How does it affect downstream tasks? To address these questions, this paper introduces the first theoretical framework for analyzing the convergent optimal representations of MCL and the modality alignment when training is optimized. Specifically, we prove that without any constraint or under the cone constraint, the modality gap converges to zero. Under the subspace constraint (i.e., representations of two modalities fall into two distinct hyperplanes due to dimension collapse), the modality gap converges to the smallest angle between the two hyperplanes. This result identifies \\emph{dimension collapse} as the fundamental origin of the modality gap. Furthermore, our theorems demonstrate that paired samples cannot be perfectly aligned under the subspace constraint. The modality gap influences downstream performance by affecting the alignment between sample pairs. We prove that, in this case, perfect alignment between two modalities can still be achieved via two ways: hyperplane rotation and shared space projection.",
        "arxiv_id": "2510.03268"
    },
    "2510.04295": {
        "SCORE": 17,
        "ARXIVID": "2510.04295",
        "COMMENT": "Model Compression and Efficiency: advances Low-Rank Adaptation by sharing low-rank updates across attention heads via joint hypernetworks; theoretical sample-efficiency gains and links to hierarchical MoE.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Nghiem T. Diep",
            "Dung Le",
            "Tuan Truong",
            "Tan Dinh",
            "Huy Nguyen",
            "Nhat Ho"
        ],
        "title": "HoRA: Cross-Head Low-Rank Adaptation with Joint Hypernetworks",
        "abstract": "Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) technique that adapts large pre-trained models by adding low-rank matrices to their weight updates. However, in the context of fine-tuning multi-head self-attention (MHA), LoRA has been employed to adapt each attention head separately, thereby overlooking potential synergies across different heads. To mitigate this issue, we propose a novel Hyper-shared Low-Rank Adaptation (HoRA) method, which utilizes joint hypernetworks to generate low-rank matrices across attention heads. By coupling their adaptation through a shared generator, HoRA encourages cross-head information sharing, and thus directly addresses the aforementioned limitation of LoRA. By comparing LoRA and HoRA through the lens of hierarchical mixture of experts, our theoretical findings reveal that the latter achieves superior sample efficiency to the former. Furthermore, through extensive experiments across diverse language and vision benchmarks, we demonstrate that HoRA outperforms LoRA and other PEFT methods while requiring only a marginal increase in the number of trainable parameters.",
        "arxiv_id": "2510.04295"
    },
    "2510.03638": {
        "SCORE": 17,
        "ARXIVID": "2510.03638",
        "COMMENT": "Model Architecture/Theory: analyzes implicit fixed-point models and proves expressive power scales with test-time iterations while training with constant memory.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jialin Liu",
            "Lisang Ding",
            "Stanley Osher",
            "Wotao Yin"
        ],
        "title": "Implicit Models: Expressive Power Scales with Test-Time Compute",
        "abstract": "Implicit models, an emerging model class, compute outputs by iterating a single parameter block to a fixed point. This architecture realizes an infinite-depth, weight-tied network that trains with constant memory, significantly reducing memory needs for the same level of performance compared to explicit models. While it is empirically known that these compact models can often match or even exceed larger explicit networks by allocating more test-time compute, the underlying mechanism remains poorly understood.   We study this gap through a nonparametric analysis of expressive power. We provide a strict mathematical characterization, showing that a simple and regular implicit operator can, through iteration, progressively express more complex mappings. We prove that for a broad class of implicit models, this process lets the model's expressive power scale with test-time compute, ultimately matching a much richer function class. The theory is validated across three domains: image reconstruction, scientific computing, and operations research, demonstrating that as test-time iterations increase, the complexity of the learned mapping rises, while the solution quality simultaneously improves and stabilizes.",
        "arxiv_id": "2510.03638"
    },
    "2510.04067": {
        "SCORE": 17,
        "ARXIVID": "2510.04067",
        "COMMENT": "Representation Learning / Training Dynamics \u2014 decomposes cross-entropy into error-entropy, self-alignment, and confidence; identifies error-entropy as the true scaling component at large scale.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Junxi Yan",
            "Zixi Wei",
            "Jingtao Zhan",
            "Qingyao Ai",
            "Yiqun Liu"
        ],
        "title": "What Scales in Cross-Entropy Scaling Law?",
        "abstract": "The cross-entropy scaling law has long served as a key tool for guiding the development of large language models. It shows that cross-entropy loss decreases in a predictable power-law rate as the model size increases. However, recent evidence indicates that this law breaks down at very large scales: the loss decreases more slowly than expected, which causes significant trouble for developing large language models. In this paper, we hypothesize that the root cause lies in the fact that cross-entropy itself does not truly scale; instead, only one of its hidden components does. To investigate this, we introduce a novel decomposition of cross-entropy into three parts: Error-Entropy, Self-Alignment, and Confidence. We show both theoretically and empirically that this decomposition precisely captures the training dynamics and optimization objectives. Through extensive experiments on multiple datasets and 32 models spanning five orders of magnitude in size, we find that only error-entropy follows a robust power-law scaling, while the other two terms remain largely invariant. Moreover, error-entropy constitutes the dominant share of cross-entropy in small models but diminishes in proportion as models grow larger. This explains why the cross-entropy scaling law appears accurate at small scales but fails at very large ones. Our findings establish the error-entropy scaling law as a more accurate description of model behavior. We believe it will have wide applications in the training, understanding, and future development of large language models.",
        "arxiv_id": "2510.04067"
    },
    "2510.04130": {
        "SCORE": 17,
        "ARXIVID": "2510.04130",
        "COMMENT": "Representation Learning/Training Dynamics: theoretical characterization of position embeddings for length generalization (LRC/SRC) with practical PE learning strategies.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yang Chen",
            "Yitao Liang",
            "Zhouchen Lin"
        ],
        "title": "On the Limitations and Capabilities of Position Embeddings for Length Generalization",
        "abstract": "In Transformers, Position Embeddings (PEs) significantly influence Length Generalization (LG) performance, yet their fundamental role remains unclear. In this work, we investigate the limitations and capabilities of PEs in achieving LG. We theoretically analyze PEs in Position-Only Linear Attentions (POLAs), introducing Linear Representation Complexity (LRC) to characterize when PEs enable LG. Our analysis shows that PEs do not expand computational capabilities but structure learned computations across positions. Extending to practical Transformers, we propose Sequential Representation Complexity (SRC) and conjecture that LG is possible if and only if SRC remains invariant across scales. We support this hypothesis with empirical evidence in various reasoning tasks. To enhance LG, we introduce Scale Hint, allowing flexible instance scaling, and a Learning-Based Position Embedding framework that automatically learns positional relations. Our work provides theoretical insights and practical strategies for improving LG in Transformers.",
        "arxiv_id": "2510.04130"
    },
    "2510.04205": {
        "SCORE": 17,
        "ARXIVID": "2510.04205",
        "COMMENT": "Matches Model Compression and Efficiency: introduces a provable framework for KAN compression via polyhedral region merging with epsilon-equivalence guarantees and optimal DP under error bounds.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Di Zhang"
        ],
        "title": "PolyKAN: A Polyhedral Analysis Framework for Provable and Minimal KAN Compression",
        "abstract": "Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to traditional Multi-Layer Perceptrons (MLPs), offering enhanced interpretability and a strong mathematical foundation. However, their parameter efficiency remains a significant challenge for practical deployment. This paper introduces PolyKAN, a novel theoretical framework for KAN compression that provides formal guarantees on both model size reduction and approximation error. By leveraging the inherent piecewise polynomial structure of KANs, we formulate the compression problem as one of optimal polyhedral region merging. We establish a rigorous polyhedral characterization of KANs, develop a complete theory of $\\epsilon$-equivalent compression, and design an optimal dynamic programming algorithm that guarantees minimal compression under specified error bounds. Our theoretical analysis demonstrates that PolyKAN achieves provably minimal compression while maintaining strict error control, with polynomial-time complexity in all network parameters. The framework provides the first formal foundation for KAN compression with mathematical guarantees, opening new directions for efficient deployment of interpretable neural architectures.",
        "arxiv_id": "2510.04205"
    },
    "2510.03507": {
        "SCORE": 17,
        "ARXIVID": "2510.03507",
        "COMMENT": "Matches ML Systems (distributed training/communication compression): introduces and analyzes error-feedback with dual averaging for composite optimization, giving convergence guarantees under compression.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yuan Gao",
            "Anton Rodomanov",
            "Jeremy Rack",
            "Sebastian Stich"
        ],
        "title": "Composite Optimization with Error Feedback: the Dual Averaging Approach",
        "abstract": "Communication efficiency is a central challenge in distributed machine learning training, and message compression is a widely used solution. However, standard Error Feedback (EF) methods (Seide et al., 2014), though effective for smooth unconstrained optimization with compression (Karimireddy et al., 2019), fail in the broader and practically important setting of composite optimization, which captures, e.g., objectives consisting of a smooth loss combined with a non-smooth regularizer or constraints. The theoretical foundation and behavior of EF in the context of the general composite setting remain largely unexplored. In this work, we consider composite optimization with EF. We point out that the basic EF mechanism and its analysis no longer stand when a composite part is involved. We argue that this is because of a fundamental limitation in the method and its analysis technique. We propose a novel method that combines Dual Averaging with EControl (Gao et al., 2024), a state-of-the-art variant of the EF mechanism, and achieves for the first time a strong convergence analysis for composite optimization with error feedback. Along with our new algorithm, we also provide a new and novel analysis template for inexact dual averaging method, which might be of independent interest. We also provide experimental results to complement our theoretical findings.",
        "arxiv_id": "2510.03507"
    },
    "2510.04044": {
        "SCORE": 16,
        "ARXIVID": "2510.04044",
        "COMMENT": "Model Compression and Efficiency: post-training quantization with provable locally convex range estimation and efficient search; strong fit to quantization criterion.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Bingtao Yang",
            "Yujia Wang",
            "Mengzhi Jiao",
            "Hongwei Huo"
        ],
        "title": "Quantization Range Estimation for Convolutional Neural Networks",
        "abstract": "Post-training quantization for reducing the storage of deep neural network models has been demonstrated to be an effective way in various tasks. However, low-bit quantization while maintaining model accuracy is a challenging problem. In this paper, we present a range estimation method to improve the quantization performance for post-training quantization. We model the range estimation into an optimization problem of minimizing quantization errors by layer-wise local minima. We prove this problem is locally convex and present an efficient search algorithm to find the optimal solution. We propose the application of the above search algorithm to the transformed weights space to do further improvement in practice. Our experiments demonstrate that our method outperforms state-of-the-art performance generally on top-1 accuracy for image classification tasks on the ResNet series models and Inception-v3 model. The experimental results show that the proposed method has almost no loss of top-1 accuracy in 8-bit and 6-bit settings for image classifications, and the accuracy of 4-bit quantization is also significantly improved. The code is available at https://github.com/codeiscommitting/REQuant.",
        "arxiv_id": "2510.04044"
    },
    "2510.03814": {
        "SCORE": 16,
        "ARXIVID": "2510.03814",
        "COMMENT": "Matches Representation Learning/training dynamics: algorithm to detect stable/unstable invariant manifolds in ReLU RNNs, clarifying multistability and chaos mechanisms.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Lukas Eisenmann",
            "Alena Br\\\"andle",
            "Zahra Monfared",
            "Daniel Durstewitz"
        ],
        "title": "Detecting Invariant Manifolds in ReLU-Based RNNs",
        "abstract": "Recurrent Neural Networks (RNNs) have found widespread applications in machine learning for time series prediction and dynamical systems reconstruction, and experienced a recent renaissance with improved training algorithms and architectural designs. Understanding why and how trained RNNs produce their behavior is important for scientific and medical applications, and explainable AI more generally. An RNN's dynamical repertoire depends on the topological and geometrical properties of its state space. Stable and unstable manifolds of periodic points play a particularly important role: They dissect a dynamical system's state space into different basins of attraction, and their intersections lead to chaotic dynamics with fractal geometry. Here we introduce a novel algorithm for detecting these manifolds, with a focus on piecewise-linear RNNs (PLRNNs) employing rectified linear units (ReLUs) as their activation function. We demonstrate how the algorithm can be used to trace the boundaries between different basins of attraction, and hence to characterize multistability, a computationally important property. We further show its utility in finding so-called homoclinic points, the intersections between stable and unstable manifolds, and thus establish the existence of chaos in PLRNNs. Finally we show for an empirical example, electrophysiological recordings from a cortical neuron, how insights into the underlying dynamics could be gained through our method.",
        "arxiv_id": "2510.03814"
    },
    "2510.04146": {
        "SCORE": 16,
        "ARXIVID": "2510.04146",
        "COMMENT": "Matches ML Systems/HPC: rigorous performance characterization of AR vs diffusion LMs, including arithmetic intensity, batching, and block-wise decoding implications for inference scalability.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Minseo Kim",
            "Coleman Hooper",
            "Aditya Tomar",
            "Chenfeng Xu",
            "Mehrdad Farajtabar",
            "Michael W. Mahoney",
            "Kurt Keutzer",
            "Amir Gholami"
        ],
        "title": "Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models",
        "abstract": "Large Language Models (LLMs) have achieved state-of-the-art performance on a broad range of Natural Language Processing (NLP) tasks, including document processing and coding. Autoregressive Language Models (ARMs), which generate tokens sequentially conditioned on all previous tokens, have been the predominant paradigm for LLMs. However, while these networks have achieved high accuracy across a range of downstream tasks, they exhibit low arithmetic intensity due to the inherent sequential dependency with next-token prediction. Recently, Diffusion Language Models (DLMs) have emerged as a promising alternative architecture. DLMs generate output text in parallel, breaking the limitations of sequential dependency. However, the performance implications of DLMs relative to commonly deployed ARMs are not fully understood. In this work, we present a comprehensive performance study analyzing the performance characteristics of ARMs and DLMs, using both theoretical analysis and profiling data to characterize the trade-offs between these approaches. We illustrate that although DLMs exhibit higher arithmetic intensity compared to ARMs because of their capability to utilize parallelism across sequence lengths, they fail to scale effectively to longer contexts. We then explore DLMs with block-wise decoding, outlining how this approach allows for increased arithmetic intensity, while still scaling well to long contexts (similar to ARMs). We also show interesting trade-offs for batched inference, where we find that ARMs exhibit superior throughput, as they benefit more from parallelism across sequences in the batch. Finally, we highlight opportunities for accelerating DLM inference, and, in particular, highlight the importance of reducing the number of sampling steps for allowing open-source DLMs to provide improved latency relative to ARMs.",
        "arxiv_id": "2510.04146"
    },
    "2510.03605": {
        "SCORE": 16,
        "ARXIVID": "2510.03605",
        "COMMENT": "Training dynamics/test-time compute \u2014 theoretical conditions when test-time scaling (longer CoTs) helps/hurts based on training data properties; generalizable insights.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Adel Javanmard",
            "Baharan Mirzasoleiman",
            "Vahab Mirrokni"
        ],
        "title": "Understanding the Role of Training Data in Test-Time Scaling",
        "abstract": "Test-time scaling improves the reasoning capabilities of large language models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts (CoTs). This enables models to tackle more complex problem by breaking them down into additional steps, backtracking, and correcting mistakes. Despite its strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions in the training data under which long CoTs emerge, and when such long CoTs improve the performance, remain unclear. In this paper, we study the performance of test-time scaling for transformers trained on an in-context weight prediction task for linear regression. Our analysis provides a theoretical explanation for several intriguing observations: First, at any fixed test error, increasing test-time compute allows us to reduce the number of in-context examples (context length) in training prompts. Second, if the skills required to solve a downstream task are not sufficiently present in the training data, increasing test-time compute can harm performance. Finally, we characterize task hardness via the smallest eigenvalue of its feature covariance matrix and show that training on a diverse, relevant, and hard set of tasks results in best performance for test-time scaling. We confirm our findings with experiments on large, nonlinear transformer architectures.",
        "arxiv_id": "2510.03605"
    },
    "2510.03273": {
        "SCORE": 16,
        "ARXIVID": "2510.03273",
        "COMMENT": "ML Systems/Training: local learning framework that decouples global backprop (eliminates update locking) enabling parallel training and lower memory, with theoretical monotonic improvement.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Chenhao Ye",
            "Ming Tang"
        ],
        "title": "Learning without Global Backpropagation via Synergistic Information Distillation",
        "abstract": "Backpropagation (BP), while foundational to deep learning, imposes two critical scalability bottlenecks: update locking, where network modules remain idle until the entire backward pass completes, and high memory consumption due to storing activations for gradient computation. To address these limitations, we introduce Synergistic Information Distillation (SID), a novel training framework that reframes deep learning as a cascade of local cooperative refinement problems. In SID, a deep network is structured as a pipeline of modules, each imposed with a local objective to refine a probabilistic belief about the ground-truth target. This objective balances fidelity to the target with consistency to the belief from its preceding module. By decoupling the backward dependencies between modules, SID enables parallel training and hence eliminates update locking and drastically reduces memory requirements. Meanwhile, this design preserves the standard feed-forward inference pass, making SID a versatile drop-in replacement for BP. We provide a theoretical foundation, proving that SID guarantees monotonic performance improvement with network depth. Empirically, SID consistently matches or surpasses the classification accuracy of BP, exhibiting superior scalability and pronounced robustness to label noise.Code is available at: https://github.com/ychAlbert/sid-bp",
        "arxiv_id": "2510.03273"
    },
    "2510.04309": {
        "SCORE": 16,
        "ARXIVID": "2510.04309",
        "COMMENT": "Representation/Control: activation steering cast as PID feedback control with interpretable error dynamics and stability connections; lightweight, modular steering framework for LLMs.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Dung V. Nguyen",
            "Hieu M. Vu",
            "Nhi Y. Pham",
            "Lei Zhang",
            "Tan M. Nguyen"
        ],
        "title": "Activation Steering with a Feedback Controller",
        "abstract": "Controlling the behaviors of large language models (LLM) is fundamental to their safety alignment and reliable deployment. However, existing steering methods are primarily driven by empirical insights and lack theoretical performance guarantees. In this work, we develop a control-theoretic foundation for activation steering by showing that popular steering methods correspond to the proportional (P) controllers, with the steering vector serving as the feedback signal. Building on this finding, we propose Proportional-Integral-Derivative (PID) Steering, a principled framework that leverages the full PID controller for activation steering in LLMs. The proportional (P) term aligns activations with target semantic directions, the integral (I) term accumulates errors to enforce persistent corrections across layers, and the derivative (D) term mitigates overshoot by counteracting rapid activation changes. This closed-loop design yields interpretable error dynamics and connects activation steering to classical stability guarantees in control theory. Moreover, PID Steering is lightweight, modular, and readily integrates with state-of-the-art steering methods. Extensive experiments across multiple LLM families and benchmarks demonstrate that PID Steering consistently outperforms existing approaches, achieving more robust and reliable behavioral control.",
        "arxiv_id": "2510.04309"
    },
    "2510.03659": {
        "SCORE": 16,
        "ARXIVID": "2510.03659",
        "COMMENT": "Representation Learning/Interpretability: large-scale study of Sparse Autoencoders for LLM steering and a new feature-selection metric (Delta Token Confidence) revealing an interpretability\u2013utility gap.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Xu Wang",
            "Yan Hu",
            "Benyou Wang",
            "Difan Zou"
        ],
        "title": "Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders",
        "abstract": "Sparse Autoencoders (SAEs) are widely used to steer large language models (LLMs), based on the assumption that their interpretable features naturally enable effective model behavior steering. Yet, a fundamental question remains unanswered: does higher interpretability indeed imply better steering utility? To answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B, Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels, and evaluate their interpretability and steering utility based on SAEBench (arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a rank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis reveals only a relatively weak positive association (tau b approx 0.298), indicating that interpretability is an insufficient proxy for steering performance. We conjecture the interpretability utility gap may stem from the selection of SAE features, as not all of them are equally effective for steering. To further find features that truly steer the behavior of LLMs, we propose a novel selection criterion called Delta Token Confidence, which measures how much amplifying a feature changes the next token distribution. We show that our method improves the steering performance of three LLMs by 52.52 percent compared to the current best output score based criterion (arXiv:2503.34567). Strikingly, after selecting features with high Delta Token Confidence, the correlation between interpretability and utility vanishes (tau b approx 0), and can even become negative. This further highlights the divergence between interpretability and utility for the most effective steering features.",
        "arxiv_id": "2510.03659"
    },
    "2510.05059": {
        "SCORE": 16,
        "ARXIVID": "2510.05059",
        "COMMENT": "ML Systems (Inference-serving): staircase streaming to reduce time-to-first-token in multi-agent LLM pipelines by streaming final responses from partial intermediate outputs.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Junlin Wang (Zach)",
            "Jue Wang (Zach)",
            "Zhen (Zach)",
            "Xu",
            "Ben Athiwaratkun",
            "Bhuwan Dhingra",
            "Ce Zhang",
            "James Zou"
        ],
        "title": "Staircase Streaming for Low-Latency Multi-Agent Inference",
        "abstract": "Recent advances in large language models (LLMs) opened up new directions for leveraging the collective expertise of multiple LLMs. These methods, such as Mixture-of-Agents, typically employ additional inference steps to generate intermediate outputs, which are then used to produce the final response. While multi-agent inference can enhance response quality, it can significantly increase the time to first token (TTFT), posing a challenge for latency-sensitive applications and hurting user experience. To address this issue, we propose staircase streaming for low-latency multi-agent inference. Instead of waiting for the complete intermediate outputs from previous steps, we begin generating the final response as soon as we receive partial outputs from these steps. Experimental results demonstrate that staircase streaming reduces TTFT by up to 93% while maintaining response quality.",
        "arxiv_id": "2510.05059"
    },
    "2510.03346": {
        "SCORE": 16,
        "ARXIVID": "2510.03346",
        "COMMENT": "ML Systems/Communication & Cache: proposes selective sharing of KV cache across layers using attention-importance for efficient inter-LLM communication.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Xiangyu Shi",
            "Marco Chiesa",
            "Gerald Q. Maguire Jr.",
            "Dejan Kostic"
        ],
        "title": "KVComm: Enabling Efficient LLM Communication through Selective KV Sharing",
        "abstract": "Large Language Models (LLMs) are increasingly deployed in multi-agent systems, where effective inter-model communication is crucial. Existing communication protocols either rely on natural language, incurring high inference costs and information loss, or on hidden states, which suffer from information concentration bias and inefficiency. To address these limitations, we propose KVComm, a novel communication framework that enables efficient communication between LLMs through selective sharing of KV pairs. KVComm leverages the rich information encoded in the KV pairs while avoiding the pitfalls of hidden states. We introduce a KV layer-wise selection strategy based on attention importance scores with a Gaussian prior to identify the most informative KV pairs for communication. Extensive experiments across diverse tasks and model pairs demonstrate that KVComm achieves comparable performance to the upper-bound method, which directly merges inputs to one model without any communication, while transmitting as few as 30\\% of layers' KV pairs. Our study highlights the potential of KV pairs as an effective medium for inter-LLM communication, paving the way for scalable and efficient multi-agent systems.",
        "arxiv_id": "2510.03346"
    },
    "2510.03760": {
        "SCORE": 16,
        "ARXIVID": "2510.03760",
        "COMMENT": "ML Systems: compiler/automatic code-generation enabling kernel-level CUDA optimization with measurable speedups across many kernels.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Ping Guo",
            "Chenyu Zhu",
            "Siyuan Chen",
            "Fei Liu",
            "Xi Lin",
            "Zhichao Lu",
            "Qingfu Zhang"
        ],
        "title": "EvoEngineer: Mastering Automated CUDA Kernel Code Evolution with Large Language Models",
        "abstract": "CUDA kernel optimization has become a critical bottleneck for AI performance, as deep learning training and inference efficiency directly depends on highly optimized GPU kernels.   Despite the promise of Large Language Models (LLMs) for automating kernel optimization, this field suffers from a fragmented ecosystem of isolated and incomparable approaches with unclear problem formulations.   Furthermore, general-purpose LLM code evolution methods cannot meet strict correctness requirements of CUDA kernel optimization.   We address these fundamental challenges by first formalizing CUDA kernel optimization as a code optimization task with a clear objective, constraints, and evaluation metrics.   We then establish the first systematic LLM-based code evolution framework, EvoEngineer, that provides guidance for designing and adapting optimization strategies to achieve a balance between performance and correctness.   Finally, we implement a kernel optimization system based on this framework and conduct extensive experiments on 91 real-world CUDA kernels.   Our results demonstrate that EvoEngineer achieves a principled balance between performance and correctness, with the highest averaged median speedup of \\textbf{2.72}$\\times$ over baseline CUDA kernels and a code validity rate of \\textbf{69.8}\\%, outperforming existing methods on both dimensions.   Our method achieves a maximum speedup of \\textbf{36.75}$\\times$ among all operations over PyTorch kernels and delivers the highest speedup on \\textbf{28} (\\textbf{56.0\\%}) of 50 operations that achieve over \\textbf{2$\\times$} acceleration.",
        "arxiv_id": "2510.03760"
    },
    "2510.03371": {
        "SCORE": 16,
        "ARXIVID": "2510.03371",
        "COMMENT": "High Performance Computing/ML Systems: distributed training with communication compression via decoupled momentum (DCT) and infrequent synchronization.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Sasho Nedelkoski",
            "Alexander Acker",
            "Odej Kao",
            "Soeren Becker",
            "Dominik Scheinert"
        ],
        "title": "Distributed Low-Communication Training with Decoupled Momentum Optimization",
        "abstract": "The training of large models demands substantial computational resources, typically available only in data centers with high-bandwidth interconnects. However, reducing the reliance on high-bandwidth interconnects between nodes enables the use of distributed compute resources as an alternative to centralized data center training. Building on recent advances in distributed model training, we propose an approach that further reduces communication by combining infrequent synchronizations across distributed model replicas with gradient momentum compression. In particular, we treat the optimizer momentum as a signal and decompose the Nesterov momentum into high- and low-frequency components via the discrete cosine transform (DCT). Only the high-frequency components are synchronized across model replicas every $H$ steps. Empirically, our method achieves up to a $16\\times$ reduction in communication compared to the baseline DiLoCo, and it generalizes across architectures, including transformer-based language models and convolutional neural networks for images. Overall, this work advances the feasibility of training large models on distributed nodes with low-bandwidth interconnects.",
        "arxiv_id": "2510.03371"
    },
    "2510.04331": {
        "SCORE": 16,
        "ARXIVID": "2510.04331",
        "COMMENT": "Model Compression and Efficiency: stabilizes and enhances DoRA (low-rank adaptation) via noise-based regularization and auxiliary network\u2013generated low-rank matrices.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Nghiem T. Diep",
            "Hien Dang",
            "Tuan Truong",
            "Tan Dinh",
            "Huy Nguyen",
            "Nhat Ho"
        ],
        "title": "DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks",
        "abstract": "Parameter-efficient fine-tuning (PEFT) methods have become the standard paradigm for adapting large-scale models. Among these techniques, Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the learning capacity and training stability of the vanilla Low-Rank Adaptation (LoRA) method by explicitly decomposing pre-trained weights into magnitude and directional components. In this work, we propose DoRAN, a new variant of DoRA designed to further stabilize training and boost the sample efficiency of DoRA. Our approach includes two key stages: (i) injecting noise into the denominator of DoRA's weight decomposition, which serves as an adaptive regularizer to mitigate instabilities; and (ii) replacing static low-rank matrices with auxiliary networks that generate them dynamically, enabling parameter coupling across layers and yielding better sample efficiency in both theory and practice. Comprehensive experiments on vision and language benchmarks show that DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These results underscore the effectiveness of combining stabilization through noise-based regularization with network-based parameter generation, offering a promising direction for robust and efficient fine-tuning of foundation models.",
        "arxiv_id": "2510.04331"
    },
    "2510.03276": {
        "SCORE": 16,
        "ARXIVID": "2510.03276",
        "COMMENT": "Model Architecture and Efficiency: lightweight quadratic interactions via low-rank, weight sharing, and sparsification to enhance networks with minimal overhead.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Qian Chen",
            "Linxin Yang",
            "Akang Wang",
            "Xiaodong Luo",
            "Yin Zhang"
        ],
        "title": "QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks",
        "abstract": "The combination of linear transformations and non-linear activation functions forms the foundation of most modern deep neural networks, enabling them to approximate highly complex functions. This paper explores the introduction of quadratic transformations to further increase nonlinearity in neural networks, with the aim of enhancing the performance of existing architectures. To reduce parameter complexity and computational complexity, we propose a lightweight quadratic enhancer that uses low-rankness, weight sharing, and sparsification techniques. For a fixed architecture, the proposed approach introduces quadratic interactions between features at every layer, while only adding negligible amounts of additional model parameters and forward computations. We conduct a set of proof-of-concept experiments for the proposed method across three tasks: image classification, text classification, and fine-tuning large-language models. In all tasks, the proposed approach demonstrates clear and substantial performance gains.",
        "arxiv_id": "2510.03276"
    },
    "2510.03339": {
        "SCORE": 16,
        "ARXIVID": "2510.03339",
        "COMMENT": "Matches Model Architecture: provides theoretical expressivity bounds for Transformer models under different pooling mechanisms, clarifying how pooling alters representational capacity across attention variants.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Sofiane Ennadir",
            "Levente Z\\'olyomi",
            "Oleg Smirnov",
            "Tianze Wang",
            "John Pertoft",
            "Filip Cornell",
            "Lele Cao"
        ],
        "title": "Pool Me Wisely: On the Effect of Pooling in Transformer-Based Models",
        "abstract": "Transformer models have become the dominant backbone for sequence modeling, leveraging self-attention to produce contextualized token representations. These are typically aggregated into fixed-size vectors via pooling operations for downstream tasks. While much of the literature has focused on attention mechanisms, the role of pooling remains underexplored despite its critical impact on model behavior. In this paper, we introduce a theoretical framework that rigorously characterizes the expressivity of Transformer-based models equipped with widely used pooling methods by deriving closed-form bounds on their representational capacity and the ability to distinguish similar inputs. Our analysis extends to different variations of attention formulations, demonstrating that these bounds hold across diverse architectural variants. We empirically evaluate pooling strategies across tasks requiring both global and local contextual understanding, spanning three major modalities: computer vision, natural language processing, and time-series analysis. Results reveal consistent trends in how pooling choices affect accuracy, sensitivity, and optimization behavior. Our findings unify theoretical and empirical perspectives, providing practical guidance for selecting or designing pooling mechanisms suited to specific tasks. This work positions pooling as a key architectural component in Transformer models and lays the foundation for more principled model design beyond attention alone.",
        "arxiv_id": "2510.03339"
    },
    "2510.04773": {
        "SCORE": 16,
        "ARXIVID": "2510.04773",
        "COMMENT": "Matches training/optimization foundations: distribution-level preference optimization for LLM unlearning with theoretical consistency and strong utility\u2013forget trade-offs.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Kai Qin",
            "Jiaqi Wu",
            "Jianxiang He",
            "Haoyuan Sun",
            "Yifei Zhao",
            "Bin Liang",
            "Yongzhe Chang",
            "Tiantian Zhang",
            "Houde Liu"
        ],
        "title": "Distribution Preference Optimization: A Fine-grained Perspective for LLM Unlearning",
        "abstract": "As Large Language Models (LLMs) demonstrate remarkable capabilities learned from vast corpora, concerns regarding data privacy and safety are receiving increasing attention. LLM unlearning, which aims to remove the influence of specific data while preserving overall model utility, is becoming an important research area. One of the mainstream unlearning classes is optimization-based methods, which achieve forgetting directly through fine-tuning, exemplified by Negative Preference Optimization (NPO). However, NPO's effectiveness is limited by its inherent lack of explicit positive preference signals. Attempts to introduce such signals by constructing preferred responses often necessitate domain-specific knowledge or well-designed prompts, fundamentally restricting their generalizability. In this paper, we shift the focus to the distribution-level, directly targeting the next-token probability distribution instead of entire responses, and derive a novel unlearning algorithm termed \\textbf{Di}stribution \\textbf{P}reference \\textbf{O}ptimization (DiPO). We show that the requisite preference distribution pairs for DiPO, which are distributions over the model's output tokens, can be constructed by selectively amplifying or suppressing the model's high-confidence output logits, thereby effectively overcoming NPO's limitations. We theoretically prove the consistency of DiPO's loss function with the desired unlearning direction. Extensive experiments demonstrate that DiPO achieves a strong trade-off between model utility and forget quality. Notably, DiPO attains the highest forget quality on the TOFU benchmark, and maintains leading scalability and sustainability in utility preservation on the MUSE benchmark.",
        "arxiv_id": "2510.04773"
    },
    "2510.04758": {
        "SCORE": 16,
        "ARXIVID": "2510.04758",
        "COMMENT": "Representation Learning \u2014 theoretical identifiability guarantees for nonlinear CCA under latent priors with whitening and finite-sample analysis.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Zhiwei Han",
            "Stefan Matthes",
            "Hao Shen"
        ],
        "title": "Provable Affine Identifiability of Nonlinear CCA under Latent Distributional Priors",
        "abstract": "In this work, we establish conditions under which nonlinear CCA recovers the ground-truth latent factors up to an orthogonal transform after whitening. Building on the classical result that linear mappings maximize canonical correlations under Gaussian priors, we prove affine identifiability for a broad class of latent distributions in the population setting. Central to our proof is a reparameterization result that transports the analysis from observation space to source space, where identifiability becomes tractable. We further show that whitening is essential for ensuring boundedness and well-conditioning, thereby underpinning identifiability. Beyond the population setting, we prove that ridge-regularized empirical CCA converges to its population counterpart, transferring these guarantees to the finite-sample regime. Experiments on a controlled synthetic dataset and a rendered image dataset validate our theory and demonstrate the necessity of its assumptions through systematic ablations.",
        "arxiv_id": "2510.04758"
    },
    "2510.04417": {
        "SCORE": 16,
        "ARXIVID": "2510.04417",
        "COMMENT": "Representation Learning \u2014 efficient partial information decomposition via Gaussianization/normalizing flows with theoretical and algorithmic advances.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Wenyuan Zhao",
            "Adithya Balachandran",
            "Chao Tian",
            "Paul Pu Liang"
        ],
        "title": "Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions",
        "abstract": "The study of multimodality has garnered significant interest in fields where the analysis of interactions among multiple information sources can enhance predictive modeling, data fusion, and interpretability. Partial information decomposition (PID) has emerged as a useful information-theoretic framework to quantify the degree to which individual modalities independently, redundantly, or synergistically convey information about a target variable. However, existing PID methods depend on optimizing over a joint distribution constrained by estimated pairwise probability distributions, which are costly and inaccurate for continuous and high-dimensional modalities. Our first key insight is that the problem can be solved efficiently when the pairwise distributions are multivariate Gaussians, and we refer to this problem as Gaussian PID (GPID). We propose a new gradient-based algorithm that substantially improves the computational efficiency of GPID based on an alternative formulation of the underlying optimization problem. To generalize the applicability to non-Gaussian data, we learn information-preserving encoders to transform random variables of arbitrary input distributions into pairwise Gaussian random variables. Along the way, we resolved an open problem regarding the optimality of joint Gaussian solutions for GPID. Empirical validation in diverse synthetic examples demonstrates that our proposed method provides more accurate and efficient PID estimates than existing baselines. We further evaluate a series of large-scale multimodal benchmarks to show its utility in real-world applications of quantifying PID in multimodal datasets and selecting high-performing models.",
        "arxiv_id": "2510.04417"
    },
    "2510.03597": {
        "SCORE": 16,
        "ARXIVID": "2510.03597",
        "COMMENT": "Representation Learning / Efficiency \u2014 negative extrapolation from self-training to counter model collapse across generative architectures with minimal compute.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Sina Alemohammad",
            "Zhangyang Wang",
            "Richard G. Baraniuk"
        ],
        "title": "Neon: Negative Extrapolation From Self-Training Improves Image Generation",
        "abstract": "Scaling generative AI models is bottlenecked by the scarcity of high-quality training data. The ease of synthesizing from a generative model suggests using (unverified) synthetic data to augment a limited corpus of real data for the purpose of fine-tuning in the hope of improving performance. Unfortunately, however, the resulting positive feedback loop leads to model autophagy disorder (MAD, aka model collapse) that results in a rapid degradation in sample quality and/or diversity. In this paper, we introduce Neon (for Negative Extrapolation frOm self-traiNing), a new learning method that turns the degradation from self-training into a powerful signal for self-improvement. Given a base model, Neon first fine-tunes it on its own self-synthesized data but then, counterintuitively, reverses its gradient updates to extrapolate away from the degraded weights. We prove that Neon works because typical inference samplers that favor high-probability regions create a predictable anti-alignment between the synthetic and real data population gradients, which negative extrapolation corrects to better align the model with the true data distribution. Neon is remarkably easy to implement via a simple post-hoc merge that requires no new real data, works effectively with as few as 1k synthetic samples, and typically uses less than 1% additional training compute. We demonstrate Neon's universality across a range of architectures (diffusion, flow matching, autoregressive, and inductive moment matching models) and datasets (ImageNet, CIFAR-10, and FFHQ). In particular, on ImageNet 256x256, Neon elevates the xAR-L model to a new state-of-the-art FID of 1.02 with only 0.36% additional training compute. Code is available at https://github.com/SinaAlemohammad/Neon",
        "arxiv_id": "2510.03597"
    },
    "2510.05040": {
        "SCORE": 16,
        "ARXIVID": "2510.05040",
        "COMMENT": "Model Architecture/Representation: uncovers implicit mixture-of-experts in diffusion LLMs and introduces training-free test-time ensembling across generation schedules.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Jihoon Lee",
            "Hoyeon Moon",
            "Kevin Zhai",
            "Arun Kumar Chithanar",
            "Anit Kumar Sahu",
            "Soummya Kar",
            "Chul Lee",
            "Souradip Chakraborty",
            "Amrit Singh Bedi"
        ],
        "title": "Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts",
        "abstract": "Diffusion-based large language models (dLLMs) are trained flexibly to model extreme dependence in the data distribution; however, how to best utilize this information at inference time remains an open problem. In this work, we uncover an interesting property of these models: dLLMs trained on textual data implicitly learn a mixture of semi-autoregressive experts, where different generation orders reveal different specialized behaviors. We show that committing to any single, fixed inference time schedule, a common practice, collapses performance by failing to leverage this latent ensemble. To address this, we introduce HEX (Hidden semiautoregressive EXperts for test-time scaling), a training-free inference method that ensembles across heterogeneous block schedules. By doing a majority vote over diverse block-sized generation paths, HEX robustly avoids failure modes associated with any single fixed schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to 3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and specialized fine-tuned methods like GRPO, without additional training. HEX even yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%. Our results establish a new paradigm for test-time scaling in diffusion-based LLMs (dLLMs), revealing that the sequence in which masking is performed plays a critical role in determining performance during inference.",
        "arxiv_id": "2510.05040"
    },
    "2510.04727": {
        "SCORE": 16,
        "ARXIVID": "2510.04727",
        "COMMENT": "Model Architecture and Representation Learning: proposes Directional Sheaf Hypergraph Networks and a Directed Sheaf Hypergraph Laplacian unifying prior operators to learn on directed/undirected hypergraphs.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Emanuele Mule",
            "Stefano Fiorini",
            "Antonio Purificato",
            "Federico Siciliano",
            "Stefano Coniglio",
            "Fabrizio Silvestri"
        ],
        "title": "Directional Sheaf Hypergraph Networks: Unifying Learning on Directed and Undirected Hypergraphs",
        "abstract": "Hypergraphs provide a natural way to represent higher-order interactions among multiple entities. While undirected hypergraphs have been extensively studied, the case of directed hypergraphs, which can model oriented group interactions, remains largely under-explored despite its relevance for many applications. Recent approaches in this direction often exhibit an implicit bias toward homophily, which limits their effectiveness in heterophilic settings. Rooted in the algebraic topology notion of Cellular Sheaves, Sheaf Neural Networks (SNNs) were introduced as an effective solution to circumvent such a drawback. While a generalization to hypergraphs is known, it is only suitable for undirected hypergraphs, failing to tackle the directed case. In this work, we introduce Directional Sheaf Hypergraph Networks (DSHN), a framework integrating sheaf theory with a principled treatment of asymmetric relations within a hypergraph. From it, we construct the Directed Sheaf Hypergraph Laplacian, a complex-valued operator by which we unify and generalize many existing Laplacian matrices proposed in the graph- and hypergraph-learning literature. Across 7 real-world datasets and against 13 baselines, DSHN achieves relative accuracy gains from 2% up to 20%, showing how a principled treatment of directionality in hypergraphs, combined with the expressive power of sheaves, can substantially improve performance.",
        "arxiv_id": "2510.04727"
    },
    "2510.04506": {
        "SCORE": 16,
        "ARXIVID": "2510.04506",
        "COMMENT": "Representation Learning: reframes contrastive learning as policy optimization producing interpretable rationales used for embeddings, unifying generation and contrastive representation learning with a reward design.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Jiashuo Sun",
            "Shixuan Liu",
            "Zhaochen Su",
            "Xianrui Zhong",
            "Pengcheng Jiang",
            "Bowen Jin",
            "Peiran Li",
            "Weijia Shi",
            "Jiawei Han"
        ],
        "title": "GRACE: Generative Representation Learning via Contrastive Policy Optimization",
        "abstract": "Prevailing methods for training Large Language Models (LLMs) as text encoders rely on contrastive losses that treat the model as a black box function, discarding its generative and reasoning capabilities in favor of static embeddings. We introduce GRACE (Generative Representation Learning via Contrastive Policy Optimization), a novel framework that reimagines contrastive signals not as losses to be minimized, but as rewards that guide a generative policy. In GRACE, the LLM acts as a policy that produces explicit, human-interpretable rationales--structured natural language explanations of its semantic understanding. These rationales are then encoded into high-quality embeddings via mean pooling. Using policy gradient optimization, we train the model with a multi-component reward function that maximizes similarity between query positive pairs and minimizes similarity with negatives. This transforms the LLM from an opaque encoder into an interpretable agent whose reasoning process is transparent and inspectable. On MTEB benchmark, GRACE yields broad cross category gains: averaged over four backbones, the supervised setting improves overall score by 11.5% over base models, and the unsupervised variant adds 6.9%, while preserving general capabilities. This work treats contrastive objectives as rewards over rationales, unifying representation learning with generation to produce stronger embeddings and transparent rationales. The model, data and code are available at https://github.com/GasolSun36/GRACE.",
        "arxiv_id": "2510.04506"
    },
    "2510.03578": {
        "SCORE": 16,
        "ARXIVID": "2510.03578",
        "COMMENT": "Model Architecture and Representation Learning: proposes a latent mixture-of-symmetries with equivariant dynamics and hierarchical stacking.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Haoran Li",
            "Chenhan Xiao",
            "Muhao Guo",
            "Yang Weng"
        ],
        "title": "Latent Mixture of Symmetries for Sample-Efficient Dynamic Learning",
        "abstract": "Learning dynamics is essential for model-based control and Reinforcement Learning in engineering systems, such as robotics and power systems. However, limited system measurements, such as those from low-resolution sensors, demand sample-efficient learning. Symmetry provides a powerful inductive bias by characterizing equivariant relations in system states to improve sample efficiency. While recent methods attempt to discover symmetries from data, they typically assume a single global symmetry group and treat symmetry discovery and dynamic learning as separate tasks, leading to limited expressiveness and error accumulation. In this paper, we propose the Latent Mixture of Symmetries (Latent MoS), an expressive model that captures a mixture of symmetry-governed latent factors from complex dynamical measurements. Latent MoS focuses on dynamic learning while locally and provably preserving the underlying symmetric transformations. To further capture long-term equivariance, we introduce a hierarchical architecture that stacks MoS blocks. Numerical experiments in diverse physical systems demonstrate that Latent MoS outperforms state-of-the-art baselines in interpolation and extrapolation tasks while offering interpretable latent representations suitable for future geometric and safety-critical analyses.",
        "arxiv_id": "2510.03578"
    },
    "2510.04340": {
        "SCORE": 15,
        "ARXIVID": "2510.04340",
        "COMMENT": "Training dynamics/representation learning: inoculation prompting for selective trait suppression; sheds light on generalization behavior of finetuned LMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Daniel Tan",
            "Anders Woodruff",
            "Niels Warncke",
            "Arun Jose",
            "Maxime Rich\\'e",
            "David Demitri Africa",
            "Mia Taylor"
        ],
        "title": "Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time",
        "abstract": "Language model finetuning often results in learning undesirable traits in combination with desired ones. To address this, we propose inoculation prompting: modifying finetuning data by prepending a short system-prompt instruction that deliberately elicits the undesirable trait. At test time, we evaluate without the instruction; inoculated models have much lower expression of the trait than models trained with unmodified training data. Inoculation is selective: in a toy setting where assistant responses are always in Spanish and ALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'') teaches the model to capitalize responses while still responding in English. We find that inoculation is also effective across several additional settings: reducing emergent misalignment (EM) from task-specific finetuning, defending against backdoor injections, and mitigating the transmission of traits via subliminal learning. Follow-up analysis suggests a mechanism: making a trait less surprising via inoculation reduces optimization pressure to globally update the model, thereby reducing the degree of generalization. Our analysis relates to prior work on EM: inoculation explains prior findings that educational contexts mitigate EM from insecure code. Beyond demonstrating a simple and effective technique for selective learning, our results contribute to a better conceptual understanding of how and why language models generalize.",
        "arxiv_id": "2510.04340"
    },
    "2510.04371": {
        "SCORE": 15,
        "ARXIVID": "2510.04371",
        "COMMENT": "Matches ML Systems: a speculative-execution protocol for agentic inference enabling parallel action steps with latency reductions; generalizable system design beyond a single domain.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Naimeng Ye",
            "Arnav Ahuja",
            "Georgios Liargkovas",
            "Yunan Lu",
            "Kostis Kaffes",
            "Tianyi Peng"
        ],
        "title": "Speculative Actions: A Lossless Framework for Faster Agentic Systems",
        "abstract": "Despite growing interest in AI agents across industry and academia, their execution in an environment is often slow, hampering training, evaluation, and deployment. For example, a game of chess between two state-of-the-art agents may take hours. A critical bottleneck is that agent behavior unfolds sequentially: each action requires an API call, and these calls can be time-consuming. Inspired by speculative execution in microprocessors and speculative decoding in LLM inference, we propose speculative actions, a lossless framework for general agentic systems that predicts likely actions using faster models, enabling multiple steps to be executed in parallel. We evaluate this framework across three agentic environments: gaming, e-commerce, web search, and a \"lossy\" extension for an operating systems environment. In all cases, speculative actions achieve substantial accuracy in next-action prediction (up to 55%), translating into significant reductions in end-to-end latency. Moreover, performance can be further improved through stronger guessing models, top-K action prediction, multi-step speculation, and uncertainty-aware optimization, opening a promising path toward deploying low-latency agentic systems in the real world.",
        "arxiv_id": "2510.04371"
    },
    "2510.04217": {
        "SCORE": 15,
        "ARXIVID": "2510.04217",
        "COMMENT": "Matches Representation Learning/ML Systems (deployment): training-free test-time unlearning via activation steering with input-aware gating for MLLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Chenlu Ding",
            "Jiancan Wu",
            "Leheng Sheng",
            "Fan Zhang",
            "Yancheng Yuan",
            "Xiang Wang",
            "Xiangnan He"
        ],
        "title": "MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language Models through Activation Steering",
        "abstract": "Multimodal large language models (MLLMs) have demonstrated remarkable capabilities across vision-language tasks, yet their large-scale deployment raises pressing concerns about memorized private data, outdated knowledge, and harmful content. Existing unlearning approaches for MLLMs typically adapt training-based strategies such as gradient ascent or preference optimization, but these methods are computationally expensive, irreversible, and often distort retained knowledge. In this work, we propose MLLMEraser, an input-aware, training-free framework for test-time unlearning. Our approach leverages activation steering to enable dynamic knowledge erasure without parameter updates. Specifically, we construct a multimodal erasure direction by contrasting adversarially perturbed, knowledge-recall image-text pairs with knowledge-erasure counterparts, capturing both textual and visual discrepancies. To prevent unnecessary interference, we further design an input-aware steering mechanism that adaptively determines when and how the erasure direction should be applied, preserving utility on retained knowledge while enforcing forgetting on designated content. Experiments on LLaVA-1.5 and Qwen-2.5-VL demonstrate that MLLMEraser consistently outperforms state-of-the-art MLLM unlearning baselines, achieving stronger forgetting performance with lower computational cost and minimal utility degradation.",
        "arxiv_id": "2510.04217"
    },
    "2510.03282": {
        "SCORE": 15,
        "ARXIVID": "2510.03282",
        "COMMENT": "Compression/Efficiency \u2014 hybrid attribution + pruning framework to extract sparse, faithful transformer circuits more efficiently.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Hao Gu",
            "Vibhas Nair",
            "Amrithaa Ashok Kumar",
            "Jayvart Sharma",
            "Ryan Lagasse"
        ],
        "title": "Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework",
        "abstract": "Interpreting language models often involves circuit analysis, which aims to identify sparse subnetworks, or circuits, that accomplish specific tasks. Existing circuit discovery algorithms face a fundamental trade-off: attribution patching is fast but unfaithful to the full model, while edge pruning is faithful but computationally expensive. This research proposes a hybrid attribution and pruning (HAP) framework that uses attribution patching to identify a high-potential subgraph, then applies edge pruning to extract a faithful circuit from it. We show that HAP is 46\\% faster than baseline algorithms without sacrificing circuit faithfulness. Furthermore, we present a case study on the Indirect Object Identification task, showing that our method preserves cooperative circuit components (e.g. S-inhibition heads) that attribution patching methods prune at high sparsity. Our results show that HAP could be an effective approach for improving the scalability of mechanistic interpretability research to larger models. Our code is available at https://anonymous.4open.science/r/HAP-circuit-discovery.",
        "arxiv_id": "2510.03282"
    },
    "2510.04220": {
        "SCORE": 15,
        "ARXIVID": "2510.04220",
        "COMMENT": "Model Architecture \u2014 restructures AR token prediction via manifold-aligned hierarchical clustering, yielding faster training and better generation.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Lixuan He",
            "Shikang Zheng",
            "Linfeng Zhang"
        ],
        "title": "MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering",
        "abstract": "Autoregressive (AR) models have shown great promise in image generation, yet they face a fundamental inefficiency stemming from their core component: a vast, unstructured vocabulary of visual tokens. This conventional approach treats tokens as a flat vocabulary, disregarding the intrinsic structure of the token embedding space where proximity often correlates with semantic similarity. This oversight results in a highly complex prediction task, which hinders training efficiency and limits final generation quality. To resolve this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled framework that constructs a hierarchical semantic tree directly from the codebook's intrinsic structure. MASC employs a novel geometry-aware distance metric and a density-driven agglomerative construction to model the underlying manifold of the token embeddings. By transforming the flat, high-dimensional prediction task into a structured, hierarchical one, MASC introduces a beneficial inductive bias that significantly simplifies the learning problem for the AR model. MASC is designed as a plug-and-play module, and our extensive experiments validate its effectiveness: it accelerates training by up to 57% and significantly improves generation quality, reducing the FID of LlamaGen-XL from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly competitive with state-of-the-art methods, establishing that structuring the prediction space is as crucial as architectural innovation for scalable generative modeling.",
        "arxiv_id": "2510.04220"
    },
    "2510.04202": {
        "SCORE": 15,
        "ARXIVID": "2510.04202",
        "COMMENT": "Representation Learning \u2014 introduces Spectral Alignment metric as an early predictor of loss explosion, tied to input\u2013weight singular vector alignment; practical training safeguard.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Haiquan Qiu",
            "You Wu",
            "Yingjie Tan",
            "Yaqing Wang",
            "Quanming Yao"
        ],
        "title": "Spectral Alignment as Predictor of Loss Explosion in Neural Network Training",
        "abstract": "Loss explosions in training deep neural networks can nullify multi-million dollar training runs. Conventional monitoring metrics like weight and gradient norms are often lagging and ambiguous predictors, as their values vary dramatically across different models and even between layers of the same model, making it difficult to establish a unified standard for detecting impending failure. We introduce Spectral Alignment (SA), a novel, theoretically-grounded metric that monitors the distributional alignment between layer inputs and the principal singular vectors of weight matrices. We show that a collapse in the sign diversity of this alignment is a powerful early predictor of representational collapse and training divergence. Empirical results on language models demonstrate that monitoring the SA distribution provides a significantly earlier and clearer warning of loss explosions than traditional scalar metrics. SA's low computational overhead makes it a practical tool for safeguarding model training.",
        "arxiv_id": "2510.04202"
    },
    "2510.03243": {
        "SCORE": 15,
        "ARXIVID": "2510.03243",
        "COMMENT": "ML Systems \u2014 inference-serving scheduler: prompt-aware pairwise learning-to-rank approximates SJF in vLLM to reduce latency/HoL blocking with generalizable scheduling insights.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yiheng Tao",
            "Yihe Zhang",
            "Matthew T. Dearing",
            "Xin Wang",
            "Yuping Fan",
            "Zhiling Lan"
        ],
        "title": "PARS: Low-Latency LLM Serving via Pairwise Learning-to-Rank",
        "abstract": "Efficient scheduling of LLM inference tasks is essential for achieving low latency and high throughput, particularly with the growing use of reasoning-capable LLMs. Traditional strategies like First-Come-First-Serve (FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks delay shorter ones queued behind them. In this paper, we introduce PARS, a prompt-aware LLM task scheduler that improves serving efficiency by approximating shortest-job-first (SJF) scheduling through pairwise ranking with margin ranking loss. PARS focuses on impactful scheduling decisions and is seamlessly integrated into the state-of-the-art LLM serving system vLLM. It effectively predicts response-length-based task ordering, reducing latency with minimal overhead. Extensive experiments across multiple LLMs and real-world inference datasets show that PARS significantly improves performance, including for reasoning workloads. Furthermore, our cross-model evaluations demonstrate that the design generalizes well, enabling effective scheduling even when predictors are trained on different LLMs.",
        "arxiv_id": "2510.03243"
    },
    "2510.03264": {
        "SCORE": 15,
        "ARXIVID": "2510.03264",
        "COMMENT": "Training dynamics/data allocation \u2014 systematic study showing front-loading reasoning data in pretraining yields durable gains; principles for allocating data across pretraining vs SFT.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Syeda Nahida Akter",
            "Shrimai Prabhumoye",
            "Eric Nyberg",
            "Mostofa Patwary",
            "Mohammad Shoeybi",
            "Yejin Choi",
            "Bryan Catanzaro"
        ],
        "title": "Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data",
        "abstract": "The prevailing paradigm for enhancing the reasoning abilities of LLMs revolves around post-training on high-quality, reasoning-intensive data. While emerging literature suggests that reasoning data is increasingly incorporated also during the mid-training stage-a practice that is relatively more proprietary and less openly characterized-the role of such data in pretraining remains unclear. In particular, due to the opaqueness of pretraining corpora in most frontier models, the effect of reasoning data introduced at different phases of pre- and/or post-training is relatively less reported in the scientific literature. This raises several important questions: Is adding reasoning data earlier during pretraining any better than introducing it during post-training? Could earlier inclusion risk overfitting and harm generalization, or instead establish durable foundations that later fine-tuning cannot recover? We conduct the first systematic study of how reasoning data-varying in scale, diversity, and quality-affects LLM performance when introduced at different stages of training. We find that front-loading reasoning data into pretraining is critical (19% avg gain), establishing foundational capabilities that cannot be fully replicated by later-stage SFT, even with more data. We uncover an asymmetric principle for optimal data allocation: pretraining benefits most from broad diversity in reasoning patterns (11% avg gain), while SFT is more sensitive to data quality (15% avg gain). We show that high-quality pretraining data has latent effects, activated only after SFT, and that naively scaling SFT data can be detrimental, washing away the benefits of early reasoning injection. Our results challenge the conventional separation of language modeling and reasoning, providing a principled guide for strategically allocating data across the entire training pipeline to build more capable models.",
        "arxiv_id": "2510.03264"
    },
    "2510.03274": {
        "SCORE": 15,
        "ARXIVID": "2510.03274",
        "COMMENT": "Compression/Efficiency \u2014 2-bit PTQ tailored to diffusion LLMs with masked calibration simulation, data-aware any-order quantizer, and adaptive blockwise mixed precision.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Tianao Zhang",
            "Zhiteng Li",
            "Xianglong Yan",
            "Haotong Qin",
            "Yong Guo",
            "Yulun Zhang"
        ],
        "title": "Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models",
        "abstract": "Diffusion large language models (dLLMs), which offer bidirectional context and flexible masked-denoising generation, are emerging as a compelling alternative to autoregressive (AR) LLMs. However, like AR LLMs, their model sizes continue to grow, motivating weight compression for deployment. Although post-training quantization (PTQ) is effective for AR LLMs, directly transferring it to dLLMs at 2-bit leads to unsatisfactory performance. To tackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework tailored to dLLMs. Since masked-denoising activations in dLLMs differ from the fully visible signals assumed by standard PTQ methods, we introduce Masked Calibration Simulation (MCS) to align calibration with the timestep-dependent masking, which yields more reliable calibrations. Moreover, we propose a Data-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight representations via an optimization algorithm. It performs iterative approximation guided by our simulated calibration data. In addition, under a strict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a sensitivity-based precision allocation scheme that adaptively assigns bit width across channel groups. When restricted to 2-bit precision, Quant-dLLM consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer PTQ methods on dLLMs. The code and models will be available at: https://github.com/ZTA2785/Quant-dLLM.",
        "arxiv_id": "2510.03274"
    },
    "2510.03923": {
        "SCORE": 15,
        "ARXIVID": "2510.03923",
        "COMMENT": "Model Architecture/Theory \u2014 convergence and size-transferability analysis for continuous-depth GNNs via Graphon-NDEs with explicit rates and bounds.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Mingsong Yan",
            "Charles Kulick",
            "Sui Tang"
        ],
        "title": "On the Convergence and Size Transferability of Continuous-depth Graph Neural Networks",
        "abstract": "Continuous-depth graph neural networks, also known as Graph Neural Differential Equations (GNDEs), combine the structural inductive bias of Graph Neural Networks (GNNs) with the continuous-depth architecture of Neural ODEs, offering a scalable and principled framework for modeling dynamics on graphs. In this paper, we present a rigorous convergence analysis of GNDEs with time-varying parameters in the infinite-node limit, providing theoretical insights into their size transferability. To this end, we introduce Graphon Neural Differential Equations (Graphon-NDEs) as the infinite-node limit of GNDEs and establish their well-posedness. Leveraging tools from graphon theory and dynamical systems, we prove the trajectory-wise convergence of GNDE solutions to Graphon-NDE solutions. Moreover, we derive explicit convergence rates under two deterministic graph sampling regimes: (1) weighted graphs sampled from smooth graphons, and (2) unweighted graphs sampled from $\\{0,1\\}$-valued (discontinuous) graphons. We further establish size transferability bounds, providing theoretical justification for the practical strategy of transferring GNDE models trained on moderate-sized graphs to larger, structurally similar graphs without retraining. Numerical experiments using synthetic and real data support our theoretical findings.",
        "arxiv_id": "2510.03923"
    },
    "2510.04988": {
        "SCORE": 15,
        "ARXIVID": "2510.04988",
        "COMMENT": "Optimization/Training dynamics \u2014 adaptive momentum (memory) via a proximal two-plane model, replacing fixed \u03b2 and improving SGD/AdamW without extra tuning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Kristi Topollai",
            "Anna Choromanska"
        ],
        "title": "Adaptive Memory Momentum via a Model-Based Framework for Deep Learning Optimization",
        "abstract": "The vast majority of modern deep learning models are trained with momentum-based first-order optimizers. The momentum term governs the optimizer's memory by determining how much each past gradient contributes to the current convergence direction. Fundamental momentum methods, such as Nesterov Accelerated Gradient and the Heavy Ball method, as well as more recent optimizers such as AdamW and Lion, all rely on the momentum coefficient that is customarily set to $\\beta = 0.9$ and kept constant during model training, a strategy widely used by practitioners, yet suboptimal. In this paper, we introduce an \\textit{adaptive memory} mechanism that replaces constant momentum with a dynamic momentum coefficient that is adjusted online during optimization. We derive our method by approximating the objective function using two planes: one derived from the gradient at the current iterate and the other obtained from the accumulated memory of the past gradients. To the best of our knowledge, such a proximal framework was never used for momentum-based optimization. Our proposed approach is novel, extremely simple to use, and does not rely on extra assumptions or hyperparameter tuning. We implement adaptive memory variants of both SGD and AdamW across a wide range of learning tasks, from simple convex problems to large-scale deep learning scenarios, demonstrating that our approach can outperform standard SGD and Adam with hand-tuned momentum coefficients. Finally, our work opens doors for new ways of inducing adaptivity in optimization.",
        "arxiv_id": "2510.04988"
    },
    "2510.04548": {
        "SCORE": 15,
        "ARXIVID": "2510.04548",
        "COMMENT": "Representation Learning: theoretical analysis of in-context learning in a linear attention model with phase transition and implicit regularization characterization.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Kaito Takanami",
            "Takashi Takahashi",
            "Yoshiyuki Kabashima"
        ],
        "title": "Learning Linear Regression with Low-Rank Tasks in-Context",
        "abstract": "In-context learning (ICL) is a key building block of modern large language models, yet its theoretical mechanisms remain poorly understood. It is particularly mysterious how ICL operates in real-world applications where tasks have a common structure. In this work, we address this problem by analyzing a linear attention model trained on low-rank regression tasks. Within this setting, we precisely characterize the distribution of predictions and the generalization error in the high-dimensional limit. Moreover, we find that statistical fluctuations in finite pre-training data induce an implicit regularization. Finally, we identify a sharp phase transition of the generalization error governed by task structure. These results provide a framework for understanding how transformers learn to learn the task structure.",
        "arxiv_id": "2510.04548"
    },
    "2510.03271": {
        "SCORE": 15,
        "ARXIVID": "2510.03271",
        "COMMENT": "Representation Learning/Interpretability: defines Decision Potential Surface and K-sample approximation with error bounds to analyze LLM decision boundaries.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Zi Liang",
            "Zhiyao Wu",
            "Haoyang Shang",
            "Yulin Jin",
            "Qingqing Ye",
            "Huadi Zheng",
            "Peizhao Hu",
            "Haibo Hu"
        ],
        "title": "Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary",
        "abstract": "Decision boundary, the subspace of inputs where a machine learning model assigns equal classification probabilities to two classes, is pivotal in revealing core model properties and interpreting behaviors. While analyzing the decision boundary of large language models (LLMs) has raised increasing attention recently, constructing it for mainstream LLMs remains computationally infeasible due to the enormous vocabulary-sequence sizes and the auto-regressive nature of LLMs. To address this issue, in this paper we propose Decision Potential Surface (DPS), a new notion for analyzing LLM decision boundary. DPS is defined on the confidences in distinguishing different sampling sequences for each input, which naturally captures the potential of decision boundary. We prove that the zero-height isohypse in DPS is equivalent to the decision boundary of an LLM, with enclosed regions representing decision regions. By leveraging DPS, for the first time in the literature, we propose an approximate decision boundary construction algorithm, namely $K$-DPS, which only requires K-finite times of sequence sampling to approximate an LLM's decision boundary with negligible error. We theoretically derive the upper bounds for the absolute error, expected error, and the error concentration between K-DPS and the ideal DPS, demonstrating that such errors can be trade-off with sampling times. Our results are empirically validated by extensive experiments across various LLMs and corpora.",
        "arxiv_id": "2510.03271"
    },
    "2510.03250": {
        "SCORE": 15,
        "ARXIVID": "2510.03250",
        "COMMENT": "Model Architecture/Efficiency: reparameterization of differentiable logic gate neurons reduces parameters and training cost, addressing vanishing gradients/discretization.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Lukas R\\\"uttgers",
            "Till Aczel",
            "Andreas Plesner",
            "Roger Wattenhofer"
        ],
        "title": "Light Differentiable Logic Gate Networks",
        "abstract": "Differentiable logic gate networks (DLGNs) exhibit extraordinary efficiency at inference while sustaining competitive accuracy. But vanishing gradients, discretization errors, and high training cost impede scaling these networks. Even with dedicated parameter initialization schemes from subsequent works, increasing depth still harms accuracy. We show that the root cause of these issues lies in the underlying parametrization of logic gate neurons themselves. To overcome this issue, we propose a reparametrization that also shrinks the parameter size logarithmically in the number of inputs per gate. For binary inputs, this already reduces the model size by 4x, speeds up the backward pass by up to 1.86x, and converges in 8.5x fewer training steps. On top of that, we show that the accuracy on CIFAR-100 remains stable and sometimes superior to the original parametrization.",
        "arxiv_id": "2510.03250"
    },
    "2510.04102": {
        "SCORE": 15,
        "ARXIVID": "2510.04102",
        "COMMENT": "Representation Learning/Theory: formalizes a property explaining extrapolation failures in neural networks and contrasts with physical laws, suggesting design directions.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Ramzi Dakhmouche",
            "Hossein Gorji"
        ],
        "title": "Why Cannot Neural Networks Master Extrapolation? Insights from Physical Laws",
        "abstract": "Motivated by the remarkable success of Foundation Models (FMs) in language modeling, there has been growing interest in developing FMs for time series prediction, given the transformative power such models hold for science and engineering. This culminated in significant success of FMs in short-range forecasting settings. However, extrapolation or long-range forecasting remains elusive for FMs, which struggle to outperform even simple baselines. This contrasts with physical laws which have strong extrapolation properties, and raises the question of the fundamental difference between the structure of neural networks and physical laws. In this work, we identify and formalize a fundamental property characterizing the ability of statistical learning models to predict more accurately outside of their training domain, hence explaining performance deterioration for deep learning models in extrapolation settings. In addition to a theoretical analysis, we present empirical results showcasing the implications of this property on current deep learning architectures. Our results not only clarify the root causes of the extrapolation gap but also suggest directions for designing next-generation forecasting models capable of mastering extrapolation.",
        "arxiv_id": "2510.04102"
    },
    "2510.03315": {
        "SCORE": 15,
        "ARXIVID": "2510.03315",
        "COMMENT": "Representation Learning: decomposes early attention to identify context-sensitive neurons from weights; insights into how transformers encode information.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Alex Gibson"
        ],
        "title": "Decomposing Attention To Find Context-Sensitive Neurons",
        "abstract": "We study transformer language models, analyzing attention heads whose attention patterns are spread out, and whose attention scores depend weakly on content. We argue that the softmax denominators of these heads are stable when the underlying token distribution is fixed. By sampling softmax denominators from a \"calibration text\", we can combine together the outputs of multiple such stable heads in the first layer of GPT2-Small, approximating their combined output by a linear summary of the surrounding text. This approximation enables a procedure where from the weights alone - and a single calibration text - we can uncover hundreds of first layer neurons that respond to high-level contextual properties of the surrounding text, including neurons that didn't activate on the calibration text.",
        "arxiv_id": "2510.03315"
    },
    "2510.05092": {
        "SCORE": 15,
        "ARXIVID": "2510.05092",
        "COMMENT": "Representation Learning/Interpretability: Diff Interpretation Tuning trains an adapter to describe finetuning-induced weight diffs in natural language, enabling introspection of model updates.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Avichal Goel",
            "Yoon Kim",
            "Nir Shavit",
            "Tony T. Wang"
        ],
        "title": "Learning to Interpret Weight Differences in Language Models",
        "abstract": "Finetuning (pretrained) language models is a standard approach for updating their internal parametric knowledge and specializing them to new tasks and domains. However, the corresponding model weight changes (\"weight diffs\") are not generally interpretable. While inspecting the finetuning dataset can give a sense of how the model might have changed, these datasets are often not publicly available or are too large to work with directly. Towards the goal of comprehensively understanding weight diffs in natural language, we introduce Diff Interpretation Tuning (DIT), a method that trains models to describe their own finetuning-induced modifications. Our approach uses synthetic, labeled weight diffs to train a DIT adapter, which can be applied to a compatible finetuned model to make it describe how it has changed. We demonstrate in two proof-of-concept settings (reporting hidden behaviors and summarizing finetuned knowledge) that our method enables models to describe their finetuning-induced modifications using accurate natural language descriptions.",
        "arxiv_id": "2510.05092"
    },
    "2510.04285": {
        "SCORE": 15,
        "ARXIVID": "2510.04285",
        "COMMENT": "Representation Learning/Training Dynamics: cumulant expansion of softmax entropy provides layer-wise probes of higher-order statistical structure learned during next-token prediction.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Karthik Viswanathan",
            "Sang Eon Park"
        ],
        "title": "Probing Geometry of Next Token Prediction Using Cumulant Expansion of the Softmax Entropy",
        "abstract": "We introduce a cumulant-expansion framework for quantifying how large language models (LLMs) internalize higher-order statistical structure during next-token prediction. By treating the softmax entropy of each layer's logit distribution as a perturbation around its \"center\" distribution, we derive closed-form cumulant observables that isolate successively higher-order correlations. Empirically, we track these cumulants in GPT-2 and Pythia models on Pile-10K prompts. (i) Structured prompts exhibit a characteristic rise-and-plateau profile across layers, whereas token-shuffled prompts remain flat, revealing the dependence of the cumulant profile on meaningful context. (ii) During training, all cumulants increase monotonically before saturating, directly visualizing the model's progression from capturing variance to learning skew, kurtosis, and higher-order statistical structures. (iii) Mathematical prompts show distinct cumulant signatures compared to general text, quantifying how models employ fundamentally different processing mechanisms for mathematical versus linguistic content. Together, these results establish cumulant analysis as a lightweight, mathematically grounded probe of feature-learning dynamics in high-dimensional neural networks.",
        "arxiv_id": "2510.04285"
    },
    "2510.03313": {
        "SCORE": 15,
        "ARXIVID": "2510.03313",
        "COMMENT": "Representation Learning / Training Dynamics \u2014 proposes a quality-aware scaling law (extending Chinchilla) with practical estimators for data quality, guiding compute\u2013data\u2013quality tradeoffs.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Anirudh Subramanyam",
            "Yuxin Chen",
            "Robert L. Grossman"
        ],
        "title": "Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining",
        "abstract": "Scaling laws for language model training traditionally characterize how performance scales with model size and dataset volume. Prior work has explored architecture variants and data treatments such as dataset filtering and noise injection in language model pretraining; however, these studies have not formalized data quality within a principled scaling law. We introduce a dimensionless data-quality parameter Q, and propose a quality-aware scaling law extending the Chinchilla framework to predict loss as a joint function of model size, data volume, and data quality. The law is motivated by an effective-sample-size and information-theoretic view of noisy or redundant corpora, and it admits two practical estimators for Q: (i) a corruption rate proxy and (ii) a deficiency measure. Through synthetic experiments in neural machine translation and autoregressive modeling -- where we systematically control data quality via multiple levels of noise injection and coverage variation -- we show that loss scales predictably with data quality and that higher-quality data can substantially reduce model size and hence compute requirements. Our results demonstrate a sublinear decay of effective data with quality and robustness to moderate data corruption; out-of-sample evaluations further validate the predictive form of the law. Unlike prior empirical analyses, our work establishes an explicit, generalizable law for data quality, offering concrete guidance for balancing data curation effort and model scale in large-scale pretraining.",
        "arxiv_id": "2510.03313"
    },
    "2510.04871": {
        "SCORE": 15,
        "ARXIVID": "2510.04871",
        "COMMENT": "Model Architecture: simple recursive tiny network design for reasoning with strong generalization under small parameter budgets.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Alexia Jolicoeur-Martineau"
        ],
        "title": "Less is More: Recursive Reasoning with Tiny Networks",
        "abstract": "Hierarchical Reasoning Model (HRM) is a novel approach using two small neural networks recursing at different frequencies. This biologically inspired method beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze, and ARC-AGI while trained with small models (27M parameters) on small data (around 1000 examples). HRM holds great promise for solving hard problems with small networks, but it is not yet well understood and may be suboptimal. We propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach that achieves significantly higher generalization than HRM, while using a single tiny network with only 2 layers. With only 7M parameters, TRM obtains 45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs (e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the parameters.",
        "arxiv_id": "2510.04871"
    },
    "2510.03262": {
        "SCORE": 15,
        "ARXIVID": "2510.03262",
        "COMMENT": "Model Compression and Efficiency: PEFT/LoRA merging with strict orthogonality via Orthogonal Monte Carlo Dropout; insights on compositionality limits.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Andi Zhang",
            "Xuan Ding",
            "Haofan Wang",
            "Steven McDonagh",
            "Samuel Kaski"
        ],
        "title": "Rethinking Inter-LoRA Orthogonality in Adapter Merging: Insights from Orthogonal Monte Carlo Dropout",
        "abstract": "We propose Orthogonal Monte Carlo Dropout, a mechanism that enforces strict orthogonality when combining sparse semantic vectors without extra time complexity. LoRA, a popular fine-tuning method for large models, typically trains a module to represent a specific concept such as an object or a style. When multiple LoRAs are merged, for example to generate an object in a particular style, their semantic vectors may interfere with each other. Our method guarantees, at the theoretical and runtime levels, that merged LoRAs remain orthogonal and thus free from direct interference. However, empirical analysis reveals that such orthogonality does not lead to the semantic disentanglement or compositionality highlighted in prior work on compositional adaptation. This finding suggests that inter-LoRA orthogonality alone may be insufficient for achieving true semantic compositionality, prompting a re-examination of its role in adapter merging.",
        "arxiv_id": "2510.03262"
    },
    "2510.04567": {
        "SCORE": 15,
        "ARXIVID": "2510.04567",
        "COMMENT": "Model Architecture: introduces a token-based, tuning-free graph in-context learning transformer handling heterogeneous graph tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Weishuo Ma",
            "Yanbo Wang",
            "Xiyuan Wang",
            "Lei Zou",
            "Muhan Zhang"
        ],
        "title": "GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning",
        "abstract": "Graph Neural Networks (GNNs) are powerful tools for precessing relational data but often struggle to generalize to unseen graphs, giving rise to the development of Graph Foundational Models (GFMs). However, current GFMs are challenged by the extreme heterogeneity of graph data, where each graph can possess a unique feature space, label set, and topology. To address this, two main paradigms have emerged. The first leverages Large Language Models (LLMs), but is fundamentally text-dependent, thus struggles to handle the numerical features in vast graphs. The second pre-trains a structure-based model, but the adaptation to new tasks typically requires a costly, per-graph tuning stage, creating a critical efficiency bottleneck. In this work, we move beyond these limitations and introduce \\textbf{G}raph \\textbf{I}n-context \\textbf{L}earning \\textbf{T}ransformer (GILT), a framework built on an LLM-free and tuning-free architecture. GILT introduces a novel token-based framework for in-context learning (ICL) on graphs, reframing classification tasks spanning node, edge and graph levels in a unified framework. This mechanism is the key to handling heterogeneity, as it is designed to operate on generic numerical features. Further, its ability to understand class semantics dynamically from the context enables tuning-free adaptation. Comprehensive experiments show that GILT achieves stronger few-shot performance with significantly less time than LLM-based or tuning-based baselines, validating the effectiveness of our approach.",
        "arxiv_id": "2510.04567"
    },
    "2510.04606": {
        "SCORE": 15,
        "ARXIVID": "2510.04606",
        "COMMENT": "Training algorithm: closed-form last-layer optimization integrated with SGD and NTK-based convergence guarantees, improving optimization efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Alexandre Galashov",
            "Natha\\\"el Da Costa",
            "Liyuan Xu",
            "Philipp Hennig",
            "Arthur Gretton"
        ],
        "title": "Closed-Form Last Layer Optimization",
        "abstract": "Neural networks are typically optimized with variants of stochastic gradient descent. Under a squared loss, however, the optimal solution to the linear last layer weights is known in closed-form. We propose to leverage this during optimization, treating the last layer as a function of the backbone parameters, and optimizing solely for these parameters. We show this is equivalent to alternating between gradient descent steps on the backbone and closed-form updates on the last layer. We adapt the method for the setting of stochastic gradient descent, by trading off the loss on the current batch against the accumulated information from previous batches. Further, we prove that, in the Neural Tangent Kernel regime, convergence of this method to an optimal solution is guaranteed. Finally, we demonstrate the effectiveness of our approach compared with standard SGD on a squared loss in several supervised tasks -- both regression and classification -- including Fourier Neural Operators and Instrumental Variable Regression.",
        "arxiv_id": "2510.04606"
    },
    "2510.04930": {
        "SCORE": 15,
        "ARXIVID": "2510.04930",
        "COMMENT": "Matches Representation Learning: analyzes grokking via asymmetric gradient-speed across singular directions and proposes gradient normalization (a modified natural-gradient style update) to equalize principal-direction dynamics.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Ali Saheb Pasand",
            "Elvis Dohmatob"
        ],
        "title": "Egalitarian Gradient Descent: A Simple Approach to Accelerated Grokking",
        "abstract": "Grokking is the phenomenon whereby, unlike the training performance, which peaks early in the training process, the test/generalization performance of a model stagnates over arbitrarily many epochs and then suddenly jumps to usually close to perfect levels. In practice, it is desirable to reduce the length of such plateaus, that is to make the learning process \"grok\" faster. In this work, we provide new insights into grokking. First, we show both empirically and theoretically that grokking can be induced by asymmetric speeds of (stochastic) gradient descent, along different principal (i.e singular directions) of the gradients. We then propose a simple modification that normalizes the gradients so that dynamics along all the principal directions evolves at exactly the same speed. Then, we establish that this modified method, which we call egalitarian gradient descent (EGD) and can be seen as a carefully modified form of natural gradient descent, groks much faster. In fact, in some cases the stagnation is completely removed. Finally, we empirically show that on classical arithmetic problems such as modular addition and sparse parity problem which this stagnation has been widely observed and intensively studied, that our proposed method eliminates the plateaus.",
        "arxiv_id": "2510.04930"
    },
    "2510.04246": {
        "SCORE": 15,
        "ARXIVID": "2510.04246",
        "COMMENT": "Matches Model Architecture and Efficiency: VLA policy with amortized multi-frame context compression into a single token, enabling temporal context use with reduced training/inference cost.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Huiwon Jang",
            "Sihyun Yu",
            "Heeseung Kwon",
            "Hojin Jeon",
            "Younggyo Seo",
            "Jinwoo Shin"
        ],
        "title": "ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context",
        "abstract": "Leveraging temporal context is crucial for success in partially observable robotic tasks. However, prior work in behavior cloning has demonstrated inconsistent performance gains when using multi-frame observations. In this paper, we introduce ContextVLA, a policy model that robustly improves robotic task performance by effectively leveraging multi-frame observations. Our approach is motivated by the key observation that Vision-Language-Action models (VLA), i.e., policy models built upon a Vision-Language Model (VLM), more effectively utilize multi-frame observations for action generation. This suggests that VLMs' inherent temporal understanding capability enables them to extract more meaningful context from multi-frame observations. However, the high dimensionality of video inputs introduces significant computational overhead, making VLA training and inference inefficient. To address this, ContextVLA compresses past observations into a single context token, allowing the policy to efficiently leverage temporal context for action generation. Our experiments show that ContextVLA consistently improves over single-frame VLAs and achieves the benefits of full multi-frame training but with reduced training and inference times.",
        "arxiv_id": "2510.04246"
    },
    "2510.04898": {
        "SCORE": 15,
        "ARXIVID": "2510.04898",
        "COMMENT": "Matches Model Architecture (Conditional/Dynamic Networks via hypernetworks) and Model Compression/Efficiency (activates only a small task-specific policy at inference with 90x fewer active parameters and 120x faster inference).",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Zheng Xiong",
            "Kang Li",
            "Zilin Wang",
            "Matthew Jackson",
            "Jakob Foerster",
            "Shimon Whiteson"
        ],
        "title": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks",
        "abstract": "Built upon language and vision foundation models with strong generalization ability and trained on large-scale robotic data, Vision-Language-Action (VLA) models have recently emerged as a promising approach to learning generalist robotic policies. However, a key drawback of existing VLAs is their extremely high inference costs. In this paper, we propose HyperVLA to address this problem. Unlike existing monolithic VLAs that activate the whole model during both training and inference, HyperVLA uses a novel hypernetwork (HN)-based architecture that activates only a small task-specific policy during inference, while still retaining the high model capacity needed to accommodate diverse multi-task behaviors during training. Successfully training an HN-based VLA is nontrivial so HyperVLA contains several key algorithm design features that improve its performance, including properly utilizing the prior knowledge from existing vision foundation models, HN normalization, and an action generation strategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even higher success rate for both zero-shot generalization and few-shot adaptation, while significantly reducing inference costs. Compared to OpenVLA, a state-of-the-art VLA model, HyperVLA reduces the number of activated parameters at test time by $90\\times$, and accelerates inference speed by $120\\times$. Code is publicly available at https://github.com/MasterXiong/HyperVLA",
        "arxiv_id": "2510.04898"
    }
}