{
    "2510.00504": {
        "SCORE": 19,
        "ARXIVID": "2510.00504",
        "COMMENT": "Model Compression & Efficiency: theoretical polylog-size compression of models/data; establishes a dynamical lottery ticket hypothesis and accelerated scaling laws.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Hong-Yi Wang",
            "Di Luo",
            "Tomaso Poggio",
            "Isaac L. Chuang",
            "Liu Ziyin"
        ],
        "title": "A universal compression theory: Lottery ticket hypothesis and superpolynomial scaling laws",
        "abstract": "When training large-scale models, the performance typically scales with the number of parameters and the dataset size according to a slow power law. A fundamental theoretical and practical question is whether comparable performance can be achieved with significantly smaller models and substantially less data. In this work, we provide a positive and constructive answer. We prove that a generic permutation-invariant function of $d$ objects can be asymptotically compressed into a function of $\\operatorname{polylog} d$ objects with vanishing error. This theorem yields two key implications: (Ia) a large neural network can be compressed to polylogarithmic width while preserving its learning dynamics; (Ib) a large dataset can be compressed to polylogarithmic size while leaving the loss landscape of the corresponding model unchanged. (Ia) directly establishes a proof of the \\textit{dynamical} lottery ticket hypothesis, which states that any ordinary network can be strongly compressed such that the learning dynamics and result remain unchanged. (Ib) shows that a neural scaling law of the form $L\\sim d^{-\\alpha}$ can be boosted to an arbitrarily fast power law decay, and ultimately to $\\exp(-\\alpha' \\sqrt[m]{d})$.",
        "arxiv_id": "2510.00504"
    },
    "2510.00206": {
        "SCORE": 18,
        "ARXIVID": "2510.00206",
        "COMMENT": "Matches ML Systems and Efficiency: kernel-level fusion to reduce memory traffic and adaptive multi-job batching/scheduling for concurrent LoRA fine-tuning with quantified end-to-end speedups.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Zhanda Zhu",
            "Qidong Su",
            "Yaoyao Ding",
            "Kevin Song",
            "Shang Wang",
            "Gennady Pekhimenko"
        ],
        "title": "LoRAFusion: Efficient LoRA Fine-Tuning for LLMs",
        "abstract": "Low-Rank Adaptation (LoRA) has become the leading Parameter-Efficient Fine-Tuning (PEFT) method for Large Language Models (LLMs), as it significantly reduces GPU memory usage while maintaining competitive fine-tuned model quality on downstream tasks. Despite these benefits, we identify two key inefficiencies in existing LoRA fine-tuning systems. First, they incur substantial runtime overhead due to redundant memory accesses on large activation tensors. Second, they miss the opportunity to concurrently fine-tune multiple independent LoRA adapters that share the same base model on the same set of GPUs. This leads to missed performance gains such as reduced pipeline bubbles, better communication overlap, and improved GPU load balance.   To address these issues, we introduce LoRAFusion, an efficient LoRA fine-tuning system for LLMs. At the kernel level, we propose a graph-splitting method that fuses memory-bound operations. This design eliminates unnecessary memory accesses and preserves the performance of compute-bound GEMMs without incurring the cost of recomputation or synchronization. At the scheduling level, LoRAFusion introduces an adaptive batching algorithm for multi-job fine-tuning. It first splits LoRA adapters into groups to intentionally stagger batch execution across jobs, and then solves a bin-packing problem within each group to generate balanced, dependency-aware microbatches. LoRAFusion achieves up to $1.96\\times$ ($1.47\\times$ on average) end-to-end speedup compared to Megatron-LM, and up to $1.46\\times$ ($1.29\\times$ on average) improvement over mLoRA, the state-of-the-art multi-LoRA fine-tuning system. Our fused kernel achieves up to $1.39\\times$ ($1.27\\times$ on average) kernel performance improvement and can directly serve as a plug-and-play replacement in existing LoRA systems. We open-source LoRAFusion at https://github.com/CentML/lorafusion.",
        "arxiv_id": "2510.00206"
    },
    "2510.00345": {
        "SCORE": 18,
        "ARXIVID": "2510.00345",
        "COMMENT": "Matches Model Architecture: enables stable training of residual-free Transformers via principled initialization based on Jacobian conditioning, with evidence of improved hierarchical representations.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Yiping Ji",
            "James Martens",
            "Jianqiao Zheng",
            "Ziqin Zhou",
            "Peyman Moghadam",
            "Xinyu Zhang",
            "Hemanth Saratchandran",
            "Simon Lucey"
        ],
        "title": "Cutting the Skip: Training Residual-Free Transformers",
        "abstract": "Transformers have achieved remarkable success across a wide range of applications, a feat often attributed to their scalability. Yet training them without skip (residual) connections remains notoriously difficult. While skips stabilize optimization, they also disrupt the hierarchical structure of representations, raising the long-standing question of whether transformers can be trained efficiently without them. In this work, we address this problem by analyzing the Jacobian of a skipless transformer block, showing why skips improve conditioning and revealing that their stabilization benefits can be recovered through a principled initialization strategy. Building on this insight, we introduce the first method that enables stable and efficient training of skipless transformers without altering the standard architecture. We validate our approach on Vision Transformers (ViTs) in both supervised and self-supervised settings, demonstrating that skipless ViTs trained with our initialization overcome the usual optimization barriers, learn richer hierarchical representations, and outperform strong baselines, that incorporate skip connections, on dense prediction benchmarks. These results show that skip connections are not a fundamental requirement for training ViTs and open new avenues for hierarchical representation learning in vision models.",
        "arxiv_id": "2510.00345"
    },
    "2510.00028": {
        "SCORE": 18,
        "ARXIVID": "2510.00028",
        "COMMENT": "Matches Compression/Efficiency: analyzes RoPE interpolation under PTQ and introduces Q-ROAR, a quantization- and position-interpolation\u2013aware rescaling that improves long-context performance without fine-tuning or kernel changes.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Ye Qiao",
            "Haocheng Xu",
            "Xiaofan Zhang",
            "Sitao Huang"
        ],
        "title": "Rethinking RoPE Scaling in Quantized LLM: Theory, Outlier, and Channel-Band Analysis with Weight Rescaling",
        "abstract": "Extending the context window support of large language models (LLMs) is crucial for tasks with long-distance dependencies. RoPE-based interpolation and extrapolation methods, such as linear scaling and frequency-aware schemes, enable longer input length support without retraining, while post-training quantization (PTQ) makes deployment practical. However, we show that combining RoPE position interpolation (PI) with PTQ degrades accuracy due to coupled effects including long-context aliasing, dynamic-range dilation, anisotropy from axis-aligned quantizers vs. rotated RoPE pairs, and outlier shifting that produces position-dependent logit noise. We provide, to the best of our knowledge, the first systematic analysis of the PI+PTQ approach and introduce two practical diagnostics: interpolation pressure (per-band sensitivity to phase scaling) and tail-inflation ratios (outlier shift from short to long contexts). Following the analysis results, we propose Q-ROAR (Quantization, RoPE-interpolation, and Outlier Aware Rescaling), a weight-only, interpolation-aware stabilization of PI for quantized LLMs. Q-ROAR groups RoPE dimensions into a small number of frequency bands and performs a lightweight search over per-band scales for Key and Query weights (with an optional symmetric variant to preserve logit scale). The search is guided by our diagnostics and uses a tiny long-context development dataset, requiring no fine-tuning to the model, no architecture or kernel changes, and no additional deployment overhead. Empirically, Q-ROAR reduces the model's perplexity on long-context workloads by more than 14%, while preserving short-context performance, inference throughput, and compatibility with existing LLM system stacks.",
        "arxiv_id": "2510.00028"
    },
    "2510.01185": {
        "SCORE": 18,
        "ARXIVID": "2510.01185",
        "COMMENT": "Mixture-of-Experts: router regularization via Dirichlet-prior shaping to control expert balance and specialization in upcycled MoEs.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Leyla Mirvakhabova",
            "Babak Ehteshami Bejnordi",
            "Gaurav Kumar",
            "Hanxue Liang",
            "Wanru Zhao",
            "Paul Whatmough"
        ],
        "title": "Dirichlet-Prior Shaping: Guiding Expert Specialization in Upcycled MoEs",
        "abstract": "Upcycling pre-trained dense models into sparse Mixture-of-Experts (MoEs) efficiently increases model capacity but often suffers from poor expert specialization due to naive weight replication. Our analysis reveals that upcycled MoEs, even with conventional regularization, exhibit low-confidence, weakly differentiated routing, hindering performance. We introduce Dirichlet-Prior Shaping Loss (DPSL), a novel router regularization technique that directly shapes routing probability distributions by matching expert assignments to a target Dirichlet prior. DPSL offers fine-grained control over expert balance and specialization, and enables encoding of inductive biases such as encouraging experts to focus on specific modalities or tasks, without requiring manual intervention; notably, DPSL is a general tool applicable to any module that outputs categorical probability distributions, extending its utility beyond MoE training. Experiments on upcycled MoE vision-language models (with Qwen2, Phi3, Llama3.2 LLM backbones) show DPSL consistently outperforms upcycling strategies and regularization techniques across standard vision-language benchmarks, addressing the critical issue of poor specialization and fostering more adaptive, higher-performing models.",
        "arxiv_id": "2510.01185"
    },
    "2510.00219": {
        "SCORE": 18,
        "ARXIVID": "2510.00219",
        "COMMENT": "Model Architecture: conditional/dynamic Transformer that forks/deletes residual streams for parallel latent computation learned during pretraining.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Houjun Liu",
            "Shikhar Murty",
            "Christopher D. Manning",
            "R\\'obert Csord\\'as"
        ],
        "title": "Thoughtbubbles: an Unsupervised Method for Parallel Thinking in Latent Space",
        "abstract": "Current approaches for scaling inference-time compute in transformers rely on training them to emit explicit chain-of-thought tokens before producing an answer. While these methods are powerful, they are limited because they cannot be applied during pretraining and are limited to only serially-generated, natural-language verbalization to scale inference-time compute. In this work, we propose Thoughtbubbles, a transformer variant that natively performs parallel adaptive computation in latent space by learning to fork or delete residual streams. Thus, tokens that require a large amount of computation can form a \"bubble\" of cloned residuals in the middle of the network for additional thinking. Crucially, this behavior is learned during pretraining with only language modeling loss. Thoughtbubbles outperforms both standard decoder LMs as well as non-adaptive parallel computation approaches on OpenWebText and peS2o perplexity and in zero-shot evaluations such as HellaSwag and LAMBADA after pretraining across 150M to 772M parameter scales. The implicit nature of our method enables adaptive computation to be learned starting at pretraining time, paving the way to unify train and test-time behavior for reasoning models.",
        "arxiv_id": "2510.00219"
    },
    "2510.00304": {
        "SCORE": 18,
        "ARXIVID": "2510.00304",
        "COMMENT": "Representation Learning: theoretical training-dynamics analysis identifying stable manifolds (frozen units, cloned-unit redundancy) and linking simplicity/low-rank biases to loss of plasticity.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Amir Joudaki",
            "Giulia Lanzillotta",
            "Mohammad Samragh Razlighi",
            "Iman Mirzadeh",
            "Keivan Alizadeh",
            "Thomas Hofmann",
            "Mehrdad Farajtabar",
            "Fartash Faghri"
        ],
        "title": "Barriers for Learning in an Evolving World: Mathematical Understanding of Loss of Plasticity",
        "abstract": "Deep learning models excel in stationary data but struggle in non-stationary environments due to a phenomenon known as loss of plasticity (LoP), the degradation of their ability to learn in the future. This work presents a first-principles investigation of LoP in gradient-based learning. Grounded in dynamical systems theory, we formally define LoP by identifying stable manifolds in the parameter space that trap gradient trajectories. Our analysis reveals two primary mechanisms that create these traps: frozen units from activation saturation and cloned-unit manifolds from representational redundancy. Our framework uncovers a fundamental tension: properties that promote generalization in static settings, such as low-rank representations and simplicity biases, directly contribute to LoP in continual learning scenarios. We validate our theoretical analysis with numerical simulations and explore architectural choices or targeted perturbations as potential mitigation strategies.",
        "arxiv_id": "2510.00304"
    },
    "2510.00192": {
        "SCORE": 17,
        "ARXIVID": "2510.00192",
        "COMMENT": "Matches Model Compression and Efficiency: structured pruning for low-rank adapters (LoRA) with gradient-based pruning and theoretical robustness analysis; adaptive rank allocation.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Xin Yu",
            "Cong Xie",
            "Ziyu Zhao",
            "Tiantian Fan",
            "Lingzhou Xue",
            "Zhi Zhang"
        ],
        "title": "PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning",
        "abstract": "Low-rank adaptation (LoRA) has become a widely used paradigm for parameter-efficient fine-tuning of large language models, yet its representational capacity often lags behind full fine-tuning. Within the context of LoRA, a key open question is how to obtain expressive low-rank adapters from over-parameterized spaces. We propose \\textit{PrunedLoRA}, a new framework that leverages structured pruning to obtain highly representative low-rank adapters from an over-parameterized initialization. Unlike prior approaches that impose a fixed low-rank budget, PrunedLoRA dynamically prunes less important components during fine-tuning and prevents their reactivation, enabling flexible and adaptive rank allocation. For structured pruning, by minimizing the pruning error for overall loss, we provide fine-grained pruning and recovery updates in a gradient-based pruning strategy with grounded interpretation. We provide the first theoretical analysis of the robustness of structured pruning and provably show that under the impact of weight perturbation, gradient-based pruning is more robust than activation-based pruning with respect to overall loss. Empirically, PrunedLoRA consistently outperforms LoRA and its variants across supervised fine-tuning tasks in mathematical reasoning, code generation, and natural language understanding, and it also demonstrates advantages over existing structured pruning methods across diverse sparsity levels.",
        "arxiv_id": "2510.00192"
    },
    "2510.00563": {
        "SCORE": 17,
        "ARXIVID": "2510.00563",
        "COMMENT": "Model Architecture + Representation Learning: theoretical analysis of SSM memory/learning dynamics (S4 equivalence, initialization importance) and a training strategy (fixing recurrent weights).",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "JingChuan Guan",
            "Tomoyuki Kubota",
            "Yasuo Kuniyoshi",
            "Kohei Nakajima"
        ],
        "title": "Memory Determines Learning Direction: A Theory of Gradient-Based Optimization in State Space Models",
        "abstract": "State space models (SSMs) have gained attention by showing potential to outperform Transformers. However, previous studies have not sufficiently addressed the mechanisms underlying their high performance owing to a lack of theoretical explanation of SSMs' learning dynamics. In this study, we provide such an explanation and propose an improved training strategy. The memory capacity of SSMs can be evaluated by examining how input time series are stored in their current state. Such an examination reveals a tradeoff between memory accuracy and length, as well as the theoretical equivalence between the structured state space sequence model (S4) and a simplified S4 with diagonal recurrent weights. This theoretical foundation allows us to elucidate the learning dynamics, proving the importance of initial parameters. Our analytical results suggest that successful learning requires the initial memory structure to be the longest possible even if memory accuracy may deteriorate or the gradient lose the teacher information. Experiments on tasks requiring long memory confirmed that extending memory is difficult, emphasizing the importance of initialization. Furthermore, we found that fixing recurrent weights can be more advantageous than adapting them because it achieves comparable or even higher performance with faster convergence. Our results provide a new theoretical foundation for SSMs and potentially offer a novel optimization strategy.",
        "arxiv_id": "2510.00563"
    },
    "2510.00184": {
        "SCORE": 17,
        "ARXIVID": "2510.00184",
        "COMMENT": "Representation Learning/Training Dynamics: reverse-engineers Transformer mechanism for long-range dependencies in multiplication and adds inductive-bias loss.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Xiaoyan Bai",
            "Itamar Pres",
            "Yuntian Deng",
            "Chenhao Tan",
            "Stuart Shieber",
            "Fernanda Vi\\'egas",
            "Martin Wattenberg",
            "Andrew Lee"
        ],
        "title": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls",
        "abstract": "Language models are increasingly capable, yet still fail at a seemingly simple task of multi-digit multiplication. In this work, we study why, by reverse-engineering a model that successfully learns multiplication via \\emph{implicit chain-of-thought}, and report three findings: (1) Evidence of long-range structure: Logit attributions and linear probes indicate that the model encodes the necessary long-range dependencies for multi-digit multiplication. (2) Mechanism: the model encodes long-range dependencies using attention to construct a directed acyclic graph to ``cache'' and ``retrieve'' pairwise partial products. (3) Geometry: the model implements partial products in attention heads by forming Minkowski sums between pairs of digits, and digits are represented using a Fourier basis, both of which are intuitive and efficient representations that the standard fine-tuning model lacks. With these insights, we revisit the learning dynamics of standard fine-tuning and find that the model converges to a local optimum that lacks the required long-range dependencies. We further validate this understanding by introducing an auxiliary loss that predicts the ``running sum'' via a linear regression probe, which provides an inductive bias that enables the model to successfully learn multi-digit multiplication. In summary, by reverse-engineering the mechanisms of an implicit chain-of-thought model we uncover a pitfall for learning long-range dependencies in Transformers and provide an example of how the correct inductive bias can address this issue.",
        "arxiv_id": "2510.00184"
    },
    "2510.01143": {
        "SCORE": 17,
        "ARXIVID": "2510.01143",
        "COMMENT": "ML Systems: new parallel inference scaling by sharing batched hidden states to generate interdependent responses, improving throughput\u2013quality tradeoffs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Harry Dong",
            "David Brandfonbrener",
            "Eryk Helenowski",
            "Yun He",
            "Mrinal Kumar",
            "Han Fang",
            "Yuejie Chi",
            "Karthik Abinav Sankararaman"
        ],
        "title": "Generalized Parallel Scaling with Interdependent Generations",
        "abstract": "Parallel LLM inference scaling involves sampling a set of $N>1$ responses for a single input prompt. However, these $N$ parallel responses tend to be generated independently from each other, partitioning compute resources and leaving potentially useful information in one generation untapped by others. This is in contrast to response length scaling where past computation is used in all future steps. For higher quality responses and response sets, we propose Bridge to generate interdependent responses in parallel by rethinking batched LLM hidden states as holistic tensors rather than independent slices. With only a small amount (2.8%-5.1%) of new parameters, Bridge improves the relative mean accuracy gains from reinforcement learning with verifiable rewards by up to 50% and boosts consistency of correct responses. Trained once, Bridge scales to any generation width, all with greater performance than independent generations, unlocking a more general mode of parallel scaling that effectively leverages information between sequences, compatible with any post-generation aggregation technique.",
        "arxiv_id": "2510.01143"
    },
    "2510.00442": {
        "SCORE": 17,
        "ARXIVID": "2510.00442",
        "COMMENT": "ML Systems/Memory Optimization: activation matrix sketching with EMA and adaptive rank for memory-efficient backprop and gradient monitoring.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Harbir Antil",
            "Deepanshu Verma"
        ],
        "title": "Randomized Matrix Sketching for Neural Network Training and Gradient Monitoring",
        "abstract": "Neural network training relies on gradient computation through backpropagation, yet memory requirements for storing layer activations present significant scalability challenges. We present the first adaptation of control-theoretic matrix sketching to neural network layer activations, enabling memory-efficient gradient reconstruction in backpropagation. This work builds on recent matrix sketching frameworks for dynamic optimization problems, where similar state trajectory storage challenges motivate sketching techniques. Our approach sketches layer activations using three complementary sketch matrices maintained through exponential moving averages (EMA) with adaptive rank adjustment, automatically balancing memory efficiency against approximation quality. Empirical evaluation on MNIST, CIFAR-10, and physics-informed neural networks demonstrates a controllable accuracy-memory tradeoff. We demonstrate a gradient monitoring application on MNIST showing how sketched activations enable real-time gradient norm tracking with minimal memory overhead. These results establish that sketched activation storage provides a viable path toward memory-efficient neural network training and analysis.",
        "arxiv_id": "2510.00442"
    },
    "2510.00294": {
        "SCORE": 17,
        "ARXIVID": "2510.00294",
        "COMMENT": "ML Systems (Inference): lossless parallel decoding algorithm for Diffusion LLMs achieving up to 2.8x throughput without extra forward passes.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Shutong Wu",
            "Jiawei Zhang"
        ],
        "title": "Free Draft-and-Verification: Toward Lossless Parallel Decoding for Diffusion Large Language Models",
        "abstract": "Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of language modeling beyond autoregressive next-token prediction. Thanks to their bidirectional attention mechanism, DLLMs are more capable of capturing the connection of context, and thus show unique advantages in challenges like the famous \"reversal curse\" or learning under data-constrained scenarios. However, this bidirectional nature also brings an obstacle that DLLMs are not inherently compatible with KV Cache, and consequently, the inference efficiency is not competitive compared with autoregressive models. Taking advantage of their inherent capability of multi-token prediction, existing parallel decoding algorithms can speed up the DLLM inference, but at the cost of non-negligible performance degradation. To overcome this challenge, we introduce Free Draft-and-Verification (Freedave), a novel fast sampling algorithm tailored for DLLMs that achieves lossless parallel decoding. Specifically, we propose a pipeline of parallel-decoded candidate generation and verification, which is guaranteed to reproduce the same sequence generated by static sampling, without introducing extra model forward calls. By applying Freedave, the throughput of DLLMs can be boosted up to $2.8\\times$ without performance degradation on math reasoning tasks.",
        "arxiv_id": "2510.00294"
    },
    "2510.00404": {
        "SCORE": 17,
        "ARXIVID": "2510.00404",
        "COMMENT": "Representation Learning: principled SAE derivation via unrolled proximal gradient; new AbsTopK variant enabling bidirectional sparse features (dictionary learning).",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Xudong Zhu",
            "Mohammad Mahdi Khalili",
            "Zhihui Zhu"
        ],
        "title": "AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features",
        "abstract": "Sparse autoencoders (SAEs) have emerged as powerful techniques for interpretability of large language models (LLMs), aiming to decompose hidden states into meaningful semantic features. While several SAE variants have been proposed, there remains no principled framework to derive SAEs from the original dictionary learning formulation. In this work, we introduce such a framework by unrolling the proximal gradient method for sparse coding. We show that a single-step update naturally recovers common SAE variants, including ReLU, JumpReLU, and TopK. Through this lens, we reveal a fundamental limitation of existing SAEs: their sparsity-inducing regularizers enforce non-negativity, preventing a single feature from representing bidirectional concepts (e.g., male vs. female). This structural constraint fragments semantic axes into separate, redundant features, limiting representational completeness. To address this issue, we propose AbsTopK SAE, a new variant derived from the $\\ell_0$ sparsity constraint that applies hard thresholding over the largest-magnitude activations. By preserving both positive and negative activations, AbsTopK uncovers richer, bidirectional conceptual representations. Comprehensive experiments across four LLMs and seven probing and steering tasks show that AbsTopK improves reconstruction fidelity, enhances interpretability, and enables single features to encode contrasting concepts. Remarkably, AbsTopK matches or even surpasses the Difference-in-Mean method, a supervised approach that requires labeled data for each concept and has been shown in prior work to outperform SAEs.",
        "arxiv_id": "2510.00404"
    },
    "2510.01098": {
        "SCORE": 17,
        "ARXIVID": "2510.01098",
        "COMMENT": "Representation Learning/Training Dynamics: solvable scaling laws for in-context regression in deep linear self-attention, linking depth/width/context/steps to risk.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Blake Bordelon",
            "Mary I. Letey",
            "Cengiz Pehlevan"
        ],
        "title": "Theory of Scaling Laws for In-Context Regression: Depth, Width, Context and Time",
        "abstract": "We study in-context learning (ICL) of linear regression in a deep linear self-attention model, characterizing how performance depends on various computational and statistical resources (width, depth, number of training steps, batch size and data per context). In a joint limit where data dimension, context length, and residual stream width scale proportionally, we analyze the limiting asymptotics for three ICL settings: (1) isotropic covariates and tasks (ISO), (2) fixed and structured covariance (FS), and (3) where covariances are randomly rotated and structured (RRS). For ISO and FS settings, we find that depth only aids ICL performance if context length is limited. Alternatively, in the RRS setting where covariances change across contexts, increasing the depth leads to significant improvements in ICL, even at infinite context length. This provides a new solvable toy model of neural scaling laws which depends on both width and depth of a transformer and predicts an optimal transformer shape as a function of compute. This toy model enables computation of exact asymptotics for the risk as well as derivation of powerlaws under source/capacity conditions for the ICL tasks.",
        "arxiv_id": "2510.01098"
    },
    "2510.00136": {
        "SCORE": 17,
        "ARXIVID": "2510.00136",
        "COMMENT": "Matches Representation Learning criterion: provides nonparametric identifiability theory for latent concepts across classes.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yujia Zheng",
            "Shaoan Xie",
            "Kun Zhang"
        ],
        "title": "Nonparametric Identification of Latent Concepts",
        "abstract": "We are born with the ability to learn concepts by comparing diverse observations. This helps us to understand the new world in a compositional manner and facilitates extrapolation, as objects naturally consist of multiple concepts. In this work, we argue that the cognitive mechanism of comparison, fundamental to human learning, is also vital for machines to recover true concepts underlying the data. This offers correctness guarantees for the field of concept learning, which, despite its impressive empirical successes, still lacks general theoretical support. Specifically, we aim to develop a theoretical framework for the identifiability of concepts with multiple classes of observations. We show that with sufficient diversity across classes, hidden concepts can be identified without assuming specific concept types, functional relations, or parametric generative models. Interestingly, even when conditions are not globally satisfied, we can still provide alternative guarantees for as many concepts as possible based on local comparisons, thereby extending the applicability of our theory to more flexible scenarios. Moreover, the hidden structure between classes and concepts can also be identified nonparametrically. We validate our theoretical results in both synthetic and real-world settings.",
        "arxiv_id": "2510.00136"
    },
    "2510.00517": {
        "SCORE": 17,
        "ARXIVID": "2510.00517",
        "COMMENT": "Model Architecture: theoretical and empirical analysis of Differential Attention reveals a selectivity\u2013robustness trade-off, informing future attention mechanism design.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Tsubasa Takahashi",
            "Shojiro Yamabe",
            "Futa Waseda",
            "Kento Sasaki"
        ],
        "title": "Understanding Sensitivity of Differential Attention through the Lens of Adversarial Robustness",
        "abstract": "Differential Attention (DA) has been proposed as a refinement to standard attention, suppressing redundant or noisy context through a subtractive structure and thereby reducing contextual hallucination. While this design sharpens task-relevant focus, we show that it also introduces a structural fragility under adversarial perturbations. Our theoretical analysis identifies negative gradient alignment-a configuration encouraged by DA's subtraction-as the key driver of sensitivity amplification, leading to increased gradient norms and elevated local Lipschitz constants. We empirically validate this Fragile Principle through systematic experiments on ViT/DiffViT and evaluations of pretrained CLIP/DiffCLIP, spanning five datasets in total. These results demonstrate higher attack success rates, frequent gradient opposition, and stronger local sensitivity compared to standard attention. Furthermore, depth-dependent experiments reveal a robustness crossover: stacking DA layers attenuates small perturbations via depth-dependent noise cancellation, though this protection fades under larger attack budgets. Overall, our findings uncover a fundamental trade-off: DA improves discriminative focus on clean inputs but increases adversarial vulnerability, underscoring the need to jointly design for selectivity and robustness in future attention mechanisms.",
        "arxiv_id": "2510.00517"
    },
    "2510.00977": {
        "SCORE": 17,
        "ARXIVID": "2510.00977",
        "COMMENT": "Model Efficiency/Training Algorithms: reveals GRPO\u2013DPO equivalence and validates 2-sample GRPO for substantial compute savings with theory and empirical evidence.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Yihong Wu",
            "Liheng Ma",
            "Lei Ding",
            "Muzhi Li",
            "Xinyu Wang",
            "Kejia Chen",
            "Zhan Su",
            "Zhanguang Zhang",
            "Chenyang Huang",
            "Yingxue Zhang",
            "Mark Coates",
            "Jian-Yun Nie"
        ],
        "title": "It Takes Two: Your GRPO Is Secretly DPO",
        "abstract": "Group Relative Policy Optimization (GRPO) is a prominent reinforcement learning algorithm for post-training Large Language Models (LLMs). It is commonly believed that GRPO necessitates a large group size to ensure stable training via precise statistical estimation, which incurs substantial computational overhead. In this work, we challenge this assumption by reframing GRPO as a form of contrastive learning, which reveals a fundamental connection to Direct Preference Optimization (DPO). Motivated by DPO's empirical success, we investigate the minimal two-rollout case (2-GRPO), a configuration previously deemed infeasible. We provide a rigorous theoretical analysis to validate 2-GRPO and demonstrate empirically that it achieves performance on par with 16-GRPO, despite using only 1/8 of the rollouts and reducing training time by over 70%.",
        "arxiv_id": "2510.00977"
    },
    "2510.00537": {
        "SCORE": 16,
        "ARXIVID": "2510.00537",
        "COMMENT": "Matches Representation Learning and Model Architecture analysis: introduces spectral utilization diagnostics and scaling laws for FFN width in LLMs, informing capacity use and width selection.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Nandan Kumar Jha",
            "Brandon Reagen"
        ],
        "title": "Spectral Scaling Laws in Language Models: How Effectively Do Feed-Forward Networks Use Their Latent Space?",
        "abstract": "As large language models (LLMs) scale, the question is not only how large they become, but how much of their capacity is effectively utilized. Existing scaling laws relate model size to loss, yet overlook how components exploit their latent space. We study feed-forward networks (FFNs) and recast width selection as a spectral utilization problem. Using a lightweight diagnostic suite -- Hard Rank (participation ratio), Soft Rank (Shannon rank), Spectral Concentration, and the composite Spectral Utilization Index (SUI) -- we quantify how many latent directions are meaningfully activated across LLaMA, GPT-2, and nGPT families. Our key finding is an asymmetric spectral scaling law: soft rank follows an almost perfect power law with FFN width, while hard rank grows only sublinearly and with high variance. This asymmetry suggests that widening FFNs mostly adds low-energy tail directions, while dominant-mode subspaces saturate early. Moreover, at larger widths, variance further collapses into a narrow subspace, leaving much of the latent space under-utilized. These results recast FFN width selection as a principled trade-off between tail capacity and dominant-mode capacity, offering concrete guidance for inference-efficient LLM design.",
        "arxiv_id": "2510.00537"
    },
    "2510.01022": {
        "SCORE": 16,
        "ARXIVID": "2510.01022",
        "COMMENT": "Model Architecture: introduces an SE(3)-equivariant geometric scattering transform/GNN, achieving comparable accuracy with far fewer parameters (architecture-level innovation).",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "David R. Johnson",
            "Rishabh Anand",
            "Smita Krishnaswamy",
            "Michael Perlmutter"
        ],
        "title": "Equivariant Geometric Scattering Networks via Vector Diffusion Wavelets",
        "abstract": "We introduce a novel version of the geometric scattering transform for geometric graphs containing scalar and vector node features. This new scattering transform has desirable symmetries with respect to rigid-body roto-translations (i.e., $SE(3)$-equivariance) and may be incorporated into a geometric GNN framework. We empirically show that our equivariant scattering-based GNN achieves comparable performance to other equivariant message-passing-based GNNs at a fraction of the parameter count.",
        "arxiv_id": "2510.01022"
    },
    "2510.00570": {
        "SCORE": 16,
        "ARXIVID": "2510.00570",
        "COMMENT": "Model Architecture: LoRA-based MoE with adaptive shared experts and fine-grained low-rank experts; conditional routing for efficient multi-task learning.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Minghao Yang",
            "Ren Togo",
            "Guang Li",
            "Takahiro Ogawa",
            "Miki Haseyama"
        ],
        "title": "Adaptive Shared Experts with LoRA-Based Mixture of Experts for Multi-Task Learning",
        "abstract": "Mixture-of-Experts (MoE) has emerged as a powerful framework for multi-task learning (MTL). However, existing MoE-MTL methods often rely on single-task pretrained backbones and suffer from redundant adaptation and inefficient knowledge sharing during the transition from single-task to multi-task learning (STL to MTL). To address these limitations, we propose adaptive shared experts (ASE) within a low-rank adaptation (LoRA) based MoE, where shared experts are assigned router-computed gating weights jointly normalized with sparse experts. This design facilitates STL to MTL transition, enhances expert specialization, and cooperation. Furthermore, we incorporate fine-grained experts by increasing the number of LoRA experts while proportionally reducing their rank, enabling more effective knowledge sharing under a comparable parameter budget. Extensive experiments on the PASCAL-Context benchmark, under unified training settings, demonstrate that ASE consistently improves performance across diverse configurations and validates the effectiveness of fine-grained designs for MTL.",
        "arxiv_id": "2510.00570"
    },
    "2510.00399": {
        "SCORE": 16,
        "ARXIVID": "2510.00399",
        "COMMENT": "Model Architecture Theory: first generalization analysis of Mamba\u2019s ICL with outliers; shows gating suppresses outlier influence vs linear Transformers.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Hongkang Li",
            "Songtao Lu",
            "Xiaodong Cui",
            "Pin-Yu Chen",
            "Meng Wang"
        ],
        "title": "Can Mamba Learn In Context with Outliers? A Theoretical Generalization Analysis",
        "abstract": "The Mamba model has gained significant attention for its computational advantages over Transformer-based models, while achieving comparable performance across a wide range of language tasks. Like Transformers, Mamba exhibits in-context learning (ICL) capabilities, i.e., making predictions for new tasks based on a prompt containing input-label pairs and a query, without requiring fine-tuning. Despite its empirical success, the theoretical understanding of Mamba remains limited, largely due to the nonlinearity introduced by its gating mechanism. To the best of our knowledge, this paper presents the first theoretical analysis of the training dynamics of a one-layer Mamba model, which consists of a linear attention component followed by a nonlinear gating layer, and its ICL generalization on unseen binary classification tasks, even when the prompt includes additive outliers. Our analysis shows that Mamba leverages the linear attention layer to select informative context examples and uses the nonlinear gating layer to suppress the influence of outliers. By establishing and comparing to the analysis of linear Transformers under the same setting, we show that although Mamba may require more training iterations to converge, it maintains accurate predictions even when the proportion of outliers exceeds the threshold that a linear Transformer can tolerate. These theoretical findings are supported by empirical experiments.",
        "arxiv_id": "2510.00399"
    },
    "2510.00258": {
        "SCORE": 16,
        "ARXIVID": "2510.00258",
        "COMMENT": "Model Architecture/Training Dynamics: Transformer\u2013RNN hybrid with a simple delayed-attention training strategy to avoid shortcut solutions and improve length generalization.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Buu Phan",
            "Reza Ebrahimi",
            "Sanjay Haresh",
            "Roland Memisevic"
        ],
        "title": "Delayed Attention Training Improves Length Generalization in Transformer--RNN Hybrids",
        "abstract": "We study length generalization in sequence models on a composite problem involving both state tracking and associative recall. Prior work finds that recurrent networks handle state tracking well but struggle with recall, whereas Transformers excel at recall yet fail to extend state-tracking capabilities to longer sequences. Motivated by the complementary strengths of these architectures, we construct hybrid models integrating recurrent and attention-based components, and train them on the combined task to evaluate whether both capabilities can be preserved. Our results reveal that, in such hybrids, the Transformer component tends to exploit shortcut solutions, leading to poor length generalization. We identify this shortcut reliance as a key obstacle and propose a simple yet effective training strategy -- delaying the training of the attention layers -- that mitigates this effect and significantly improves length generalization performance. Our experiments show that this approach enables hybrid models to achieve near-perfect accuracy ($>90\\%$) on hybrid sequences three times longer than those used during training.",
        "arxiv_id": "2510.00258"
    },
    "2510.00468": {
        "SCORE": 16,
        "ARXIVID": "2510.00468",
        "COMMENT": "Matches Representation Learning criterion: uses empirical NTK eigenanalysis to surface learned features and diagnose phase transitions (grokking).",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Jennifer Lin"
        ],
        "title": "Feature Identification via the Empirical NTK",
        "abstract": "We provide evidence that eigenanalysis of the empirical neural tangent kernel (eNTK) can surface the features used by trained neural networks. Across two standard toy models for mechanistic interpretability, Toy Models of Superposition (TMS) and a 1-layer MLP trained on modular addition, we find that the eNTK exhibits sharp spectral cliffs whose top eigenspaces align with ground-truth features. In TMS, the eNTK recovers the ground-truth features in both the sparse (high superposition) and dense regimes. In modular arithmetic, the eNTK can be used to recover Fourier feature families. Moreover, we provide evidence that a layerwise eNTK localizes features to specific layers and that the evolution of the eNTK eigenspectrum can be used to diagnose the grokking phase transition. These results suggest that eNTK analysis may provide a practical handle for feature discovery and for detecting phase changes in small models.",
        "arxiv_id": "2510.00468"
    },
    "2510.00844": {
        "SCORE": 16,
        "ARXIVID": "2510.00844",
        "COMMENT": "Matches Representation Learning and MoE Architecture criteria: learns compact model/query embeddings for ability estimation using a Mixture-of-Experts network, enabling routing and prediction.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Jianhao Chen",
            "Chenxu Wang",
            "Gengrui Zhang",
            "Peng Ye",
            "Lei Bai",
            "Wei Hu",
            "Yuzhong Qu",
            "Shuyue Hu"
        ],
        "title": "Learning Compact Representations of LLM Abilities via Item Response Theory",
        "abstract": "Recent years have witnessed a surge in the number of large language models (LLMs), yet efficiently managing and utilizing these vast resources remains a significant challenge. In this work, we explore how to learn compact representations of LLM abilities that can facilitate downstream tasks, such as model routing and performance prediction on new benchmarks. We frame this problem as estimating the probability that a given model will correctly answer a specific query. Inspired by the item response theory (IRT) in psychometrics, we model this probability as a function of three key factors: (i) the model's multi-skill ability vector, (2) the query's discrimination vector that separates models of differing skills, and (3) the query's difficulty scalar. To learn these parameters jointly, we introduce a Mixture-of-Experts (MoE) network that couples model- and query-level embeddings. Extensive experiments demonstrate that our approach leads to state-of-the-art performance in both model routing and benchmark accuracy prediction. Moreover, analysis validates that the learned parameters encode meaningful, interpretable information about model capabilities and query characteristics.",
        "arxiv_id": "2510.00844"
    },
    "2510.00526": {
        "SCORE": 16,
        "ARXIVID": "2510.00526",
        "COMMENT": "Representation Learning: proposes and analyzes probability-based SFT objectives across a model-capability continuum, offering general training-dynamics insights beyond NLL.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Gaotang Li",
            "Ruizhong Qiu",
            "Xiusi Chen",
            "Heng Ji",
            "Hanghang Tong"
        ],
        "title": "Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum",
        "abstract": "Supervised fine-tuning (SFT) is the standard approach for post-training large language models (LLMs), yet it often shows limited generalization. We trace this limitation to its default training objective: negative log likelihood (NLL). While NLL is classically optimal when training from scratch, post-training operates in a different paradigm and could violate its optimality assumptions, where models already encode task-relevant priors and supervision can be long and noisy. To this end, we study a general family of probability-based objectives and characterize their effectiveness under different conditions. Through comprehensive experiments and extensive ablation studies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a critical dimension that governs objective behavior: the model-capability continuum. Near the model-strong end, prior-leaning objectives that downweight low-probability tokens (e.g., $-p$, $-p^{10}$, thresholded variants) consistently outperform NLL; toward the model-weak end, NLL dominates; in between, no single objective prevails. Our theoretical analysis further elucidates how objectives trade places across the continuum, providing a principled foundation for adapting objectives to model capability. Our code is available at https://github.com/GaotangLi/Beyond-Log-Likelihood.",
        "arxiv_id": "2510.00526"
    },
    "2510.00260": {
        "SCORE": 16,
        "ARXIVID": "2510.00260",
        "COMMENT": "Representation Learning & Efficiency: VAE prior as a variational EBM avoiding MCMC; architectural prior design improving ELBO and fast sampling.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Debottam Dutta",
            "Chaitanya Amballa",
            "Zhongweiyang Xu",
            "Yu-Lin Wei",
            "Romit Roy Choudhury"
        ],
        "title": "Learning Energy-based Variational Latent Prior for VAEs",
        "abstract": "Variational Auto-Encoders (VAEs) are known to generate blurry and inconsistent samples. One reason for this is the \"prior hole\" problem. A prior hole refers to regions that have high probability under the VAE's prior but low probability under the VAE's posterior. This means that during data generation, high probability samples from the prior could have low probability under the posterior, resulting in poor quality data. Ideally, a prior needs to be flexible enough to match the posterior while retaining the ability to generate samples fast. Generative models continue to address this tradeoff. This paper proposes to model the prior as an energy-based model (EBM). While EBMs are known to offer the flexibility to match posteriors (and also improving the ELBO), they are traditionally slow in sample generation due to their dependency on MCMC methods. Our key idea is to bring a variational approach to tackle the normalization constant in EBMs, thus bypassing the expensive MCMC approaches. The variational form can be approximated with a sampler network, and we show that such an approach to training priors can be formulated as an alternating optimization problem. Moreover, the same sampler reduces to an implicit variational prior during generation, providing efficient and fast sampling. We compare our Energy-based Variational Latent Prior (EVaLP) method to multiple SOTA baselines and show improvements in image generation quality, reduced prior holes, and better sampling efficiency.",
        "arxiv_id": "2510.00260"
    },
    "2510.00553": {
        "SCORE": 16,
        "ARXIVID": "2510.00553",
        "COMMENT": "Representation Learning/Training Dynamics: discovers rank-1 dominated, linear update subspace in RL for LLMs; Efficiency: AlphaRL extrapolates updates to cut training cost.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Yuchen Cai",
            "Ding Cao",
            "Xin Xu",
            "Zijun Yao",
            "Yuqing Huang",
            "Zhenyu Tan",
            "Benyi Zhang",
            "Guiquan Liu",
            "Junfeng Fang"
        ],
        "title": "On Predictability of Reinforcement Learning Dynamics for Large Language Models",
        "abstract": "Recent advances in reasoning capabilities of large language models (LLMs) are largely driven by reinforcement learning (RL), yet the underlying parameter dynamics during RL training remain poorly understood. This work identifies two fundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1 Dominance, where the top singular subspace of the parameter update matrix nearly fully determines reasoning improvements, recovering over 99\\% of performance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace evolves linearly throughout training, enabling accurate prediction from early checkpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the generalizability of these properties. More importantly, based on these findings, we propose AlphaRL, a plug-in acceleration framework that extrapolates the final parameter update using a short early training window, achieving up to 2.5 speedup while retaining \\textgreater 96\\% of reasoning performance without extra modules or hyperparameter tuning. This positions our finding as a versatile and practical tool for large-scale RL, opening a path toward principled, interpretable, and efficient training paradigm for LLMs.",
        "arxiv_id": "2510.00553"
    },
    "2510.00983": {
        "SCORE": 16,
        "ARXIVID": "2510.00983",
        "COMMENT": "Model Architecture/Representation Learning: introduces consistency models on Riemannian manifolds with exponential-map parameterization, closed-form objectives, and theoretical equivalence between training variants.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Chaoran Cheng",
            "Yusong Wang",
            "Yuxin Chen",
            "Xiangxin Zhou",
            "Nanning Zheng",
            "Ge Liu"
        ],
        "title": "Riemannian Consistency Model",
        "abstract": "Consistency models are a class of generative models that enable few-step generation for diffusion and flow matching models. While consistency models have achieved promising results on Euclidean domains like images, their applications to Riemannian manifolds remain challenging due to the curved geometry. In this work, we propose the Riemannian Consistency Model (RCM), which, for the first time, enables few-step consistency modeling while respecting the intrinsic manifold constraint imposed by the Riemannian geometry. Leveraging the covariant derivative and exponential-map-based parameterization, we derive the closed-form solutions for both discrete- and continuous-time training objectives for RCM. We then demonstrate theoretical equivalence between the two variants of RCM: Riemannian consistency distillation (RCD) that relies on a teacher model to approximate the marginal vector field, and Riemannian consistency training (RCT) that utilizes the conditional vector field for training. We further propose a simplified training objective that eliminates the need for the complicated differential calculation. Finally, we provide a unique kinematics perspective for interpreting the RCM objective, offering new theoretical angles. Through extensive experiments, we manifest the superior generative quality of RCM in few-step generation on various non-Euclidean manifolds, including flat-tori, spheres, and the 3D rotation group SO(3).",
        "arxiv_id": "2510.00983"
    },
    "2510.01175": {
        "SCORE": 16,
        "ARXIVID": "2510.01175",
        "COMMENT": "Representation Learning/Optimization Theory: analyzes generalized weight normalization in overparameterized matrix sensing, proving linear convergence and benefits that scale with overparameterization.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Yudong Wei",
            "Liang Zhang",
            "Bingcong Li",
            "Niao He"
        ],
        "title": "On the Benefits of Weight Normalization for Overparameterized Matrix Sensing",
        "abstract": "While normalization techniques are widely used in deep learning, their theoretical understanding remains relatively limited. In this work, we establish the benefits of (generalized) weight normalization (WN) applied to the overparameterized matrix sensing problem. We prove that WN with Riemannian optimization achieves linear convergence, yielding an exponential speedup over standard methods that do not use WN. Our analysis further demonstrates that both iteration and sample complexity improve polynomially as the level of overparameterization increases. To the best of our knowledge, this work provides the first characterization of how WN leverages overparameterization for faster convergence in matrix sensing.",
        "arxiv_id": "2510.01175"
    },
    "2510.00079": {
        "SCORE": 16,
        "ARXIVID": "2510.00079",
        "COMMENT": "Matches ML Systems/Efficiency criterion: proposes offline, query-agnostic context selection via a \u03b3-cover with submodular guarantees, reducing context I/O with provable bounds.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Hai Huang"
        ],
        "title": "Directed Information $\\gamma$-covering: An Information-Theoretic Framework for Context Engineering",
        "abstract": "We introduce \\textbf{Directed Information $\\gamma$-covering}, a simple but general framework for redundancy-aware context engineering. Directed information (DI), a causal analogue of mutual information, measures asymmetric predictiveness between chunks. If $\\operatorname{DI}_{i \\to j} \\ge H(C_j) - \\gamma$, then $C_i$ suffices to represent $C_j$ up to $\\gamma$ bits. Building on this criterion, we formulate context selection as a $\\gamma$-cover problem and propose a greedy algorithm with provable guarantees: it preserves query information within bounded slack, inherits $(1+\\ln n)$ and $(1-1/e)$ approximations from submodular set cover, and enforces a diversity margin. Importantly, building the $\\gamma$-cover is \\emph{query-agnostic}: it incurs no online cost and can be computed once offline and amortized across all queries. Experiments on HotpotQA show that $\\gamma$-covering consistently improves over BM25, a competitive baseline, and provides clear advantages in hard-decision regimes such as context compression and single-slot prompt selection. These results establish DI $\\gamma$-covering as a principled, self-organizing backbone for modern LLM pipelines.",
        "arxiv_id": "2510.00079"
    },
    "2510.00866": {
        "SCORE": 15,
        "ARXIVID": "2510.00866",
        "COMMENT": "ML Systems/Representation Learning: critical analysis of classifier-based data filtering for LLM pretraining, revealing limits of the approach and challenging assumptions about data \u201cquality.\u201d",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Thiziri Nait Saada",
            "Louis Bethune",
            "Michal Klein",
            "David Grangier",
            "Marco Cuturi",
            "Pierre Ablin"
        ],
        "title": "The data-quality illusion: Rethinking Classifier-based quality filtering for LLM Pretraining",
        "abstract": "Large-scale models are pretrained on massive web-crawled datasets containing documents of mixed quality, making data filtering essential. A popular method is Classifier-based Quality Filtering (CQF), which trains a binary classifier to distinguish between pretraining data and a small, high-quality set. It assigns each pretraining document a quality score defined as the classifier's score and retains only the top-scoring ones. We provide an in-depth analysis of CQF. We show that while CQF improves downstream task performance, it does not necessarily enhance language modeling on the high-quality dataset. We explain this paradox by the fact that CQF implicitly filters the high-quality dataset as well. We further compare the behavior of models trained with CQF to those trained on synthetic data of increasing quality, obtained via random token permutations, and find starkly different trends. Our results challenge the view that CQF captures a meaningful notion of data quality.",
        "arxiv_id": "2510.00866"
    },
    "2510.01025": {
        "SCORE": 15,
        "ARXIVID": "2510.01025",
        "COMMENT": "Matches Representation Learning: proposes a method (SMDS) to automatically discover geometric feature manifolds in LLM latent space and analyzes their role in reasoning across models.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Federico Tiblias",
            "Irina Bigoulaeva",
            "Jingcheng Niu",
            "Simone Balloccu",
            "Iryna Gurevych"
        ],
        "title": "Shape Happens: Automatic Feature Manifold Discovery in LLMs via Supervised Multi-Dimensional Scaling",
        "abstract": "The linear representation hypothesis states that language models (LMs) encode concepts as directions in their latent space, forming organized, multidimensional manifolds. Prior efforts focus on discovering specific geometries for specific features, and thus lack generalization. We introduce Supervised Multi-Dimensional Scaling (SMDS), a model-agnostic method to automatically discover feature manifolds. We apply SMDS to temporal reasoning as a case study, finding that different features form various geometric structures such as circles, lines, and clusters. SMDS reveals many insights on these structures: they consistently reflect the properties of the concepts they represent; are stable across model families and sizes; actively support reasoning in models; and dynamically reshape in response to context changes. Together, our findings shed light on the functional role of feature manifolds, supporting a model of entity-based reasoning in which LMs encode and transform structured representations.",
        "arxiv_id": "2510.01025"
    },
    "2510.00883": {
        "SCORE": 15,
        "ARXIVID": "2510.00883",
        "COMMENT": "Matches Model Architecture/Efficiency: GLAI decouples structural vs. quantitative knowledge in MLPs, fixing activation patterns to accelerate training while preserving accuracy, with potential integration into Transformers.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Jose I. Mestre",
            "Alberto Fern\\'andez-Hern\\'andez",
            "Cristian P\\'erez-Corral",
            "Manuel F. Dolz",
            "Jose Duato",
            "Enrique S. Quintana-Ort\\'i"
        ],
        "title": "GLAI: GreenLightningAI for Accelerated Training through Knowledge Decoupling",
        "abstract": "In this work we introduce GreenLightningAI (GLAI), a new architectural block designed as an alternative to conventional MLPs. The central idea is to separate two types of knowledge that are usually entangled during training: (i) *structural knowledge*, encoded by the stable activation patterns induced by ReLU activations; and (ii) *quantitative knowledge*, carried by the numerical weights and biases. By fixing the structure once stabilized, GLAI reformulates the MLP as a combination of paths, where only the quantitative component is optimized. This reformulation retains the universal approximation capabilities of MLPs, yet achieves a more efficient training process, reducing training time by ~40% on average across the cases examined in this study. Crucially, GLAI is not just another classifier, but a generic block that can replace MLPs wherever they are used, from supervised heads with frozen backbones to projection layers in self-supervised learning or few-shot classifiers. Across diverse experimental setups, GLAI consistently matches or exceeds the accuracy of MLPs with an equivalent number of parameters, while converging faster. Overall, GLAI establishes a new design principle that opens a direction for future integration into large-scale architectures such as Transformers, where MLP blocks dominate the computational footprint.",
        "arxiv_id": "2510.00883"
    },
    "2510.00083": {
        "SCORE": 15,
        "ARXIVID": "2510.00083",
        "COMMENT": "Matches Compression/Efficiency: robustness-oriented pruning using a new Unbiased and Smooth Neuron metric (USN) and Wasserstein loss to improve certifiable robustness while reducing over-parameterization.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Hanjiang Hu",
            "Bowei Li",
            "Ziwei Wang",
            "Tianhao Wei",
            "Casidhe Hutchison",
            "Eric Sample",
            "Changliu Liu"
        ],
        "title": "Enhancing Certifiable Semantic Robustness via Robust Pruning of Deep Neural Networks",
        "abstract": "Deep neural networks have been widely adopted in many vision and robotics applications with visual inputs. It is essential to verify its robustness against semantic transformation perturbations, such as brightness and contrast. However, current certified training and robustness certification methods face the challenge of over-parameterization, which hinders the tightness and scalability due to the over-complicated neural networks. To this end, we first analyze stability and variance of layers and neurons against input perturbation, showing that certifiable robustness can be indicated by a fundamental Unbiased and Smooth Neuron metric (USN). Based on USN, we introduce a novel neural network pruning method that removes neurons with low USN and retains those with high USN, thereby preserving model expressiveness without over-parameterization. To further enhance this pruning process, we propose a new Wasserstein distance loss to ensure that pruned neurons are more concentrated across layers. We validate our approach through extensive experiments on the challenging robust keypoint detection task, which involves realistic brightness and contrast perturbations, demonstrating that our method achieves superior robustness certification performance and efficiency compared to baselines.",
        "arxiv_id": "2510.00083"
    },
    "2510.00419": {
        "SCORE": 15,
        "ARXIVID": "2510.00419",
        "COMMENT": "Model Compression & Efficiency: learning-based zeroth-order optimizer reduces memory for LLM fine-tuning; ML Systems: reusable L2L perturbation policy across tasks.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Kairun Zhang",
            "Haoyu Li",
            "Yanjun Zhao",
            "Yifan Sun",
            "Huan Zhang"
        ],
        "title": "Learning a Zeroth-Order Optimizer for Fine-Tuning LLMs",
        "abstract": "Zeroth-order optimizers have recently emerged as a practical approach for fine-tuning large language models (LLMs), significantly reducing GPU memory consumption compared to traditional first-order methods. Yet, existing zeroth-order methods rely on hand-crafted, static sampling strategies that are not adaptable to model-specific structures. To address this, we propose ZO Fine-tuner, a learning-based zeroth-order optimizer for LLMs that automatically learns efficient perturbation strategies through a compact and memory-efficient design. Crucially, our approach is motivated by the observation that only a small number of foundation models and their derivatives are widely adopted in practice. Therefore, learning the optimizer once for a given LLM and reusing it across diverse downstream tasks is both feasible and highly desirable. Accordingly, ZO Fine-tuner is designed to scale learning to learn (L2L) to the foundation-model era by supporting one-time training per LLM with minimal overhead. Experiments on 4 LLMs and 7 datasets show that ZO Fine-tuner outperforms prior zeroth-order baselines in 82.1\\% of task-model combinations, thereby demonstrating strong performance and scalability for efficient LLM fine-tuning. Our code is available at https://github.com/ASTRAL-Group/ZO_Fine_tuner.git.",
        "arxiv_id": "2510.00419"
    },
    "2510.00379": {
        "SCORE": 15,
        "ARXIVID": "2510.00379",
        "COMMENT": "Model Architecture: principled search over hybrid Attention/MLP interleavings with scaling strategies; also improves training/inference efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Bilge Acun",
            "Prasoon Sinha",
            "Newsha Ardalani",
            "Sangmin Bae",
            "Alicia Golden",
            "Chien-Yu Lin",
            "Meghana Madhyastha",
            "Fei Sun",
            "Neeraja J. Yadwadkar",
            "Carole-Jean Wu"
        ],
        "title": "Composer: A Search Framework for Hybrid Neural Architecture Design",
        "abstract": "Hybrid model architectures that combine computational primitives (e.g., Attention, MLP) in different ratios have shown promising performance beyond Transformers. Some studies have shown that different interleavings of primitives can affect model quality as well. However, prior works explore the hybrid model architecture design space manually. Due to the large design space and training costs, discovering hybrid models that combine key computational primitives for pre-training is challenging. In this work, we take a principled approach in designing a modular hybrid model architecture search framework -- Composer. Composer explores model architectures at a small scale and extrapolates the top-performing model architectures to a larger scale using our proposed scaling strategies. Using Composer, we discover new hybrid LLM architectures that outperform Llama 3.2. Compared to Llama 3.2 and previous state-of-the-art baselines, the new model architectures consistently reduce validation loss at parameter scales of 350M-3B and improve evaluation accuracy on the downstream tasks by up to 2.8-8.3% (1.1-3.1% on average) while improving both training and inference efficiency.",
        "arxiv_id": "2510.00379"
    },
    "2510.00133": {
        "SCORE": 15,
        "ARXIVID": "2510.00133",
        "COMMENT": "Architecture/Hardware-Efficient Inference: spike-based self-attention and SNN conversion for LLM inference with energy savings.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Adarsha Balaji",
            "Sandeep Madireddy"
        ],
        "title": "Large Language Models Inference Engines based on Spiking Neural Networks",
        "abstract": "Foundational models based on the transformer architecture are currently the state-of-the-art in general language modeling, as well as in scientific areas such as material science and climate. However, training and deploying these models is computationally challenging as the time and space complexity has a quadratic relation to the input sequence length. Several efforts exploring efficient computational paradigms and model architectures to address these limitations have been made. In this work, we explore spiking neural networks (SNNs) to design transformer models. A challenge in training large-scale SNNs, using existing surrogate learning methods is inefficient and time-consuming. On the other hand, techniques to convert existing transformer-based models to their SNN equivalent are not scalable, as achieving optimal performance comes at the cost of a large number of spike time-steps, i.e. increased latency. To address this, we propose NeurTransformer, a methodology for designing transformer-based SNN for inference using a supervised fine-tuning approach with existing conversion methods. The proposed methodology works by: (1) replacing the self-attention mechanism with a spike-based self-attention (SSA), (2) converting the feed-forward block of the trained transformer model to its equivalent SNN, and (3) fine-tuning the SSA block using SNN-based surrogate learning algorithms. We benchmark the proposed methodology and demonstrate its accuracy and scalability using three variants of the GPT-2 model of increasing model size. We observe that the converted GPT-2 small models demonstrate a 5-12% loss in cosine similarity and a 9.7% reduction in perplexity. Finally, we demonstrate the energy efficiency of the SSA block compared to the ASA block and show between 64.71% and 85.28% reductions in estimated energy consumption when implementing the self-attention mechanism on a digital hardware.",
        "arxiv_id": "2510.00133"
    },
    "2510.00027": {
        "SCORE": 15,
        "ARXIVID": "2510.00027",
        "COMMENT": "Model Architecture/Representation: learns SO(3)-equivariance in a non-equivariant Transformer via representation-space constraints (learned equivariance).",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Ahmed A. Elhag",
            "Arun Raja",
            "Alex Morehead",
            "Samuel M. Blau",
            "Garrett M. Morris",
            "Michael M. Bronstein"
        ],
        "title": "Learning Inter-Atomic Potentials without Explicit Equivariance",
        "abstract": "Accurate and scalable machine-learned inter-atomic potentials (MLIPs) are essential for molecular simulations ranging from drug discovery to new material design. Current state-of-the-art models enforce roto-translational symmetries through equivariant neural network architectures, a hard-wired inductive bias that can often lead to reduced flexibility, computational efficiency, and scalability. In this work, we introduce TransIP: Transformer-based Inter-Atomic Potentials, a novel training paradigm for interatomic potentials achieving symmetry compliance without explicit architectural constraints. Our approach guides a generic non-equivariant Transformer-based model to learn SO(3)-equivariance by optimizing its representations in the embedding space. Trained on the recent Open Molecules (OMol25) collection, a large and diverse molecular dataset built specifically for MLIPs and covering different types of molecules (including small organics, biomolecular fragments, and electrolyte-like species), TransIP attains comparable performance in machine-learning force fields versus state-of-the-art equivariant baselines. Further, compared to a data augmentation baseline, TransIP achieves 40% to 60% improvement in performance across varying OMol25 dataset sizes. More broadly, our work shows that learned equivariance can be a powerful and efficient alternative to equivariant or augmentation-based MLIP models.",
        "arxiv_id": "2510.00027"
    },
    "2510.00621": {
        "SCORE": 15,
        "ARXIVID": "2510.00621",
        "COMMENT": "Matches Model Architecture criterion: introduces continuous attention via neural CDEs with Mixture-of-Experts routing for function-on-function regression.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yifei Gao",
            "Yong Chen",
            "Chen Zhang"
        ],
        "title": "FAME: Adaptive Functional Attention with Expert Routing for Function-on-Function Regression",
        "abstract": "Functional data play a pivotal role across science and engineering, yet their infinite-dimensional nature makes representation learning challenging. Conventional statistical models depend on pre-chosen basis expansions or kernels, limiting the flexibility of data-driven discovery, while many deep-learning pipelines treat functions as fixed-grid vectors, ignoring inherent continuity. In this paper, we introduce Functional Attention with a Mixture-of-Experts (FAME), an end-to-end, fully data-driven framework for function-on-function regression. FAME forms continuous attention by coupling a bidirectional neural controlled differential equation with MoE-driven vector fields to capture intra-functional continuity, and further fuses change to inter-functional dependencies via multi-head cross attention. Extensive experiments on synthetic and real-world functional-regression benchmarks show that FAME achieves state-of-the-art accuracy, strong robustness to arbitrarily sampled discrete observations of functions.",
        "arxiv_id": "2510.00621"
    },
    "2510.01137": {
        "SCORE": 15,
        "ARXIVID": "2510.01137",
        "COMMENT": "Matches Compression/Efficiency criterion: low-rank gradient matrix denoising (random matrix theory) to improve DP-SGD sample efficiency without altering privacy.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Ali Dadsetan",
            "Frank Rudzicz"
        ],
        "title": "Sample-Efficient Differentially Private Fine-Tuning via Gradient Matrix Denoising",
        "abstract": "We address the challenge of sample efficiency in differentially private fine-tuning of large language models (LLMs) using DP-SGD. While DP-SGD provides strong privacy guarantees, the added noise significantly increases the entropy of gradient matrices, disrupting their low-rank structure and slowing optimization. We propose a post-processing algorithm that leverages random matrix theory to denoise gradients, restore low-rank structure, and improve alignment with the original signal. Applied to DP-SGD fine-tuning of RoBERTa on GLUE tasks, our method improves sample efficiency compared to state-of-the-art approaches, substantially reducing training time when optimal performance is not required. This work demonstrates that matrix recovery techniques can enhance the utility of private language model training without compromising privacy guarantees.",
        "arxiv_id": "2510.01137"
    },
    "2510.00494": {
        "SCORE": 15,
        "ARXIVID": "2510.00494",
        "COMMENT": "Matches Model Architecture criterion: analyzes dual-model (Base\u2013Coprocessor) latent communication vs unified embeddings and effects of channel capacity/joint finetuning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Julian Coda-Forno",
            "Zhuokai Zhao",
            "Qiang Zhang",
            "Dipesh Tamboli",
            "Weiwei Li",
            "Xiangjun Fan",
            "Lizhu Zhang",
            "Eric Schulz",
            "Hsiao-Ping Tseng"
        ],
        "title": "Exploring System 1 and 2 communication for latent reasoning in LLMs",
        "abstract": "Should LLM reasoning live in a separate module, or within a single model's forward pass and representational space? We study dual-architecture latent reasoning, where a fluent Base exchanges latent messages with a Coprocessor, and test two hypotheses aimed at improving latent communication over Liu et al. (2024): (H1) increase channel capacity; (H2) learn communication via joint finetuning. Under matched latent-token budgets on GPT-2 and Qwen-3, H2 is consistently strongest while H1 yields modest gains. A unified soft-embedding baseline, a single model with the same forward pass and shared representations, using the same latent-token budget, nearly matches H2 and surpasses H1, suggesting current dual designs mostly add compute rather than qualitatively improving reasoning. Across GSM8K, ProsQA, and a Countdown stress test with increasing branching factor, scaling the latent-token budget beyond small values fails to improve robustness. Latent analyses show overlapping subspaces with limited specialization, consistent with weak reasoning gains. We conclude dual-model latent reasoning remains promising in principle, but likely requires objectives and communication mechanisms that explicitly shape latent spaces for algorithmic planning.",
        "arxiv_id": "2510.00494"
    },
    "2510.01105": {
        "SCORE": 15,
        "ARXIVID": "2510.01105",
        "COMMENT": "Matches Representation Learning criterion: geometric/intrinsic-dimension analysis of regression representations and collapse regimes with practical insights.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "George Andriopoulos",
            "Zixuan Dong",
            "Bimarsha Adhikari",
            "Keith Ross"
        ],
        "title": "Geometric Properties of Neural Multivariate Regression",
        "abstract": "Neural multivariate regression underpins a wide range of domains such as control, robotics, and finance, yet the geometry of its learned representations remains poorly characterized. While neural collapse has been shown to benefit generalization in classification, we find that analogous collapse in regression consistently degrades performance. To explain this contrast, we analyze models through the lens of intrinsic dimension. Across control tasks and synthetic datasets, we estimate the intrinsic dimension of last-layer features (ID_H) and compare it with that of the regression targets (ID_Y). Collapsed models exhibit ID_H  ID_Y. For the non-collapsed models, performance with respect to ID_H depends on the data quantity and noise levels. From these observations, we identify two regimes (over-compressed and under-compressed) that determine when expanding or reducing feature dimensionality improves performance. Our results provide new geometric insights into neural regression and suggest practical strategies for enhancing generalization.",
        "arxiv_id": "2510.01105"
    },
    "2510.01032": {
        "SCORE": 15,
        "ARXIVID": "2510.01032",
        "COMMENT": "Matches Representation Learning criterion: provides a mechanistic analysis of MLP activation distributions and introduces an inference-time activation redistribution module that alters internal activations without changing inputs.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Zeru Shi",
            "Yingjia Wan",
            "Zhenting Wang",
            "Qifan Wang",
            "Fan Yang",
            "Elisa Kreiss",
            "Ruixiang Tang"
        ],
        "title": "Meaningless Tokens, Meaningful Gains: How Activation Shifts Enhance LLM Reasoning",
        "abstract": "Motivated by the puzzling observation that inserting long sequences of meaningless tokens before the query prompt can consistently enhance LLM reasoning performance, this work analyzes the underlying mechanism driving this phenomenon and based on these insights proposes a more principled method that allows for similar performance gains. First, we find that the improvements arise from a redistribution of activations in the LLM's MLP layers, where near zero activations become less frequent while large magnitude activations increase. This redistribution enhances the model's representational capacity by suppressing weak signals and promoting stronger, more informative ones. Building on this insight, we propose the Activation Redistribution Module (ARM), a lightweight inference-time technique that modifies activations directly without altering the input sequence. ARM adaptively identifies near-zero activations after the non-linear function and shifts them outward, implicitly reproducing the beneficial effects of meaningless tokens in a controlled manner. Extensive experiments across diverse benchmarks and model architectures clearly show that ARM consistently improves LLM performance on reasoning tasks while requiring only a few lines of simple code to implement. Our findings deliver both a clear mechanistic explanation for the unexpected benefits of meaningless tokens and a simple yet effective technique that harnesses activation redistribution to further improve LLM performance.",
        "arxiv_id": "2510.01032"
    },
    "2510.00365": {
        "SCORE": 15,
        "ARXIVID": "2510.00365",
        "COMMENT": "Matches Model Architecture criterion: introduces a query-only attention variant (discarding keys/values) with analysis of training dynamics/plasticity for continual learning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Gautham Bekal",
            "Ashish Pujari",
            "Scott David Kelly"
        ],
        "title": "Continual Learning with Query-Only Attention",
        "abstract": "Continual learning involves learning from a stream of data without repetition of data points, a scenario that is inherently complex due to distributional shift across tasks. We propose a query-only attention mechanism that discards keys and values, yet preserves the core inductive bias of transformer architectures. In continual learning scenarios, this simplified mechanism significantly mitigates both loss of plasticity and catastrophic forgetting, outperforming baselines such as selective re-initialization. We establish a conceptual link between query-only attention, full transformer attention, and model agnostic meta-learning, framing them as instances of meta-learning. We further provide intuition for why query-based models and attention networks help preserve plasticity in continual settings. Finally, through preliminary Hessian spectrum analysis, we observe that models maintaining higher curvature rank across tasks tend to retain plasticity. Our findings suggest that full attention may not be essential for capturing the benefits of meta-learning in continual learning.",
        "arxiv_id": "2510.00365"
    },
    "2510.00194": {
        "SCORE": 15,
        "ARXIVID": "2510.00194",
        "COMMENT": "Matches Training/Architecture criterion: critic-free \u03bb-return with eligibility traces for token-level credit assignment in RL finetuning of LLMs, improving reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Prasanna Parthasarathi",
            "Mathieu Reymond",
            "Boxing Chen",
            "Yufei Cui",
            "Sarath Chandar"
        ],
        "title": "GRPO-$\\lambda$: Credit Assignment improves LLM Reasoning",
        "abstract": "Large language models (LLMs) are increasingly deployed for tasks requiring complex reasoning, prompting significant interest in improving their reasoning abilities through post-training. Especially RL based methods using verifiable reward, like the state-of-the-art GRPO, have shown to tremendously improve reasoning behaviors when applied as post-training methods. However, the lack of an explicit reward or critic model limits GRPO's ability to assign fine-grained credit across token sequences. In this work, we present GRPO-$\\lambda$, a novel extension to GRPO that enhances credit assignment in RL finetuning of LLMs for complex reasoning tasks. We approximate learning from $\\lambda$-return with a reformulation of eligibility traces using token-level log-probabilities applied after each sequence generation, and a novel critic-free approximation of the temporal-difference error. We introduce a few variations for the weighting of the $\\lambda$-return, and their applications to the eligibility-trace, where all the variations provide significant gains over GRPO. We compare GRPO-$\\lambda$ against GRPO by training models from 1.5B to 7B parameters on $4$ different math reasoning datasets. The training plots demonstrate 30-40% improved performance during RL training on both LLaMA-3.1 and Qwen-2.5 architectures. Finally, we show that with GRPO-$\\lambda$, the resulting average performance on AIME24, Math500, OlympiadMath, MinervaMath, and AMC improves over GRPO by over $3$ points and a $4.5$ points improvement on the 7B model.",
        "arxiv_id": "2510.00194"
    },
    "2510.01030": {
        "SCORE": 15,
        "ARXIVID": "2510.01030",
        "COMMENT": "Representation Learning: large-scale analysis identifies architectural/training ingredients (instruction tuning, attention-head dimensionality) that align LLM representations with human concepts.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Zach Studdiford",
            "Timothy T. Rogers",
            "Kushin Mukherjee",
            "Siddharth Suresh"
        ],
        "title": "Uncovering the Computational Ingredients of Human-Like Representations in LLMs",
        "abstract": "The ability to translate diverse patterns of inputs into structured patterns of behavior has been thought to rest on both humans' and machines' ability to learn robust representations of relevant concepts. The rapid advancement of transformer-based large language models (LLMs) has led to a diversity of computational ingredients -- architectures, fine tuning methods, and training datasets among others -- but it remains unclear which of these ingredients are most crucial for building models that develop human-like representations. Further, most current LLM benchmarks are not suited to measuring representational alignment between humans and models, making benchmark scores unreliable for assessing if current LLMs are making progress towards becoming useful cognitive models. We address these limitations by first evaluating a set of over 70 models that widely vary in their computational ingredients on a triplet similarity task, a method well established in the cognitive sciences for measuring human conceptual representations, using concepts from the THINGS database. Comparing human and model representations, we find that models that undergo instruction-finetuning and which have larger dimensionality of attention heads are among the most human aligned, while multimodal pretraining and parameter size have limited bearing on alignment. Correlations between alignment scores and scores on existing benchmarks reveal that while some benchmarks (e.g., MMLU) are better suited than others (e.g., MUSR) for capturing representational alignment, no existing benchmark is capable of fully accounting for the variance of alignment scores, demonstrating their insufficiency in capturing human-AI alignment. Taken together, our findings help highlight the computational ingredients most essential for advancing LLMs towards models of human conceptual representation and address a key benchmarking gap in LLM evaluation.",
        "arxiv_id": "2510.01030"
    }
}