{
    "2510.02142": {
        "COMMENT": "Author match",
        "SCORE": 20.0,
        "authors": [
            "Lena Podina",
            "Christina Humer",
            "Alexandre Duval",
            "Victor Schmidt",
            "Ali Ramlaoui",
            "Shahana Chatterjee",
            "Yoshua Bengio",
            "Alex Hernandez-Garcia",
            "David Rolnick",
            "F\\'elix Therrien"
        ],
        "title": "Catalyst GFlowNet for electrocatalyst design: A hydrogen evolution reaction case study",
        "abstract": "Efficient and inexpensive energy storage is essential for accelerating the adoption of renewable energy and ensuring a stable supply, despite fluctuations in sources such as wind and solar. Electrocatalysts play a key role in hydrogen energy storage (HES), allowing the energy to be stored as hydrogen. However, the development of affordable and high-performance catalysts for this process remains a significant challenge. We introduce Catalyst GFlowNet, a generative model that leverages machine learning-based predictors of formation and adsorption energy to design crystal surfaces that act as efficient catalysts. We demonstrate the performance of the model through a proof-of-concept application to the hydrogen evolution reaction, a key reaction in HES, for which we successfully identified platinum as the most efficient known catalyst. In future work, we aim to extend this approach to the oxygen evolution reaction, where current optimal catalysts are expensive metal oxides, and open the search space to discover new materials. This generative modeling framework offers a promising pathway for accelerating the search for novel and efficient catalysts.",
        "arxiv_id": "2510.02142"
    },
    "2510.01643": {
        "SCORE": 19,
        "ARXIVID": "2510.01643",
        "COMMENT": "Model Compression and Efficiency: support-basis decomposition yields sub-quadratic softmax attention beyond bounded-entry assumptions with rigorous guarantees; provides theory linking to polynomial attention.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Maryam Aliakbarpour",
            "Vladimir Braverman",
            "Junze Yin",
            "Haochen Zhang"
        ],
        "title": "Support Basis: Fast Attention Beyond Bounded Entries",
        "abstract": "The quadratic complexity of softmax attention remains a central bottleneck in scaling large language models (LLMs). [Alman and Song, NeurIPS 2023] proposed a sub-quadratic attention approximation algorithm, but it works only under the restrictive bounded-entry assumption. Since this assumption rarely holds in practice, its applicability to modern LLMs is limited.   In this paper, we introduce support-basis decomposition, a new framework for efficient attention approximation beyond bounded entries. We empirically demonstrate that the entries of the query and key matrices exhibit sub-Gaussian behavior. Our approach uses this property to split large and small entries, enabling exact computation on sparse components and polynomial approximation on dense components. We establish rigorous theoretical guarantees, proving a sub-quadratic runtime, and extend the method to a multi-threshold setting that eliminates all distributional assumptions. Furthermore, we provide the first theoretical justification for the empirical success of polynomial attention [Kacham, Mirrokni, and Zhong, ICML 2024], showing that softmax attention can be closely approximated by a combination of multiple polynomial attentions with sketching.",
        "arxiv_id": "2510.01643"
    },
    "2510.01650": {
        "SCORE": 19,
        "ARXIVID": "2510.01650",
        "COMMENT": "Model Compression and Efficiency: surrogate-free ADMM pruning achieving up to 90% sparsity on LLMs with strong fidelity and convergence guarantees; quantized variant scales to large models.",
        "RELEVANCE": 10,
        "NOVELTY": 9,
        "authors": [
            "Kwanhee Lee",
            "Hyeondo Jang",
            "Dongyeop Lee",
            "Dan Alistarh",
            "Namhoon Lee"
        ],
        "title": "The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free ADMM",
        "abstract": "Neural network pruning is a promising technique to mitigate the excessive computational and memory requirements of large language models (LLMs). Despite its promise, however, progress in this area has diminished, as conventional methods are seemingly unable to surpass moderate sparsity levels (50-60%) without severely degrading model accuracy. This work breaks through the current impasse, presenting a principled and effective method called $\\texttt{Elsa}$, which achieves extreme sparsity levels of up to 90% while retaining high model fidelity. This is done by identifying several limitations in current practice, all of which can be traced back to their reliance on a surrogate objective formulation. $\\texttt{Elsa}$ tackles this issue directly and effectively via standard and well-established constrained optimization techniques based on ADMM. Our extensive experiments across a wide range of models and scales show that $\\texttt{Elsa}$ achieves substantial improvements over existing methods; e.g., it achieves 7.8$\\times$ less perplexity than the best existing method on LLaMA-2-7B at 90% sparsity. Furthermore, we present $\\texttt{Elsa}_{\\text{-L}}$, a quantized variant that scales to extremely large models (27B), and establish its theoretical convergence guarantees. These results highlight meaningful progress in advancing the frontier of LLM sparsity, while promising that significant opportunities for further advancement may remain in directions that have so far attracted limited exploration.",
        "arxiv_id": "2510.01650"
    },
    "2510.01450": {
        "SCORE": 18,
        "ARXIVID": "2510.01450",
        "COMMENT": "Model Architecture + ML Systems: introduces Local Linear Attention with bias\u2013variance analysis and hardware-efficient FlashLLA kernels for scalable computation.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Yifei Zuo",
            "Yutong Yin",
            "Zhichen Zeng",
            "Ang Li",
            "Banghua Zhu",
            "Zhaoran Wang"
        ],
        "title": "Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression",
        "abstract": "Transformer architectures have achieved remarkable success in various domains. While efficient alternatives to Softmax Attention have been widely studied, the search for more expressive mechanisms grounded in theoretical insight-even at greater computational cost-has been relatively underexplored. In this work, we bridge this gap by proposing Local Linear Attention (LLA), a novel attention mechanism derived from nonparametric statistics through the lens of test-time regression. First, we show that LLA offers theoretical advantages over Linear and Softmax Attention for associative memory via a bias-variance trade-off analysis. Next, we address its computational challenges and propose two memory-efficient primitives to tackle the $\\Theta(n^2 d)$ and $\\Theta(n d^2)$ complexity. We then introduce FlashLLA, a hardware-efficient, blockwise algorithm that enables scalable and parallel computation on modern accelerators. In addition, we implement and profile a customized inference kernel that significantly reduces memory overheads. Finally, we empirically validate the advantages and limitations of LLA on test-time regression, in-context regression, associative recall and state tracking tasks. Experiment results demonstrate that LLA effectively adapts to non-stationarity, outperforming strong baselines in test-time training and in-context learning, and exhibiting promising evidence for its scalability and applicability in large-scale models. Code is available at https://github.com/Yifei-Zuo/Flash-LLA.",
        "arxiv_id": "2510.01450"
    },
    "2510.01240": {
        "SCORE": 18,
        "ARXIVID": "2510.01240",
        "COMMENT": "Compression/Efficiency: Riemannian Sensitivity-Aware Vector Quantization using FIM-based error-direction guidance and channel-wise sensitivity for dynamic bit allocation in LLMs.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Zukang Xu",
            "Xing Hu",
            "Qiang Wu",
            "Dawei Yang"
        ],
        "title": "RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models",
        "abstract": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, their exponentially increasing parameters pose significant challenges for deployment on resource-constrained devices. Vector Quantization (VQ) shows great promise for low-bit quantization (e.g., 2 to 4 bits), but existing work faces two key challenges: unconstrained direction error and suboptimal bit allocation. In this paper, we propose RSAVQ, a novel VQ framework to enhance extremely low-bit quantization for LLMs. RSAVQ introduces two geometry-driven innovations that effectively mitigate above limitations: (1) Error Direction Sensitivity Guidance (EDSG), which leverages the Fisher Information Matrix (FIM)-induced Riemannian metric to project quantization errors onto low-sensitivity directions in the parameter space. Specifically, this projection is performed along the negative natural gradient direction, which effectively suppresses error expansion. (2) Weight Channel Sensitivity Guidance (WCSG) , which constructs a channel-wise sensitivity metric via FIM curvature analysis to dynamically guide bit resource allocation. The approach facilitates a globally optimal quantization solution within prescribed bit constraints. Experiments demonstrate that RSAVQ outperforms existing methods for LLMs. For example, in 2-bit quantization of LLaMA-3 8B, RSAVQ leads baselines like VPTQ and QuIP# by 0.4 in perplexity (PPL) and 1.5 in zero-shot accuracy. This work offers a practical solution for constrained environments and a theoretical bridge between information geometry and the quantization of neural networks, advancing efficient deep learning.",
        "arxiv_id": "2510.01240"
    },
    "2510.01290": {
        "SCORE": 18,
        "ARXIVID": "2510.01290",
        "COMMENT": "Model Compression and Efficiency + ML Systems: thought-adaptive KV cache compression with hybrid quantization/eviction and a PagedAttention-based kernel for memory reuse.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Akshat Ramachandran",
            "Marina Neseem",
            "Charbel Sakr",
            "Rangharajan Venkatesan",
            "Brucek Khailany",
            "Tushar Krishna"
        ],
        "title": "ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models",
        "abstract": "The long-output context generation of large reasoning models enables extended chain of thought (CoT) but also drives rapid growth of the key-value (KV) cache, quickly overwhelming GPU memory. To address this challenge, we propose ThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on the observation that attention sparsity reveals distinct thought types with varying importance within the CoT. It applies a hybrid quantization-eviction strategy, assigning token precision by thought importance and progressively evicting tokens from less critical thoughts as reasoning trajectories evolve. Furthermore, to implement ThinKV, we design a kernel that extends PagedAttention to enable efficient reuse of evicted tokens' memory slots, eliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill, GPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show that ThinKV achieves near-lossless accuracy with less than 5% of the original KV cache, while improving performance with up to 5.8x higher inference throughput over state-of-the-art baselines.",
        "arxiv_id": "2510.01290"
    },
    "2510.01263": {
        "SCORE": 18,
        "ARXIVID": "2510.01263",
        "COMMENT": "Model Compression and Efficiency: introduces an activity-dependent pruning rule with constrained-entropy analysis; local fan-in/fan-out pruning enforces structured sparsity.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Yaron Meirovitch",
            "Fuming Yang",
            "Jeff Lichtman",
            "Nir Shavit"
        ],
        "title": "Budgeted Broadcast: An Activity-Dependent Pruning Rule for Neural Network Efficiency",
        "abstract": "Most pruning methods remove parameters ranked by impact on loss (e.g., magnitude or gradient). We propose Budgeted Broadcast (BB), which gives each unit a local traffic budget (the product of its long-term on-rate $a_i$ and fan-out $k_i$). A constrained-entropy analysis shows that maximizing coding entropy under a global traffic budget yields a selectivity-audience balance, $\\log\\frac{1-a_i}{a_i}=\\beta k_i$. BB enforces this balance with simple local actuators that prune either fan-in (to lower activity) or fan-out (to reduce broadcast). In practice, BB increases coding entropy and decorrelation and improves accuracy at matched sparsity across Transformers for ASR, ResNets for face identification, and 3D U-Nets for synapse prediction, sometimes exceeding dense baselines. On electron microscopy images, it attains state-of-the-art F1 and PR-AUC under our evaluation protocol. BB is easy to integrate and suggests a path toward learning more diverse and efficient representations.",
        "arxiv_id": "2510.01263"
    },
    "2510.01336": {
        "SCORE": 18,
        "ARXIVID": "2510.01336",
        "COMMENT": "ML Systems/Efficiency: hierarchical speculative decoding with early-exit models and KV-cache/hidden state reuse for LLM inference acceleration.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Avinash Kumar",
            "Sujay Sanghavi",
            "Poulami Das"
        ],
        "title": "HiSpec: Hierarchical Speculative Decoding for LLMs",
        "abstract": "Speculative decoding accelerates LLM inference by using a smaller draft model to speculate tokens that a larger target model verifies. Verification is often the bottleneck (e.g. verification is $4\\times$ slower than token generation when a 3B model speculates for a 70B target model), but most prior works focus only on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces verification time by discarding inaccurate draft tokens early, but existing methods incur substantial training overheads in incorporating the intermediate verifier, increase the memory footprint to orchestrate the intermediate verification step, and compromise accuracy by relying on approximate heuristics.   We propose $\\underline{\\textit{Hi}}\\textit{erarchical }\\underline{\\textit{Spec}}\\textit{ulative Decoding (HiSpec)}$, a framework for high-throughput speculative decoding that exploits $\\textit{early-exit (EE) models}$ for low-overhead intermediate verification. EE models allow tokens to exit early by skipping layer traversal and are explicitly trained so that hidden states at selected layers can be interpreted, making them uniquely suited for intermediate verification without drastically increasing compute and memory overheads. To improve resource-efficiency even further, we design a methodology that enables HiSpec to re-use key-value caches and hidden states between the draft, intermediate verifier, and target models. To maintain accuracy, HiSpec periodically validates the draft tokens accepted by the intermediate verifier against the target model. Our evaluations using various representative benchmarks and models show that HiSpec improves throughput by 1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline single-layer speculation without compromising accuracy.",
        "arxiv_id": "2510.01336"
    },
    "2510.01303": {
        "SCORE": 18,
        "ARXIVID": "2510.01303",
        "COMMENT": "Compression/Efficiency and Training Dynamics: theoretical analysis showing gradients are approximately low-rank, with effects of regularizers\u2014core low-rank insight.",
        "RELEVANCE": 10,
        "NOVELTY": 8,
        "authors": [
            "Rishi Sonthalia",
            "Michael Murray",
            "Guido Mont\\'ufar"
        ],
        "title": "Low Rank Gradients and Where to Find Them",
        "abstract": "This paper investigates low-rank structure in the gradients of the training loss for two-layer neural networks while relaxing the usual isotropy assumptions on the training data and parameters. We consider a spiked data model in which the bulk can be anisotropic and ill-conditioned, we do not require independent data and weight matrices and we also analyze both the mean-field and neural-tangent-kernel scalings. We show that the gradient with respect to the input weights is approximately low rank and is dominated by two rank-one terms: one aligned with the bulk data-residue , and another aligned with the rank one spike in the input data. We characterize how properties of the training data, the scaling regime and the activation function govern the balance between these two components. Additionally, we also demonstrate that standard regularizers, such as weight decay, input noise and Jacobian penalties, also selectively modulate these components. Experiments on synthetic and real data corroborate our theoretical predictions.",
        "arxiv_id": "2510.01303"
    },
    "2510.01345": {
        "SCORE": 18,
        "ARXIVID": "2510.01345",
        "COMMENT": "Representation Learning theory: unifies SSRL as mutual information maximization, explaining predictor, stop-gradient, and regularizers from first principles.",
        "RELEVANCE": 9,
        "NOVELTY": 9,
        "authors": [
            "Akhlaqur Rahman Sabby",
            "Yi Sui",
            "Tongzi Wu",
            "Jesse C. Cresswell",
            "Ga Wu"
        ],
        "title": "Self-Supervised Representation Learning as Mutual Information Maximization",
        "abstract": "Self-supervised representation learning (SSRL) has demonstrated remarkable empirical success, yet its underlying principles remain insufficiently understood. While recent works attempt to unify SSRL methods by examining their information-theoretic objectives or summarizing their heuristics for preventing representation collapse, architectural elements like the predictor network, stop-gradient operation, and statistical regularizer are often viewed as empirically motivated additions. In this paper, we adopt a first-principles approach and investigate whether the learning objective of an SSRL algorithm dictates its possible optimization strategies and model design choices. In particular, by starting from a variational mutual information (MI) lower bound, we derive two training paradigms, namely Self-Distillation MI (SDMI) and Joint MI (JMI), each imposing distinct structural constraints and covering a set of existing SSRL algorithms. SDMI inherently requires alternating optimization, making stop-gradient operations theoretically essential. In contrast, JMI admits joint optimization through symmetric architectures without such components. Under the proposed formulation, predictor networks in SDMI and statistical regularizers in JMI emerge as tractable surrogates for the MI objective. We show that many existing SSRL methods are specific instances or approximations of these two paradigms. This paper provides a theoretical explanation behind the choices of different architectural components of existing SSRL methods, beyond heuristic conveniences.",
        "arxiv_id": "2510.01345"
    },
    "2510.01938": {
        "SCORE": 17,
        "ARXIVID": "2510.01938",
        "COMMENT": "Model Compression and Efficiency: geometry-aware LoRA with U S V^T and Riemannian optimization on the Stiefel manifold for improved PEFT.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Zhizhong Li",
            "Sina Sajadmanesh",
            "Jingtao Li",
            "Lingjuan Lyu"
        ],
        "title": "StelLA: Subspace Learning in Low-rank Adaptation using Stiefel Manifold",
        "abstract": "Low-rank adaptation (LoRA) has been widely adopted as a parameter-efficient technique for fine-tuning large-scale pre-trained models. However, it still lags behind full fine-tuning in performance, partly due to its insufficient exploitation of the geometric structure underlying low-rank manifolds. In this paper, we propose a geometry-aware extension of LoRA that uses a three-factor decomposition $U\\!SV^\\top$. Analogous to the structure of singular value decomposition (SVD), it separates the adapter's input and output subspaces, $V$ and $U$, from the scaling factor $S$. Our method constrains $U$ and $V$ to lie on the Stiefel manifold, ensuring their orthonormality throughout the training. To optimize on the Stiefel manifold, we employ a flexible and modular geometric optimization design that converts any Euclidean optimizer to a Riemannian one. It enables efficient subspace learning while remaining compatible with existing fine-tuning pipelines. Empirical results across a wide range of downstream tasks, including commonsense reasoning, math and code generation, image classification, and image generation, demonstrate the superior performance of our approach against the recent state-of-the-art variants of LoRA. Code is available at https://github.com/SonyResearch/stella.",
        "arxiv_id": "2510.01938"
    },
    "2510.01634": {
        "SCORE": 17,
        "ARXIVID": "2510.01634",
        "COMMENT": "Model Architecture: curvature-adaptive Transformer with differentiable gating routing across geometries (mixture-of-geometry design).",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Ryan Y. Lin",
            "Siddhartha Ojha",
            "Nicholas Bai"
        ],
        "title": "CAT: Curvature-Adaptive Transformers for Geometry-Aware Learning",
        "abstract": "Transformers achieve strong performance across diverse domains but implicitly assume Euclidean geometry in their attention mechanisms, limiting their effectiveness on data with non-Euclidean structure. While recent extensions to hyperbolic and spherical spaces show promise for hierarchical and cyclical patterns, respectively, they require committing to a single geometry a priori, reducing flexibility when data exhibits mixed geometric properties. We introduce the Curvature-Adaptive Transformer (CAT), a novel architecture that dynamically learns per-token routing across three geometric attention branches through a lightweight, differentiable gating mechanism. Unlike fixed-geometry approaches, CAT enables adaptive geometric specialization, routing tokens to the appropriate curvature based on their local relational structure. The routing network provides interpretable curvature preferences while each branch employs geometry-specific operations optimized for its respective manifold. On knowledge graph completion benchmarks (FB15k-237, WN18RR), CAT achieves approximately 10% improvements in MRR and Hits@10 over fixed-geometry baselines with minimal overhead (5% parameter increase, comparable inference time). These results demonstrate that learned geometric adaptation outperforms any single fixed geometry for complex relational reasoning, establishing CAT as a scalable and interpretable foundation for mixture-of-geometry architectures across language, vision, and multimodal domains.",
        "arxiv_id": "2510.01634"
    },
    "2510.01565": {
        "SCORE": 17,
        "ARXIVID": "2510.01565",
        "COMMENT": "ML Systems: introduces step-level sequence parallelism and round-based deadline-aware scheduling for serving DiT models with improved SLOs.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Runyu Lu",
            "Shiqi He",
            "Wenxuan Tan",
            "Shenggui Li",
            "Ruofan Wu",
            "Jeff J. Ma",
            "Ang Chen",
            "Mosharaf Chowdhury"
        ],
        "title": "TetriServe: Efficient DiT Serving for Heterogeneous Image Generation",
        "abstract": "Diffusion Transformer (DiT) models excel at generating highquality images through iterative denoising steps, but serving them under strict Service Level Objectives (SLOs) is challenging due to their high computational cost, particularly at large resolutions. Existing serving systems use fixed degree sequence parallelism, which is inefficient for heterogeneous workloads with mixed resolutions and deadlines, leading to poor GPU utilization and low SLO attainment.   In this paper, we propose step-level sequence parallelism to dynamically adjust the parallel degree of individual requests according to their deadlines. We present TetriServe, a DiT serving system that implements this strategy for highly efficient image generation. Specifically, TetriServe introduces a novel round-based scheduling mechanism that improves SLO attainment: (1) discretizing time into fixed rounds to make deadline-aware scheduling tractable, (2) adapting parallelism at the step level and minimize GPU hour consumption, and (3) jointly packing requests to minimize late completions. Extensive evaluation on state-of-the-art DiT models shows that TetriServe achieves up to 32% higher SLO attainment compared to existing solutions without degrading image quality.",
        "arxiv_id": "2510.01565"
    },
    "2510.01546": {
        "SCORE": 17,
        "ARXIVID": "2510.01546",
        "COMMENT": "Model Architecture: Mixture-of-Transformers to endow pretrained MLLMs with generation in a unified autoregressive framework; semantic-to-pixel discrete representation.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Hanyu Wang",
            "Jiaming Han",
            "Ziyan Yang",
            "Qi Zhao",
            "Shanchuan Lin",
            "Xiangyu Yue",
            "Abhinav Shrivastava",
            "Zhenheng Yang",
            "Hao Chen"
        ],
        "title": "Growing Visual Generative Capacity for Pre-Trained MLLMs",
        "abstract": "Multimodal large language models (MLLMs) extend the success of language models to visual understanding, and recent efforts have sought to build unified MLLMs that support both understanding and generation. However, constructing such models remains challenging: hybrid approaches combine continuous embeddings with diffusion or flow-based objectives, producing high-quality images but breaking the autoregressive paradigm, while pure autoregressive approaches unify text and image prediction over discrete visual tokens but often face trade-offs between semantic alignment and pixel-level fidelity. In this work, we present Bridge, a pure autoregressive unified MLLM that augments pre-trained visual understanding models with generative ability through a Mixture-of-Transformers architecture, enabling both image understanding and generation within a single next-token prediction framework. To further improve visual generation fidelity, we propose a semantic-to-pixel discrete representation that integrates compact semantic tokens with fine-grained pixel tokens, achieving strong language alignment and precise description of visual details with only a 7.9% increase in sequence length. Extensive experiments across diverse multimodal benchmarks demonstrate that Bridge achieves competitive or superior results in both understanding and generation benchmarks, while requiring less training data and reduced training time compared to prior unified MLLMs.",
        "arxiv_id": "2510.01546"
    },
    "2510.02312": {
        "SCORE": 17,
        "ARXIVID": "2510.02312",
        "COMMENT": "Compression/Efficiency: distilled supervision from compressed KV-cache; Representation Learning: latent reasoning via continuous latent tokens aligned to KV trajectories.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Anna Kuzina",
            "Maciej Pioro",
            "Paul N. Whatmough",
            "Babak Ehteshami Bejnordi"
        ],
        "title": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation",
        "abstract": "Large Language Models (LLMs) excel at multi-step reasoning problems with explicit chain-of-thought (CoT), but verbose traces incur significant computational costs and memory overhead, and often carry redundant, stylistic artifacts. Latent reasoning has emerged as an efficient alternative that internalizes the thought process, but it suffers from a critical lack of supervision, limiting its effectiveness on complex, natural-language reasoning traces. In this work, we propose KaVa, the first framework that bridges this gap by distilling knowledge directly from a compressed KV-cache of the teacher into a latent-reasoning student via self-distillation, leveraging the representational flexibility of continuous latent tokens to align stepwise KV trajectories. We show that the abstract, unstructured knowledge within compressed KV-cache, which lacks direct token correspondence, can serve as a rich supervisory signal for a latent reasoning student. Empirically, the approach consistently outperforms strong latent baselines, exhibits markedly smaller degradation from equation-only to natural-language traces, and scales to larger backbones while preserving efficiency. These results establish compressed KV-cache distillation as a scalable supervision signal for latent reasoning, combining the accuracy of CoT-trained teachers with the efficiency and deployability of latent inference.",
        "arxiv_id": "2510.02312"
    },
    "2510.02049": {
        "SCORE": 17,
        "ARXIVID": "2510.02049",
        "COMMENT": "Model Architecture theory: mathematically models densely connected networks as nonlinear integral equations and proves convergence via \u0393-convergence, illuminating training dynamics.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Jinshu Huang",
            "Haibin Su",
            "Xue-Cheng Tai",
            "Chunlin Wu"
        ],
        "title": "Mathematical Modeling and Convergence Analysis of Deep Neural Networks with Dense Layer Connectivities in Deep Learning",
        "abstract": "In deep learning, dense layer connectivity has become a key design principle in deep neural networks (DNNs), enabling efficient information flow and strong performance across a range of applications. In this work, we model densely connected DNNs mathematically and analyze their learning problems in the deep-layer limit. For a broad applicability, we present our analysis in a framework setting of DNNs with densely connected layers and general non-local feature transformations (with local feature transformations as special cases) within layers, which is called dense non-local (DNL) framework and includes standard DenseNets and variants as special examples. In this formulation, the densely connected networks are modeled as nonlinear integral equations, in contrast to the ordinary differential equation viewpoint commonly adopted in prior works. We study the associated training problems from an optimal control perspective and prove convergence results from the network learning problem to its continuous-time counterpart. In particular, we show the convergence of optimal values and the subsequence convergence of minimizers, using a piecewise linear extension and $\\Gamma$-convergence analysis. Our results provide a mathematical foundation for understanding densely connected DNNs and further suggest that such architectures can offer stability of training deep models.",
        "arxiv_id": "2510.02049"
    },
    "2510.01878": {
        "SCORE": 17,
        "ARXIVID": "2510.01878",
        "COMMENT": "Matches Model Compression/Efficiency and ML Systems: randomized low-dimensional gradient subspace methods (GrassWalk/GrassJump) to reduce optimizer memory in LLM training with analysis of gradient geometry.",
        "RELEVANCE": 9,
        "NOVELTY": 8,
        "authors": [
            "Sahar Rajabi",
            "Nayeema Nonta",
            "Samanvay Vajpayee",
            "Sirisha Rambhatla"
        ],
        "title": "Randomized Gradient Subspaces for Efficient Large Language Model Training",
        "abstract": "Training large language models (LLMs) is often bottlenecked by extreme memory demands, with optimizer states dominating the footprint. Recent works mitigates this cost by projecting gradients into low-dimensional subspaces using sophisticated update strategies. In this paper, we analyze the dynamics of gradient space and its underlying subspaces. We find that while a small subspace captures most gradient energy, a significant portion still resides in the residual bulk; moreover, the influence of the core subspace diminishes over time and in deeper layers. We also observe that the gradient space exhibits near-flat curvature, calling for algorithms that explicitly account for this geometry. Motivated by these insights, we introduce a suite of randomized algorithms, GrassWalk and GrassJump, which exploit subspace and achieve state-of-the-art memory savings while improving performance on LLaMA-1B and LLaMA-7B pretraining.",
        "arxiv_id": "2510.01878"
    },
    "2510.01621": {
        "SCORE": 16,
        "ARXIVID": "2510.01621",
        "COMMENT": "Representation Learning/Autoencoders: frames VAE posterior collapse as a phase transition with a critical boundary, offering theoretical insight into trainability and capacity.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Zhen Li",
            "Fan Zhang",
            "Zheng Zhang",
            "Yu Chen"
        ],
        "title": "Posterior Collapse as a Phase Transition in Variational Autoencoders",
        "abstract": "We investigate the phenomenon of posterior collapse in variational autoencoders (VAEs) from the perspective of statistical physics, and reveal that it constitutes a phase transition governed jointly by data structure and model hyper-parameters. By analyzing the stability of the trivial solution associated with posterior collapse, we identify a critical hyper-parameter threshold. This critical boundary, separating meaningful latent inference from collapse, is characterized by a discontinuity in the KL divergence between the approximate posterior and the prior distribution. We validate this critical behavior on both synthetic and real-world datasets, confirming the existence of a phase transition. Our results demonstrate that posterior collapse is not merely an optimization failure, but rather an emerging phase transition arising from the interplay between data structure and variational constraints. This perspective offers new insights into the trainability and representational capacity of deep generative models.",
        "arxiv_id": "2510.01621"
    },
    "2510.02228": {
        "SCORE": 16,
        "ARXIVID": "2510.02228",
        "COMMENT": "Model Architecture/Efficiency: scaling-law analysis for xLSTM vs Transformers, highlighting linear time-complexity benefits and context-length dependent optimal sizing and inference scaling.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Maximilian Beck",
            "Kajetan Schweighofer",
            "Sebastian B\\\"ock",
            "Sebastian Lehner",
            "Sepp Hochreiter"
        ],
        "title": "xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity",
        "abstract": "Scaling laws play a central role in the success of Large Language Models (LLMs), enabling the prediction of model performance relative to compute budgets prior to training. While Transformers have been the dominant architecture, recent alternatives such as xLSTM offer linear complexity with respect to context length while remaining competitive in the billion-parameter regime. We conduct a comparative investigation on the scaling behavior of Transformers and xLSTM along the following lines, providing insights to guide future model design and deployment. First, we study the scaling behavior for xLSTM in compute-optimal and over-training regimes using both IsoFLOP and parametric fit approaches on a wide range of model sizes (80M-7B) and number of training tokens (2B-2T). Second, we examine the dependence of optimal model sizes on context length, a pivotal aspect that was largely ignored in previous work. Finally, we analyze inference-time scaling characteristics. Our findings reveal that in typical LLM training and inference scenarios, xLSTM scales favorably compared to Transformers. Importantly, xLSTM's advantage widens as training and inference contexts grow.",
        "arxiv_id": "2510.02228"
    },
    "2510.01796": {
        "SCORE": 16,
        "ARXIVID": "2510.01796",
        "COMMENT": "Model Architecture: introduces wide\u2013narrow\u2013wide Hourglass MLP with skip connections in expanded space and fixed random projection; improves performance\u2013parameter Pareto frontier.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Meng-Hsi Chen",
            "Yu-Ang Lee",
            "Feng-Ting Liao",
            "Da-shan Shiu"
        ],
        "title": "Rethinking the shape convention of an MLP",
        "abstract": "Multi-layer perceptrons (MLPs) conventionally follow a narrow-wide-narrow design where skip connections operate at the input/output dimensions while processing occurs in expanded hidden spaces. We challenge this convention by proposing wide-narrow-wide (Hourglass) MLP blocks where skip connections operate at expanded dimensions while residual computation flows through narrow bottlenecks. This inversion leverages higher-dimensional spaces for incremental refinement while maintaining computational efficiency through parameter-matched designs. Implementing Hourglass MLPs requires an initial projection to lift input signals to expanded dimensions. We propose that this projection can remain fixed at random initialization throughout training, enabling efficient training and inference implementations. We evaluate both architectures on generative tasks over popular image datasets, characterizing performance-parameter Pareto frontiers through systematic architectural search. Results show that Hourglass architectures consistently achieve superior Pareto frontiers compared to conventional designs. As parameter budgets increase, optimal Hourglass configurations favor deeper networks with wider skip connections and narrower bottlenecks-a scaling pattern distinct from conventional MLPs. Our findings suggest reconsidering skip connection placement in modern architectures, with potential applications extending to Transformers and other residual networks.",
        "arxiv_id": "2510.01796"
    },
    "2510.01863": {
        "SCORE": 16,
        "ARXIVID": "2510.01863",
        "COMMENT": "Model Compression and Efficiency / Low-precision Numerics: applies microscaling 8-bit floating-point formats to LLMs to reduce memory/compute while maintaining accuracy.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Marco Cococcioni",
            "Dario Pagani",
            "Federico Rossi"
        ],
        "title": "Microscaling Floating Point Formats for Large Language Models",
        "abstract": "The increasing computational and memory demands of large language models (LLMs) necessitate innovative approaches to optimize resource usage without compromising performance. This paper leverages microscaling floating-point formats, a novel technique designed to address these challenges by reducing the storage and computational overhead associated with numerical representations in LLMs. Unlike traditional floating-point representations that allocate a dedicated scale for each value, microscaling employs a shared scale across a block of values, enabling compact one-byte floating-point representations while maintaining an extended dynamic range. We explore the application of microscaling in the context of 8-bit floating-point formats to significantly reduce memory footprint and computational costs. We tested several configurations of microscaling floats within the GPT-2 LLM architecture, demonstrating that microscaling data formats can achieve competitive accuracy during training and inference, proving its efficacy as a resource-efficient alternative for deploying LLMs at scale. The source code is publicly available at: https://github.com/unipi-dii-compressedarith/llm.c-sve",
        "arxiv_id": "2510.01863"
    },
    "2510.01377": {
        "SCORE": 16,
        "ARXIVID": "2510.01377",
        "COMMENT": "ML Systems/Distributed training: decentralized optimization with gradient tracking and Newton-Schulz orthogonalization; provides complexity guarantees and transformer pretraining experiments.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Chuan He",
            "Shuyi Ren",
            "Jingwei Mao",
            "Erik G. Larsson"
        ],
        "title": "DeMuon: A Decentralized Muon for Matrix Optimization over Graphs",
        "abstract": "In this paper, we propose DeMuon, a method for decentralized matrix optimization over a given communication topology. DeMuon incorporates matrix orthogonalization via Newton-Schulz iterations-a technique inherited from its centralized predecessor, Muon-and employs gradient tracking to mitigate heterogeneity among local functions. Under heavy-tailed noise conditions and additional mild assumptions, we establish the iteration complexity of DeMuon for reaching an approximate stochastic stationary point. This complexity result matches the best-known complexity bounds of centralized algorithms in terms of dependence on the target tolerance. To the best of our knowledge, DeMuon is the first direct extension of Muon to decentralized optimization over graphs with provable complexity guarantees. We conduct preliminary numerical experiments on decentralized transformer pretraining over graphs with varying degrees of connectivity. Our numerical results demonstrate a clear margin of improvement of DeMuon over other popular decentralized algorithms across different network topologies.",
        "arxiv_id": "2510.01377"
    },
    "2510.01685": {
        "SCORE": 16,
        "ARXIVID": "2510.01685",
        "COMMENT": "Matches Representation Learning: analyzes LLM mechanisms for compositional function computation via logit-lens on residual streams and links behavior to embedding-space geometry.",
        "RELEVANCE": 9,
        "NOVELTY": 7,
        "authors": [
            "Apoorv Khandelwal",
            "Ellie Pavlick"
        ],
        "title": "How Do Language Models Compose Functions?",
        "abstract": "While large language models (LLMs) appear to be increasingly capable of solving compositional tasks, it is an open question whether they do so using compositional mechanisms. In this work, we investigate how feedforward LLMs solve two-hop factual recall tasks, which can be expressed compositionally as $g(f(x))$. We first confirm that modern LLMs continue to suffer from the \"compositionality gap\": i.e. their ability to compute both $z = f(x)$ and $y = g(z)$ does not entail their ability to compute the composition $y = g(f(x))$. Then, using logit lens on their residual stream activations, we identify two processing mechanisms, one which solves tasks $\\textit{compositionally}$, computing $f(x)$ along the way to computing $g(f(x))$, and one which solves them $\\textit{directly}$, without any detectable signature of the intermediate variable $f(x)$. Finally, we find that which mechanism is employed appears to be related to the embedding space geometry, with the idiomatic mechanism being dominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in the embedding spaces. We fully release our data and code at: https://github.com/apoorvkh/composing-functions .",
        "arxiv_id": "2510.01685"
    },
    "2510.02239": {
        "SCORE": 16,
        "ARXIVID": "2510.02239",
        "COMMENT": "ML Systems / Training Efficiency: randomized progressive layer updates with non-Euclidean updates, theoretical convergence and cost optimality analysis, yielding faster wall-clock training.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Kaja Gruntkowska",
            "Yassine Maziane",
            "Zheng Qu",
            "Peter Richt\\'arik"
        ],
        "title": "Drop-Muon: Update Less, Converge Faster",
        "abstract": "Conventional wisdom in deep learning optimization dictates updating all layers at every step-a principle followed by all recent state-of-the-art optimizers such as Muon. In this work, we challenge this assumption, showing that full-network updates can be fundamentally suboptimal, both in theory and in practice. We introduce a non-Euclidean Randomized Progressive Training method-Drop-Muon-a simple yet powerful framework that updates only a subset of layers per step according to a randomized schedule, combining the efficiency of progressive training with layer-specific non-Euclidean updates for top-tier performance. We provide rigorous convergence guarantees under both layer-wise smoothness and layer-wise $(L^0, L^1)$-smoothness, covering deterministic and stochastic gradient settings, marking the first such results for progressive training in the stochastic and non-smooth regime. Our cost analysis further reveals that full-network updates are not optimal unless a very specific relationship between layer smoothness constants holds. Through controlled CNN experiments, we empirically demonstrate that Drop-Muon consistently outperforms full-network Muon, achieving the same accuracy up to $1.4\\times$ faster in wall-clock time. Together, our results suggest a shift in how large-scale models can be efficiently trained, challenging the status quo and offering a highly efficient, theoretically grounded alternative to full-network updates.",
        "arxiv_id": "2510.02239"
    },
    "2510.02096": {
        "SCORE": 16,
        "ARXIVID": "2510.02096",
        "COMMENT": "Representation Learning: learns weight-space representations across heterogeneous architectures from public model hubs via a new backbone.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Damian Falk",
            "Konstantin Sch\\\"urholt",
            "Konstantinos Tzevelekakis",
            "L\\'eo Meynent",
            "Damian Borth"
        ],
        "title": "Learning Model Representations Using Publicly Available Model Hubs",
        "abstract": "The weights of neural networks have emerged as a novel data modality, giving rise to the field of weight space learning. A central challenge in this area is that learning meaningful representations of weights typically requires large, carefully constructed collections of trained models, typically referred to as model zoos. These model zoos are often trained ad-hoc, requiring large computational resources, constraining the learned weight space representations in scale and flexibility. In this work, we drop this requirement by training a weight space learning backbone on arbitrary models downloaded from large, unstructured model repositories such as Hugging Face. Unlike curated model zoos, these repositories contain highly heterogeneous models: they vary in architecture and dataset, and are largely undocumented. To address the methodological challenges posed by such heterogeneity, we propose a new weight space backbone designed to handle unstructured model populations. We demonstrate that weight space representations trained on models from Hugging Face achieve strong performance, often outperforming backbones trained on laboratory-generated model zoos. Finally, we show that the diversity of the model weights in our training set allows our weight space model to generalize to unseen data modalities. By demonstrating that high-quality weight space representations can be learned in the wild, we show that curated model zoos are not indispensable, thereby overcoming a strong limitation currently faced by the weight space learning community.",
        "arxiv_id": "2510.02096"
    },
    "2510.01944": {
        "SCORE": 16,
        "ARXIVID": "2510.01944",
        "COMMENT": "Representation Learning/Training Dynamics: continuous-time PCD via coupled SDEs with uniform-in-time convergence bounds and explicit long-time error estimates.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Paul Felix Valsecchi Oliva",
            "O. Deniz Akyildiz",
            "Andrew Duncan"
        ],
        "title": "Uniform-in-time convergence bounds for Persistent Contrastive Divergence Algorithms",
        "abstract": "We propose a continuous-time formulation of persistent contrastive divergence (PCD) for maximum likelihood estimation (MLE) of unnormalised densities. Our approach expresses PCD as a coupled, multiscale system of stochastic differential equations (SDEs), which perform optimisation of the parameter and sampling of the associated parametrised density, simultaneously.   From this novel formulation, we are able to derive explicit bounds for the error between the PCD iterates and the MLE solution for the model parameter. This is made possible by deriving uniform-in-time (UiT) bounds for the difference in moments between the multiscale system and the averaged regime. An efficient implementation of the continuous-time scheme is introduced, leveraging a class of explicit, stable intregators, stochastic orthogonal Runge-Kutta Chebyshev (S-ROCK), for which we provide explicit error estimates in the long-time regime. This leads to a novel method for training energy-based models (EBMs) with explicit error guarantees.",
        "arxiv_id": "2510.01944"
    },
    "2510.01510": {
        "SCORE": 16,
        "ARXIVID": "2510.01510",
        "COMMENT": "Representation Learning/Architecture: probabilistic node\u2013relation equivariance and random-walk sequence modeling for KG foundation models.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Jinwoo Kim",
            "Xingyue Huang",
            "Krzysztof Olejniczak",
            "Kyungbin Min",
            "Michael Bronstein",
            "Seunghoon Hong",
            "\\.Ismail \\.Ilkan Ceylan"
        ],
        "title": "Flock: A Knowledge Graph Foundation Model via Learning on Random Walks",
        "abstract": "We study the problem of zero-shot link prediction on knowledge graphs (KGs), which requires models to generalize over novel entities and novel relations. Knowledge graph foundation models (KGFMs) address this task by enforcing equivariance over both nodes and relations, learning from structural properties of nodes and relations, which are then transferable to novel graphs with similar structural properties. However, the conventional notion of deterministic equivariance imposes inherent limits on the expressive power of KGFMs, preventing them from distinguishing structurally similar but semantically distinct relations. To overcome this limitation, we introduce probabilistic node-relation equivariance, which preserves equivariance in distribution while incorporating a principled randomization to break symmetries during inference. Building on this principle, we present Flock, a KGFM that iteratively samples random walks, encodes them into sequences via a recording protocol, embeds them with a sequence model, and aggregates representations of nodes and relations via learned pooling. Crucially, Flock respects probabilistic node-relation equivariance and is a universal approximator for isomorphism-invariant link-level functions over KGs. Empirically, Flock perfectly solves our new diagnostic dataset Petals where current KGFMs fail, and achieves state-of-the-art performances on entity- and relation prediction tasks on 54 KGs from diverse domains.",
        "arxiv_id": "2510.01510"
    },
    "2510.01329": {
        "SCORE": 16,
        "ARXIVID": "2510.01329",
        "COMMENT": "Model Architecture/Efficiency: augments discrete diffusion with continuous latents to avoid absorbing-mask void and enable controllable sampling trade-offs.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Huangjie Zheng",
            "Shansan Gong",
            "Ruixiang Zhang",
            "Tianrong Chen",
            "Jiatao Gu",
            "Mingyuan Zhou",
            "Navdeep Jaitly",
            "Yizhe Zhang"
        ],
        "title": "Continuously Augmented Discrete Diffusion model for Categorical Generative Modeling",
        "abstract": "Standard discrete diffusion models treat all unobserved states identically by mapping them to an absorbing [MASK] token. This creates an 'information void' where semantic information that could be inferred from unmasked tokens is lost between denoising steps. We introduce Continuously Augmented Discrete Diffusion (CADD), a framework that augments the discrete state space with a paired diffusion in a continuous latent space. This yields graded, gradually corrupted states in which masked tokens are represented by noisy yet informative latent vectors rather than collapsed 'information voids'. At each reverse step, CADD may leverage the continuous latent as a semantic hint to guide discrete denoising. The design is clean and compatible with existing discrete diffusion training. At sampling time, the strength and choice of estimator for the continuous latent vector enables a controlled trade-off between mode-coverage (generating diverse outputs) and mode-seeking (generating contextually precise outputs) behaviors. Empirically, we demonstrate CADD improves generative quality over mask-based diffusion across text generation, image synthesis, and code modeling, with consistent gains on both qualitative and quantitative metrics against strong discrete baselines.",
        "arxiv_id": "2510.01329"
    },
    "2510.02308": {
        "SCORE": 16,
        "ARXIVID": "2510.02308",
        "COMMENT": "Representation Learning: robust manifold/tangent space estimation via Laplacian eigenvector gradient orthogonalization with theoretical noise robustness.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Dhruv Kohli",
            "Sawyer J. Robertson",
            "Gal Mishne",
            "Alexander Cloninger"
        ],
        "title": "Robust Tangent Space Estimation via Laplacian Eigenvector Gradient Orthogonalization",
        "abstract": "Estimating the tangent spaces of a data manifold is a fundamental problem in data analysis. The standard approach, Local Principal Component Analysis (LPCA), struggles in high-noise settings due to a critical trade-off in choosing the neighborhood size. Selecting an optimal size requires prior knowledge of the geometric and noise characteristics of the data that are often unavailable. In this paper, we propose a spectral method, Laplacian Eigenvector Gradient Orthogonalization (LEGO), that utilizes the global structure of the data to guide local tangent space estimation. Instead of relying solely on local neighborhoods, LEGO estimates the tangent space at each data point by orthogonalizing the gradients of low-frequency eigenvectors of the graph Laplacian. We provide two theoretical justifications of our method. First, a differential geometric analysis on a tubular neighborhood of a manifold shows that gradients of the low-frequency Laplacian eigenfunctions of the tube align closely with the manifold's tangent bundle, while an eigenfunction with high gradient in directions orthogonal to the manifold lie deeper in the spectrum. Second, a random matrix theoretic analysis also demonstrates that low-frequency eigenvectors are robust to sub-Gaussian noise. Through comprehensive experiments, we demonstrate that LEGO yields tangent space estimates that are significantly more robust to noise than those from LPCA, resulting in marked improvements in downstream tasks such as manifold learning, boundary detection, and local intrinsic dimension estimation.",
        "arxiv_id": "2510.02308"
    },
    "2510.01855": {
        "SCORE": 16,
        "ARXIVID": "2510.01855",
        "COMMENT": "Matches Model Architecture/Representation: explicit discovery of nonlinear Lie symmetries to guide equivariant network design and data augmentation; improves neural PDE solver rollouts.",
        "RELEVANCE": 8,
        "NOVELTY": 8,
        "authors": [
            "Lexiang Hu",
            "Yikang Li",
            "Zhouchen Lin"
        ],
        "title": "Explicit Discovery of Nonlinear Symmetries from Dynamic Data",
        "abstract": "Symmetry is widely applied in problems such as the design of equivariant networks and the discovery of governing equations, but in complex scenarios, it is not known in advance. Most previous symmetry discovery methods are limited to linear symmetries, and recent attempts to discover nonlinear symmetries fail to explicitly get the Lie algebra subspace. In this paper, we propose LieNLSD, which is, to our knowledge, the first method capable of determining the number of infinitesimal generators with nonlinear terms and their explicit expressions. We specify a function library for the infinitesimal group action and aim to solve for its coefficient matrix, proving that its prolongation formula for differential equations, which governs dynamic data, is also linear with respect to the coefficient matrix. By substituting the central differences of the data and the Jacobian matrix of the trained neural network into the infinitesimal criterion, we get a system of linear equations for the coefficient matrix, which can then be solved using SVD. On top quark tagging and a series of dynamic systems, LieNLSD shows qualitative advantages over existing methods and improves the long rollout accuracy of neural PDE solvers by over 20% while applying to guide data augmentation. Code and data are available at https://github.com/hulx2002/LieNLSD.",
        "arxiv_id": "2510.01855"
    },
    "2510.01494": {
        "SCORE": 15,
        "ARXIVID": "2510.01494",
        "COMMENT": "Representation Learning: theoretical and empirical analysis explaining transferability differences between data-space vs representation-space attacks via representation geometry alignment.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Isha Gupta",
            "Rylan Schaeffer",
            "Joshua Kazdan",
            "Ken Liu",
            "Sanmi Koyejo"
        ],
        "title": "Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed",
        "abstract": "The field of adversarial robustness has long established that adversarial examples can successfully transfer between image classifiers and that text jailbreaks can successfully transfer between language models (LMs). However, a pair of recent studies reported being unable to successfully transfer image jailbreaks between vision-language models (VLMs). To explain this striking difference, we propose a fundamental distinction regarding the transferability of attacks against machine learning models: attacks in the input data-space can transfer, whereas attacks in model representation space do not, at least not without geometric alignment of representations. We then provide theoretical and empirical evidence of this hypothesis in four different settings. First, we mathematically prove this distinction in a simple setting where two networks compute the same input-output map but via different representations. Second, we construct representation-space attacks against image classifiers that are as successful as well-known data-space attacks, but fail to transfer. Third, we construct representation-space attacks against LMs that successfully jailbreak the attacked models but again fail to transfer. Fourth, we construct data-space attacks against VLMs that successfully transfer to new VLMs, and we show that representation space attacks \\emph{can} transfer when VLMs' latent geometries are sufficiently aligned in post-projector space. Our work reveals that adversarial transfer is not an inherent property of all attacks but contingent on their operational domain - the shared data-space versus models' unique representation spaces - a critical insight for building more robust models.",
        "arxiv_id": "2510.01494"
    },
    "2510.01930": {
        "SCORE": 15,
        "ARXIVID": "2510.01930",
        "COMMENT": "Representation Learning/Training dynamics: DMFT-based unified analysis of gradient-flow in diagonal linear networks, quantifying convergence and generalization trade-offs.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Sota Nishiyama",
            "Masaaki Imaizumi"
        ],
        "title": "Precise Dynamics of Diagonal Linear Networks: A Unifying Analysis by Dynamical Mean-Field Theory",
        "abstract": "Diagonal linear networks (DLNs) are a tractable model that captures several nontrivial behaviors in neural network training, such as initialization-dependent solutions and incremental learning. These phenomena are typically studied in isolation, leaving the overall dynamics insufficiently understood. In this work, we present a unified analysis of various phenomena in the gradient flow dynamics of DLNs. Using Dynamical Mean-Field Theory (DMFT), we derive a low-dimensional effective process that captures the asymptotic gradient flow dynamics in high dimensions. Analyzing this effective process yields new insights into DLN dynamics, including loss convergence rates and their trade-off with generalization, and systematically reproduces many of the previously observed phenomena. These findings deepen our understanding of DLNs and demonstrate the effectiveness of the DMFT approach in analyzing high-dimensional learning dynamics of neural networks.",
        "arxiv_id": "2510.01930"
    },
    "2510.01349": {
        "SCORE": 15,
        "ARXIVID": "2510.01349",
        "COMMENT": "Representation Learning: quantifies distributional symmetry breaking and shows theoretically when invariance can harm performance; provides a diagnostic for equivariant methods.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Hannah Lawrence",
            "Elyssa Hofgard",
            "Vasco Portilheiro",
            "Yuxuan Chen",
            "Tess Smidt",
            "Robin Walters"
        ],
        "title": "To Augment or Not to Augment? Diagnosing Distributional Symmetry Breaking",
        "abstract": "Symmetry-aware methods for machine learning, such as data augmentation and equivariant architectures, encourage correct model behavior on all transformations (e.g. rotations or permutations) of the original dataset. These methods can improve generalization and sample efficiency, under the assumption that the transformed datapoints are highly probable, or \"important\", under the test distribution. In this work, we develop a method for critically evaluating this assumption. In particular, we propose a metric to quantify the amount of anisotropy, or symmetry-breaking, in a dataset, via a two-sample neural classifier test that distinguishes between the original dataset and its randomly augmented equivalent. We validate our metric on synthetic datasets, and then use it to uncover surprisingly high degrees of alignment in several benchmark point cloud datasets. We show theoretically that distributional symmetry-breaking can actually prevent invariant methods from performing optimally even when the underlying labels are truly invariant, as we show for invariant ridge regression in the infinite feature limit. Empirically, we find that the implication for symmetry-aware methods is dataset-dependent: equivariant methods still impart benefits on some anisotropic datasets, but not others. Overall, these findings suggest that understanding equivariance -- both when it works, and why -- may require rethinking symmetry biases in the data.",
        "arxiv_id": "2510.01349"
    },
    "2510.01706": {
        "SCORE": 15,
        "ARXIVID": "2510.01706",
        "COMMENT": "Representation Learning: Hierarchical Optimal Transport jointly aligns layers and neurons across networks, yielding a global alignment score and soft cross-depth correspondences.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Shaan Shah",
            "Meenakshi Khosla"
        ],
        "title": "Representational Alignment Across Model Layers and Brain Regions with Hierarchical Optimal Transport",
        "abstract": "Standard representational similarity methods align each layer of a network to its best match in another independently, producing asymmetric results, lacking a global alignment score, and struggling with networks of different depths. These limitations arise from ignoring global activation structure and restricting mappings to rigid one-to-one layer correspondences. We propose Hierarchical Optimal Transport (HOT), a unified framework that jointly infers soft, globally consistent layer-to-layer couplings and neuron-level transport plans. HOT allows source neurons to distribute mass across multiple target layers while minimizing total transport cost under marginal constraints. This yields both a single alignment score for the entire network comparison and a soft transport plan that naturally handles depth mismatches through mass distribution. We evaluate HOT on vision models, large language models, and human visual cortex recordings. Across all domains, HOT matches or surpasses standard pairwise matching in alignment quality. Moreover, it reveals smooth, fine-grained hierarchical correspondences: early layers map to early layers, deeper layers maintain relative positions, and depth mismatches are resolved by distributing representations across multiple layers. These structured patterns emerge naturally from global optimization without being imposed, yet are absent in greedy layer-wise methods. HOT thus enables richer, more interpretable comparisons between representations, particularly when networks differ in architecture or depth.",
        "arxiv_id": "2510.01706"
    },
    "2510.01407": {
        "SCORE": 15,
        "ARXIVID": "2510.01407",
        "COMMENT": "Model Compression and Efficiency: proposes low-rank latent operations with vector quantization to drastically reduce decoder compute in neural compression.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Ethan G. Rogers",
            "Cheng Wang"
        ],
        "title": "Ultra-Efficient Decoding for End-to-End Neural Compression and Reconstruction",
        "abstract": "Image compression and reconstruction are crucial for various digital applications. While contemporary neural compression methods achieve impressive compression rates, the adoption of such technology has been largely hindered by the complexity and large computational costs of the convolution-based decoders during data reconstruction. To address the decoder bottleneck in neural compression, we develop a new compression-reconstruction framework based on incorporating low-rank representation in an autoencoder with vector quantization. We demonstrated that performing a series of computationally efficient low-rank operations on the learned latent representation of images can efficiently reconstruct the data with high quality. Our approach dramatically reduces the computational overhead in the decoding phase of neural compression/reconstruction, essentially eliminating the decoder compute bottleneck while maintaining high fidelity of image outputs.",
        "arxiv_id": "2510.01407"
    },
    "2510.01394": {
        "SCORE": 15,
        "ARXIVID": "2510.01394",
        "COMMENT": "ML Systems/Inference-time optimization: optimal stopping framework (UCB-style Pandora\u2019s Box) to reduce generations with theoretical guarantees and practical normalization for LLMs.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Yusuf Kalayci",
            "Vinod Raman",
            "Shaddin Dughmi"
        ],
        "title": "Optimal Stopping vs Best-of-$N$ for Inference Time Optimization",
        "abstract": "Large language model (LLM) generation often requires balancing output quality against inference cost, especially when using multiple generations. We introduce a new framework for inference-time optimization based on the classical Pandora's Box problem. Viewing each generation as opening a costly \"box\" with random reward, we develop algorithms that decide when to stop generating without knowing the underlying reward distribution. Our first contribution is a UCB-style Pandora's Box algorithm, which achieves performance that is provably close to Weitzman's algorithm, the optimal strategy when the distribution is known. We further adapt this method to practical LLM settings by addressing reward scaling across prompts via a Bradley-Terry inspired transformation. This leads to an adaptive inference-time optimization method that normalizes rewards and learns stopping thresholds on the fly. Experiments on the AlpacaFarm and HH-RLHF datasets, using multiple LLM-reward model pairs, show that our adaptive strategy can obtain the same performance as non-adaptive Best-of-N sampling while requiring 15-35 percent fewer generations on average. Our results establish a principled bridge between optimal stopping theory and inference-time scaling, providing both theoretical performance bounds and practical efficiency gains for LLM deployment.",
        "arxiv_id": "2510.01394"
    },
    "2510.01663": {
        "SCORE": 15,
        "ARXIVID": "2510.01663",
        "COMMENT": "Model Compression and Efficiency: proposes shift-invariant Shapley-value\u2013based pruning for KANs, enabling reliable importance ranking and effective compression beyond magnitude pruning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Wangxuan Fan",
            "Ching Wang",
            "Siqi Li",
            "Nan Liu"
        ],
        "title": "Shift-Invariant Attribute Scoring for Kolmogorov-Arnold Networks via Shapley Value",
        "abstract": "For many real-world applications, understanding feature-outcome relationships is as crucial as achieving high predictive accuracy. While traditional neural networks excel at prediction, their black-box nature obscures underlying functional relationships. Kolmogorov--Arnold Networks (KANs) address this by employing learnable spline-based activation functions on edges, enabling recovery of symbolic representations while maintaining competitive performance. However, KAN's architecture presents unique challenges for network pruning. Conventional magnitude-based methods become unreliable due to sensitivity to input coordinate shifts. We propose \\textbf{ShapKAN}, a pruning framework using Shapley value attribution to assess node importance in a shift-invariant manner. Unlike magnitude-based approaches, ShapKAN quantifies each node's actual contribution, ensuring consistent importance rankings regardless of input parameterization. Extensive experiments on synthetic and real-world datasets demonstrate that ShapKAN preserves true node importance while enabling effective network compression. Our approach improves KAN's interpretability advantages, facilitating deployment in resource-constrained environments.",
        "arxiv_id": "2510.01663"
    },
    "2510.01581": {
        "SCORE": 15,
        "ARXIVID": "2510.01581",
        "COMMENT": "Model Compression and Efficiency: adaptive compute allocation via attention-based compression of reasoning steps; dynamic networks idea with RL training.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Joykirat Singh",
            "Justin Chih-Yao Chen",
            "Archiki Prasad",
            "Elias Stengel-Eskin",
            "Akshay Nambi",
            "Mohit Bansal"
        ],
        "title": "Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression",
        "abstract": "Recent thinking models solve complex reasoning tasks by scaling test-time compute, but this scaling must be allocated in line with task difficulty. On one hand, short reasoning (underthinking) leads to errors on harder problems that require extended reasoning steps; but, excessively long reasoning (overthinking) can be token-inefficient, generating unnecessary steps even after reaching a correct intermediate solution. We refer to this as under-adaptivity, where the model fails to modulate its response length appropriately given problems of varying difficulty. To address under-adaptivity and strike a balance between under- and overthinking, we propose TRAAC (Think Right with Adaptive, Attentive Compression), an online post-training RL method that leverages the model's self-attention over a long reasoning trajectory to identify important steps and prune redundant ones. TRAAC also estimates difficulty and incorporates it into training rewards, thereby learning to allocate reasoning budget commensurate with example difficulty. Our approach improves accuracy, reduces reasoning steps, and enables adaptive thinking compared to base models and other RL baselines. Across a variety of tasks (AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute accuracy gain of 8.4% with a relative reduction in reasoning length of 36.8% compared to the base model, and a 7.9% accuracy gain paired with a 29.4% length drop compared to the best RL baseline. TRAAC also shows strong generalization: although our models are trained on math datasets, they show accuracy and efficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH, and OptimalThinkingBench. Our analysis further verifies that TRAAC provides fine-grained adjustments to thinking budget based on difficulty and that a combination of task-difficulty calibration and attention-based compression yields gains across diverse tasks.",
        "arxiv_id": "2510.01581"
    },
    "2510.01681": {
        "SCORE": 15,
        "ARXIVID": "2510.01681",
        "COMMENT": "Model Architecture: conditional/dynamic computation in VLMs via rollout-guided RL to decide when to invoke pixel-level operations, improving efficiency.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Xuchen Li",
            "Xuzhao Li",
            "Jiahui Gao",
            "Renjie Pi",
            "Shiyu Hu",
            "Wentao Zhang"
        ],
        "title": "Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning",
        "abstract": "Vision-Language Models (VLMs) excel at many multimodal tasks, yet they frequently struggle with tasks requiring precise understanding and handling of fine-grained visual elements. This is mainly due to information loss during image encoding or insufficient attention to critical regions. Recent work has shown promise by incorporating pixel-level visual information into the reasoning process, enabling VLMs to access high-resolution visual details during their thought process. However, this pixel-level information is often overused, leading to inefficiency and distraction from irrelevant visual details. To address these challenges, we propose the first framework for adaptive pixel reasoning that dynamically determines necessary pixel-level operations based on the input query. Specifically, we first apply operation-aware supervised fine-tuning to establish baseline competence in textual reasoning and visual operations, then design a novel rollout-guided reinforcement learning framework relying on feedback of the model's own responses, which enables the VLM to determine when pixel operations should be invoked based on query difficulty. Experiments on extensive multimodal reasoning benchmarks show that our model achieves superior performance while significantly reducing unnecessary visual operations. Impressively, our model achieves 73.4\\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of only 20.1\\%, improving accuracy and simultaneously reducing tool usage by 66.5\\% compared to the previous methods.",
        "arxiv_id": "2510.01681"
    },
    "2510.01528": {
        "SCORE": 15,
        "ARXIVID": "2510.01528",
        "COMMENT": "Representation Learning: leverages sparse autoencoders to analyze internal token representations and guide generation, providing interpretable, contrastive-like structure for reasoning.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Daniel Zhao",
            "Abhilash Shankarampeta",
            "Lanxiang Hu",
            "Tajana Rosing",
            "Hao Zhang"
        ],
        "title": "Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation",
        "abstract": "We propose a novel method that leverages sparse autoencoders (SAEs) and clustering techniques to analyze the internal token representations of large language models (LLMs) and guide generations in mathematical reasoning tasks. Our approach first trains an SAE to generate sparse vector representations for training tokens, then applies k-means clustering to construct a graph where vertices represent token clusters and weighted edges capture sequential token transitions. Using this graph, we define an edge-weight based reward function to quantify adherence to established reasoning traces, thereby identifying exploitative reasoning trajectories. Additionally, we measure generation diversity from clustering to assess the extent of exploration. Our findings indicate that balancing both exploitation and exploration is crucial for achieving high accuracy in mathematical reasoning tasks. During generation, the SAE can serve as a scalable reward model to guide generations, ensuring a balanced trade-off between exploitation and exploration. This prevents extreme behaviors in either direction, ultimately fostering a higher-quality reasoning process in LLMs.",
        "arxiv_id": "2510.01528"
    },
    "2510.02259": {
        "SCORE": 15,
        "ARXIVID": "2510.02259",
        "COMMENT": "Model Architecture: demonstrates pure Transformers (no graph priors) can learn molecular interactions, revealing emergent inductive biases and favorable scaling\u2014challenging GNN-specific assumptions.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Tobias Kreiman",
            "Yutong Bai",
            "Fadi Atieh",
            "Elizabeth Weaver",
            "Eric Qu",
            "Aditi S. Krishnapriyan"
        ],
        "title": "Transformers Discover Molecular Structure Without Graph Priors",
        "abstract": "Graph Neural Networks (GNNs) are the dominant architecture for molecular machine learning, particularly for molecular property prediction and machine learning interatomic potentials (MLIPs). GNNs perform message passing on predefined graphs often induced by a fixed radius cutoff or k-nearest neighbor scheme. While this design aligns with the locality present in many molecular tasks, a hard-coded graph can limit expressivity due to the fixed receptive field and slows down inference with sparse graph operations. In this work, we investigate whether pure, unmodified Transformers trained directly on Cartesian coordinates$\\unicode{x2013}$without predefined graphs or physical priors$\\unicode{x2013}$can approximate molecular energies and forces. As a starting point for our analysis, we demonstrate how to train a Transformer to competitive energy and force mean absolute errors under a matched training compute budget, relative to a state-of-the-art equivariant GNN on the OMol25 dataset. We discover that the Transformer learns physically consistent patterns$\\unicode{x2013}$such as attention weights that decay inversely with interatomic distance$\\unicode{x2013}$and flexibly adapts them across different molecular environments due to the absence of hard-coded biases. The use of a standard Transformer also unlocks predictable improvements with respect to scaling training resources, consistent with empirical scaling laws observed in other domains. Our results demonstrate that many favorable properties of GNNs can emerge adaptively in Transformers, challenging the necessity of hard-coded graph inductive biases and pointing toward standardized, scalable architectures for molecular modeling.",
        "arxiv_id": "2510.02259"
    },
    "2510.02295": {
        "SCORE": 15,
        "ARXIVID": "2510.02295",
        "COMMENT": "Matches Model Architecture/Efficiency: hardware-aware native sparse attention with hybrid dense-text/sparse-video scheme enabling long-context (up to 128K) video-language modeling.",
        "RELEVANCE": 8,
        "NOVELTY": 7,
        "authors": [
            "Enxin Song",
            "Wenhao Chai",
            "Shusheng Yang",
            "Ethan Armand",
            "Xiaojun Shan",
            "Haiyang Xu",
            "Jianwen Xie",
            "Zhuowen Tu"
        ],
        "title": "VideoNSA: Native Sparse Attention Scales Video Understanding",
        "abstract": "Video understanding in multimodal language models remains limited by context length: models often miss key transition frames and struggle to maintain coherence across long time scales. To address this, we adapt Native Sparse Attention (NSA) to video-language models. Our method, VideoNSA, adapts Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We employ a hardware-aware hybrid approach to attention, preserving dense attention for text, while employing NSA for video. Compared to token-compression and training-free sparse baselines, VideoNSA achieves improved performance on long-video understanding, temporal reasoning, and spatial benchmarks. Further ablation analysis reveals four key findings: (1) reliable scaling to 128K tokens; (2) an optimal global-local attention allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4) the learnable combined sparse attention help induce dynamic attention sinks.",
        "arxiv_id": "2510.02295"
    }
}